{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["zv3uvOnIRCXW","oGhHinFqU_K_","PRP0m_1rVEsy","I-bZQz-GYqWo","kgRYmEFHXo14","vgNmJMOcG2ZR","mTKiZ-eIMiTv"],"machine_shape":"hm","authorship_tag":"ABX9TyNlbVUBuy+3d4u1QQW43MTy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install geopandas matplotlib_scalebar\n","!pip install rasterio"],"metadata":{"id":"FeC_QatXRMxc","executionInfo":{"status":"ok","timestamp":1757040218324,"user_tz":-480,"elapsed":18554,"user":{"displayName":"weilin xin","userId":"14777990448514190568"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5b94731b-46a1-440f-c406-4051edbad580","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: geopandas in /usr/local/lib/python3.12/dist-packages (1.1.1)\n","Collecting matplotlib_scalebar\n","  Downloading matplotlib_scalebar-0.9.0-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.0.2)\n","Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas) (0.11.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from geopandas) (25.0)\n","Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.2.2)\n","Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (3.7.2)\n","Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.1.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from matplotlib_scalebar) (3.10.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2025.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from pyogrio>=0.7.2->geopandas) (2025.8.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib_scalebar) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib_scalebar) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib_scalebar) (4.59.2)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib_scalebar) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib_scalebar) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib_scalebar) (3.2.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->geopandas) (1.17.0)\n","Downloading matplotlib_scalebar-0.9.0-py3-none-any.whl (16 kB)\n","Installing collected packages: matplotlib_scalebar\n","Successfully installed matplotlib_scalebar-0.9.0\n","Collecting rasterio\n","  Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n","Collecting affine (from rasterio)\n","  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.3.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2025.8.3)\n","Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.2.1)\n","Collecting cligj>=0.5 (from rasterio)\n","  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.0.2)\n","Collecting click-plugins (from rasterio)\n","  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.3)\n","Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n","Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n","Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n","Installing collected packages: cligj, click-plugins, affine, rasterio\n","Successfully installed affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 rasterio-1.4.3\n"]}]},{"cell_type":"code","source":["pip install pyarrow rasterstats"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ve1xIyNy-Fjz","executionInfo":{"status":"ok","timestamp":1756199444554,"user_tz":-480,"elapsed":6346,"user":{"displayName":"weilin xin","userId":"14777990448514190568"}},"outputId":"2211dd4f-0698-4411-f616-f86e229697b3","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n","Collecting rasterstats\n","  Downloading rasterstats-0.20.0-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: affine in /usr/local/lib/python3.12/dist-packages (from rasterstats) (2.4.0)\n","Requirement already satisfied: click>7.1 in /usr/local/lib/python3.12/dist-packages (from rasterstats) (8.2.1)\n","Requirement already satisfied: cligj>=0.4 in /usr/local/lib/python3.12/dist-packages (from rasterstats) (0.7.2)\n","Collecting fiona (from rasterstats)\n","  Downloading fiona-1.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.12/dist-packages (from rasterstats) (2.0.2)\n","Requirement already satisfied: rasterio>=1.0 in /usr/local/lib/python3.12/dist-packages (from rasterstats) (1.4.3)\n","Requirement already satisfied: simplejson in /usr/local/lib/python3.12/dist-packages (from rasterstats) (3.20.1)\n","Requirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (from rasterstats) (2.1.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.0->rasterstats) (25.3.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.0->rasterstats) (2025.8.3)\n","Requirement already satisfied: click-plugins in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.0->rasterstats) (1.1.1.2)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.0->rasterstats) (3.2.3)\n","Downloading rasterstats-0.20.0-py3-none-any.whl (17 kB)\n","Downloading fiona-1.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: fiona, rasterstats\n","Successfully installed fiona-1.10.1 rasterstats-0.20.0\n"]}]},{"cell_type":"code","source":["pip install torch_geometric"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yTGiLNuIej8l","executionInfo":{"status":"ok","timestamp":1756199451158,"user_tz":-480,"elapsed":6601,"user":{"displayName":"weilin xin","userId":"14777990448514190568"}},"outputId":"73ddc914-8c1d-4cac-ccb4-de0284da6761","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_geometric\n","  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.12.15)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.8.3)\n","Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.14.1)\n","Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch_geometric\n","Successfully installed torch_geometric-2.6.1\n"]}]},{"cell_type":"code","source":["pip install causal-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xLXbSbRzI4D7","outputId":"833c0381-afa0-4018-b093-115fb482b39a","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: causal-learn in /usr/local/lib/python3.12/dist-packages (0.1.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from causal-learn) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from causal-learn) (1.15.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from causal-learn) (1.6.1)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from causal-learn) (0.21)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (from causal-learn) (0.14.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from causal-learn) (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from causal-learn) (3.10.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from causal-learn) (3.5)\n","Requirement already satisfied: pydot in /usr/local/lib/python3.12/dist-packages (from causal-learn) (3.0.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from causal-learn) (4.67.1)\n","Requirement already satisfied: momentchi2 in /usr/local/lib/python3.12/dist-packages (from causal-learn) (0.1.8)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causal-learn) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causal-learn) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causal-learn) (4.59.2)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causal-learn) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causal-learn) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causal-learn) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causal-learn) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causal-learn) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->causal-learn) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->causal-learn) (2025.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->causal-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->causal-learn) (3.6.0)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels->causal-learn) (1.0.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->causal-learn) (1.17.0)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n","    status = run_func(*args)\n","             ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n","    return func(self, options, args)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n","    conflicts = self._determine_conflicts(to_install)\n","                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n","    return check_install_conflicts(to_install)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n","^C\n"]}]},{"cell_type":"code","source":["pip install dowhy econml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"pBX5MxUWnC5u","executionInfo":{"status":"ok","timestamp":1757208395561,"user_tz":-480,"elapsed":26132,"user":{"displayName":"weilin xin","userId":"14777990448514190568"}},"outputId":"dbca0e06-62a5-4144-9fa0-b73d6c6c763e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dowhy\n","  Downloading dowhy-0.13-py3-none-any.whl.metadata (18 kB)\n","Collecting econml\n","  Downloading econml-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (37 kB)\n","Collecting causal-learn>=0.1.3.0 (from dowhy)\n","  Downloading causal_learn-0.1.4.3-py3-none-any.whl.metadata (4.6 kB)\n","Collecting cvxpy<1.5 (from dowhy)\n","  Downloading cvxpy-1.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n","Requirement already satisfied: cython>=3.0 in /usr/local/lib/python3.12/dist-packages (from dowhy) (3.0.12)\n","Requirement already satisfied: joblib>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from dowhy) (1.5.2)\n","Requirement already satisfied: networkx>=3.3 in /usr/local/lib/python3.12/dist-packages (from dowhy) (3.5)\n","Requirement already satisfied: numba>=0.59 in /usr/local/lib/python3.12/dist-packages (from dowhy) (0.60.0)\n","Requirement already satisfied: numpy>1.0 in /usr/local/lib/python3.12/dist-packages (from dowhy) (2.0.2)\n","Requirement already satisfied: pandas>1.0 in /usr/local/lib/python3.12/dist-packages (from dowhy) (2.2.2)\n","Requirement already satisfied: scikit-learn>1.0 in /usr/local/lib/python3.12/dist-packages (from dowhy) (1.6.1)\n","Collecting scipy<=1.15.3 (from dowhy)\n","  Downloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: statsmodels>=0.14 in /usr/local/lib/python3.12/dist-packages (from dowhy) (0.14.5)\n","Requirement already satisfied: sympy>=1.10.1 in /usr/local/lib/python3.12/dist-packages (from dowhy) (1.13.3)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from dowhy) (4.67.1)\n","Collecting sparse (from econml)\n","  Downloading sparse-0.17.0-py2.py3-none-any.whl.metadata (5.3 kB)\n","Requirement already satisfied: shap<0.49.0,>=0.38.1 in /usr/local/lib/python3.12/dist-packages (from econml) (0.48.0)\n","Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (from econml) (4.6.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from econml) (25.0)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from causal-learn>=0.1.3.0->dowhy) (0.21)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from causal-learn>=0.1.3.0->dowhy) (3.10.0)\n","Requirement already satisfied: pydot in /usr/local/lib/python3.12/dist-packages (from causal-learn>=0.1.3.0->dowhy) (3.0.4)\n","Collecting momentchi2 (from causal-learn>=0.1.3.0->dowhy)\n","  Downloading momentchi2-0.1.8-py3-none-any.whl.metadata (6.1 kB)\n","Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from cvxpy<1.5->dowhy) (1.0.4)\n","Collecting ecos>=2 (from cvxpy<1.5->dowhy)\n","  Downloading ecos-2.0.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n","Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from cvxpy<1.5->dowhy) (0.11.1)\n","Requirement already satisfied: scs>=3.0 in /usr/local/lib/python3.12/dist-packages (from cvxpy<1.5->dowhy) (3.2.8)\n","Collecting pybind11 (from cvxpy<1.5->dowhy)\n","  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.59->dowhy) (0.43.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>1.0->dowhy) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>1.0->dowhy) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>1.0->dowhy) (2025.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>1.0->dowhy) (3.6.0)\n","Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap<0.49.0,>=0.38.1->econml) (0.0.8)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap<0.49.0,>=0.38.1->econml) (3.1.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap<0.49.0,>=0.38.1->econml) (4.15.0)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.14->dowhy) (1.0.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.10.1->dowhy) (1.3.0)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.12/dist-packages (from clarabel>=0.5.0->cvxpy<1.5->dowhy) (1.17.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from osqp>=0.6.2->cvxpy<1.5->dowhy) (3.1.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from osqp>=0.6.2->cvxpy<1.5->dowhy) (75.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>1.0->dowhy) (1.17.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (4.59.2)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (3.2.3)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi->clarabel>=0.5.0->cvxpy<1.5->dowhy) (2.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->osqp>=0.6.2->cvxpy<1.5->dowhy) (3.0.2)\n","Downloading dowhy-0.13-py3-none-any.whl (399 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.2/399.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading econml-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading causal_learn-0.1.4.3-py3-none-any.whl (192 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.0/193.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cvxpy-1.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.15.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sparse-0.17.0-py2.py3-none-any.whl (259 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ecos-2.0.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.1/222.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading momentchi2-0.1.8-py3-none-any.whl (11 kB)\n","Downloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scipy, pybind11, sparse, momentchi2, ecos, cvxpy, econml, causal-learn, dowhy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.16.1\n","    Uninstalling scipy-1.16.1:\n","      Successfully uninstalled scipy-1.16.1\n","  Attempting uninstall: cvxpy\n","    Found existing installation: cvxpy 1.6.7\n","    Uninstalling cvxpy-1.6.7:\n","      Successfully uninstalled cvxpy-1.6.7\n","Successfully installed causal-learn-0.1.4.3 cvxpy-1.4.4 dowhy-0.13 econml-0.16.0 ecos-2.0.14 momentchi2-0.1.8 pybind11-3.0.1 scipy-1.15.3 sparse-0.17.0\n"]}]},{"cell_type":"code","source":["pip install pysal pwlf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"cQVR_P15FyM1","executionInfo":{"status":"ok","timestamp":1757208416984,"user_tz":-480,"elapsed":21426,"user":{"displayName":"weilin xin","userId":"14777990448514190568"}},"outputId":"60aedbe1-cdb2-47e6-8b56-f03382cc11de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pysal\n","  Downloading pysal-25.7-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: beautifulsoup4>=4.10 in /usr/local/lib/python3.12/dist-packages (from pysal) (4.13.5)\n","Requirement already satisfied: geopandas>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from pysal) (1.1.1)\n","Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from pysal) (2.0.2)\n","Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.12/dist-packages (from pysal) (25.0)\n","Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.12/dist-packages (from pysal) (2.2.2)\n","Requirement already satisfied: platformdirs>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from pysal) (4.4.0)\n","Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.12/dist-packages (from pysal) (2.32.4)\n","Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.12/dist-packages (from pysal) (1.15.3)\n","Requirement already satisfied: shapely>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from pysal) (2.1.1)\n","Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.12/dist-packages (from pysal) (1.6.1)\n","Requirement already satisfied: libpysal>=4.13.0 in /usr/local/lib/python3.12/dist-packages (from pysal) (4.13.0)\n","Collecting access>=1.1.9 (from pysal)\n","  Downloading access-1.1.9-py3-none-any.whl.metadata (2.4 kB)\n","Collecting esda>=2.7.1 (from pysal)\n","  Downloading esda-2.7.1-py3-none-any.whl.metadata (2.0 kB)\n","Collecting giddy>=2.3.6 (from pysal)\n","  Downloading giddy-2.3.6-py3-none-any.whl.metadata (6.3 kB)\n","Collecting inequality>=1.1.2 (from pysal)\n","  Downloading inequality-1.1.2-py3-none-any.whl.metadata (3.9 kB)\n","Collecting pointpats>=2.5.1 (from pysal)\n","  Downloading pointpats-2.5.2-py3-none-any.whl.metadata (4.7 kB)\n","Collecting segregation>=2.5.2 (from pysal)\n","  Downloading segregation-2.5.2-py3-none-any.whl.metadata (2.2 kB)\n","Collecting spaghetti>=1.7.6 (from pysal)\n","  Downloading spaghetti-1.7.6-py3-none-any.whl.metadata (12 kB)\n","Collecting mgwr>=2.2.1 (from pysal)\n","  Downloading mgwr-2.2.1-py3-none-any.whl.metadata (1.5 kB)\n","Collecting momepy>=0.10.0 (from pysal)\n","  Downloading momepy-0.10.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting spglm>=1.1.0 (from pysal)\n","  Downloading spglm-1.1.0-py3-none-any.whl.metadata (3.9 kB)\n","Collecting spint>=1.0.7 (from pysal)\n","  Downloading spint-1.0.7.tar.gz (28 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting spreg>=1.8.3 (from pysal)\n","  Downloading spreg-1.8.3-py3-none-any.whl.metadata (1.7 kB)\n","Collecting tobler>=0.12.1 (from pysal)\n","  Downloading tobler-0.12.1-py3-none-any.whl.metadata (1.9 kB)\n","Collecting mapclassify>=2.10.0 (from pysal)\n","  Downloading mapclassify-2.10.0-py3-none-any.whl.metadata (3.1 kB)\n","Collecting splot>=1.1.7 (from pysal)\n","  Downloading splot-1.1.7-py3-none-any.whl.metadata (8.9 kB)\n","Collecting spopt>=0.7.0 (from pysal)\n","  Downloading spopt-0.7.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.10->pysal) (2.8)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.10->pysal) (4.15.0)\n","Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas>=0.10.0->pysal) (0.11.1)\n","Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from geopandas>=0.10.0->pysal) (3.7.2)\n","Collecting quantecon>=0.7 (from giddy>=2.3.6->pysal)\n","  Downloading quantecon-0.10.1-py3-none-any.whl.metadata (5.3 kB)\n","Requirement already satisfied: matplotlib>=3.8 in /usr/local/lib/python3.12/dist-packages (from inequality>=1.1.2->pysal) (3.10.0)\n","Requirement already satisfied: networkx>=3.2 in /usr/local/lib/python3.12/dist-packages (from mapclassify>=2.10.0->pysal) (3.5)\n","Requirement already satisfied: tqdm>=4.65 in /usr/local/lib/python3.12/dist-packages (from momepy>=0.10.0->pysal) (4.67.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4->pysal) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4->pysal) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4->pysal) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27->pysal) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27->pysal) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27->pysal) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27->pysal) (2025.8.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1->pysal) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1->pysal) (3.6.0)\n","Collecting deprecation (from segregation>=2.5.2->pysal)\n","  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from segregation>=2.5.2->pysal) (0.13.2)\n","Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from segregation>=2.5.2->pysal) (0.60.0)\n","Collecting rtree>=1.0 (from spaghetti>=1.7.6->pysal)\n","  Downloading rtree-1.4.1-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n","Collecting pulp>=2.8 (from spopt>=0.7.0->pysal)\n","  Downloading pulp-3.2.2-py3-none-any.whl.metadata (6.9 kB)\n","Collecting rasterio (from tobler>=0.12.1->pysal)\n","  Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (from tobler>=0.12.1->pysal) (0.14.5)\n","Collecting rasterstats (from tobler>=0.12.1->pysal)\n","  Downloading rasterstats-0.20.0-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (4.59.2)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (3.2.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->pysal) (1.17.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from quantecon>=0.7->giddy>=2.3.6->pysal) (1.13.3)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->segregation>=2.5.2->pysal) (0.43.0)\n","Collecting affine (from rasterio->tobler>=0.12.1->pysal)\n","  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio->tobler>=0.12.1->pysal) (25.3.0)\n","Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio->tobler>=0.12.1->pysal) (8.2.1)\n","Collecting cligj>=0.5 (from rasterio->tobler>=0.12.1->pysal)\n","  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n","Collecting click-plugins (from rasterio->tobler>=0.12.1->pysal)\n","  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting fiona (from rasterstats->tobler>=0.12.1->pysal)\n","  Downloading fiona-1.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: simplejson in /usr/local/lib/python3.12/dist-packages (from rasterstats->tobler>=0.12.1->pysal) (3.20.1)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels->tobler>=0.12.1->pysal) (1.0.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->quantecon>=0.7->giddy>=2.3.6->pysal) (1.3.0)\n","Downloading pysal-25.7-py3-none-any.whl (17 kB)\n","Downloading access-1.1.9-py3-none-any.whl (21 kB)\n","Downloading esda-2.7.1-py3-none-any.whl (142 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading giddy-2.3.6-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading inequality-1.1.2-py3-none-any.whl (21 kB)\n","Downloading mapclassify-2.10.0-py3-none-any.whl (882 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m882.2/882.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mgwr-2.2.1-py3-none-any.whl (47 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading momepy-0.10.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pointpats-2.5.2-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading segregation-2.5.2-py3-none-any.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.6/141.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading spaghetti-1.7.6-py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading spglm-1.1.0-py3-none-any.whl (41 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading splot-1.1.7-py3-none-any.whl (39 kB)\n","Downloading spopt-0.7.0-py3-none-any.whl (248 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.1/248.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading spreg-1.8.3-py3-none-any.whl (389 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.9/389.9 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tobler-0.12.1-py3-none-any.whl (28 kB)\n","Downloading pulp-3.2.2-py3-none-any.whl (16.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading quantecon-0.10.1-py3-none-any.whl (325 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.7/325.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rtree-1.4.1-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (507 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.6/507.6 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n","Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rasterstats-0.20.0-py3-none-any.whl (17 kB)\n","Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n","Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n","Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n","Downloading fiona-1.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: spint\n","  Building wheel for spint (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for spint: filename=spint-1.0.7-py3-none-any.whl size=31354 sha256=784714b13eb75db15b3671a49fa4bf47cb1478fb3293a246d1f8d16e26b8ba5b\n","  Stored in directory: /root/.cache/pip/wheels/d9/82/ea/d31b72061cd6dc1760ec44a1d3e74408b1e6ca590a3361d3f9\n","Successfully built spint\n","Installing collected packages: rtree, pulp, deprecation, cligj, click-plugins, affine, rasterio, quantecon, fiona, rasterstats, mapclassify, access, tobler, spreg, segregation, pointpats, momepy, inequality, esda, spglm, spaghetti, giddy, spopt, splot, spint, mgwr, pysal\n","Successfully installed access-1.1.9 affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 deprecation-2.1.0 esda-2.7.1 fiona-1.10.1 giddy-2.3.6 inequality-1.1.2 mapclassify-2.10.0 mgwr-2.2.1 momepy-0.10.0 pointpats-2.5.2 pulp-3.2.2 pysal-25.7 quantecon-0.10.1 rasterio-1.4.3 rasterstats-0.20.0 rtree-1.4.1 segregation-2.5.2 spaghetti-1.7.6 spglm-1.1.0 spint-1.0.7 splot-1.1.7 spopt-0.7.0 spreg-1.8.3 tobler-0.12.1\n"]}]},{"cell_type":"markdown","source":["#Singapore Prediction"],"metadata":{"id":"zv3uvOnIRCXW"}},{"cell_type":"code","source":["import numpy as np\n","import geopandas as gpd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import math\n","import time\n","from tqdm import tqdm\n","import pickle\n","import rasterio\n","import rasterio.transform\n","from rasterio.mask import mask\n","import concurrent.futures\n","import multiprocessing\n","import pandas as pd\n","\n","# =============================================================================\n","# 1. 配置区 (Configuration Area)\n","# =============================================================================\n","\n","# ⭐ 请根据您的环境修改此基础路径\n","# Please modify this base path according to your environment\n","DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","\n","# 输入文件夹路径\n","# Input directory paths\n","PREDICTION_DIR = DRIVE_BASE_PATH / \"Result/0000Contrast_result/Validation/Final_Predictions_Detailed_Inpainted_Corrected\"\n","SHAPEFILE_PARENT_DIR = DRIVE_BASE_PATH / \"raw_data\"\n","# --- 新增: 新加坡边界文件路径 ---\n","# --- NEW: Path to the Singapore boundary file ---\n","BOUNDARY_GEOJSON_PATH = DRIVE_BASE_PATH / \"CASE_DATA\" / \"MasterPlan2019SubzoneBoundaryNoSeaGEOJSON.geojson\"\n","\n","\n","# 输出文件夹路径\n","# Output directory path\n","CACHE_PATH = DRIVE_BASE_PATH / \"Result/0000Contrast_result/Validation/Heat_Exposure_Reports\"\n","OUTPUT_DIR = DRIVE_BASE_PATH / \"Result/0000Contrast_result/Final_Result/Final_Singapore_UTCI_Maps_inferno\"\n","OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# 缓存文件路径\n","# Path for the spatial information cache\n","SPATIAL_CACHE_PATH = CACHE_PATH / \"spatial_info_cache.pkl\"\n","\n","\n","# 定义常量\n","# Define constants\n","TILE_SHAPE = (250, 250)  # 每个预测瓦片的像素尺寸\n","RESOLUTION = 4.0         # 每个像素代表的地理尺寸 (米)\n","CMAP = 'inferno'         # 使用的颜色图\n","\n","# --- 目标时间 ---\n","# --- Target hour ---\n","TARGET_HOUR_INDEX = 5\n","MAX_WORKERS = None # 并行处理使用的核心数 (None = all available)\n","\n","# =============================================================================\n","# 2. 辅助函数\n","# =============================================================================\n","\n","def get_or_create_spatial_info(scene_parent_dir: Path, resolution: float, cache_path: Path):\n","    \"\"\"\n","    检查空间信息缓存是否存在。如果不存在，则通过读取场景shapefile来创建它，\n","    以确定用于拼接的总边界和偏移量。\n","    \"\"\"\n","    if cache_path.exists():\n","        print(f\"--- 成功从缓存加载空间信息: {cache_path.name} ---\")\n","        with open(cache_path, 'rb') as f:\n","            # 确保即使缓存中包含旧的'id_int'字段也能正常工作\n","            return pickle.load(f)\n","\n","    print(\"--- 未找到缓存，首次运行：正在创建空间映射信息 ---\")\n","    shp_files = [p for p in scene_parent_dir.glob(\"*/*.shp\") if p.parent.name.isdigit() and p.stem.startswith('surface_')]\n","    if not shp_files:\n","        raise FileNotFoundError(f\"在 '{scene_parent_dir}' 的纯数字子目录中未找到任何 'surface_*.shp' 文件。\")\n","\n","    print(f\"正在从 {len(shp_files)} 个场景shapefile中计算总边界...\")\n","    first_gdf = gpd.read_file(shp_files[0])\n","    target_crs = first_gdf.crs\n","\n","    all_bounds_list = [gpd.read_file(p).total_bounds for p in tqdm(shp_files, desc=\"读取SHP元数据\")]\n","    bounds_df = pd.DataFrame(all_bounds_list, columns=['minx', 'miny', 'maxx', 'maxy'])\n","\n","    min_x, min_y = bounds_df['minx'].min(), bounds_df['miny'].min()\n","    max_x, max_y = bounds_df['maxx'].max(), bounds_df['maxy'].max()\n","\n","    canvas_width = int(np.ceil((max_x - min_x) / resolution))\n","    canvas_height = int(np.ceil((max_y - min_y) / resolution))\n","    canvas_transform = rasterio.transform.from_origin(min_x, max_y, resolution, resolution)\n","\n","    scene_mappings = {}\n","    for shp_path in tqdm(shp_files, desc=\"计算场景偏移\"):\n","        scene_id_str = shp_path.stem.split('_')[-1]\n","        patch_min_x, _, _, patch_max_y = gpd.read_file(shp_path).total_bounds\n","        row_offset, col_offset = rasterio.transform.rowcol(canvas_transform, patch_min_x, patch_max_y)\n","        scene_mappings[scene_id_str] = {\n","            'row_offset': row_offset,\n","            'col_offset': col_offset,\n","        }\n","\n","    spatial_info = {\n","        'canvas_height': canvas_height, 'canvas_width': canvas_width,\n","        'canvas_transform': canvas_transform, 'scene_mappings': scene_mappings,\n","        'crs': target_crs\n","    }\n","\n","    with open(cache_path, 'wb') as f:\n","        pickle.dump(spatial_info, f)\n","    print(f\"空间信息已成功计算并缓存至: {cache_path}\")\n","    return spatial_info\n","\n","def process_tile(scene_id, prediction_dir, target_hour_index):\n","    \"\"\"\n","    加载单个瓦片数据并提取目标小时的数据（用于并行处理）。\n","    \"\"\"\n","    try:\n","        npy_path = prediction_dir / f\"prediction_{scene_id}.npy\"\n","        if not npy_path.exists():\n","            return scene_id, None\n","        tile_data_all_hours = np.load(npy_path)\n","        tile_data_target_hour = tile_data_all_hours[:, :, target_hour_index]\n","        return scene_id, tile_data_target_hour\n","    except Exception as e:\n","        print(f\"处理瓦片 {scene_id} 时出错: {e}\")\n","        return scene_id, None\n","\n","# =============================================================================\n","# 3. 主执行逻辑 (无平滑，直接拼接)\n","# =============================================================================\n","\n","def main():\n","    \"\"\"\n","    主函数，执行所有阶段的任务。\n","    \"\"\"\n","    total_start_time = time.time()\n","\n","    # --- Phase 1: 资产准备与地理空间定位 ---\n","    print(\"\\n--- Phase 1: 资产准备与地理空间定位 ---\")\n","    spatial_info = get_or_create_spatial_info(SHAPEFILE_PARENT_DIR, RESOLUTION, SPATIAL_CACHE_PATH)\n","    prediction_files = {p.stem.split('_')[-1]: p for p in PREDICTION_DIR.glob(\"prediction_*.npy\")}\n","    common_scene_ids = sorted(list(prediction_files.keys() & spatial_info['scene_mappings'].keys()))\n","    if not common_scene_ids:\n","        raise FileNotFoundError(\"在预测目录和空间缓存中找不到任何匹配的场景文件。\")\n","    print(f\"成功找到 {len(common_scene_ids)} 个匹配的场景瓦片。\")\n","\n","    # --- Phase 2: 构建“原始拼接图” ---\n","    print(f\"\\n--- Phase 2: 拼接中午12点 (T+{TARGET_HOUR_INDEX}) 的数据 (无平滑) ---\")\n","    canvas_height = spatial_info['canvas_height']\n","    canvas_width = spatial_info['canvas_width']\n","\n","    # 创建原始值画布\n","    raw_canvas = np.full((canvas_height, canvas_width), np.nan, dtype=np.float32)\n","\n","    # 使用并行处理来加速文件读取\n","    with concurrent.futures.ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n","        futures = {\n","            executor.submit(process_tile, scene_id, PREDICTION_DIR, TARGET_HOUR_INDEX): scene_id\n","            for scene_id in common_scene_ids\n","        }\n","\n","        for future in tqdm(concurrent.futures.as_completed(futures), total=len(common_scene_ids), desc=\"并行读取并直接拼接瓦片\"):\n","            scene_id, tile_data = future.result()\n","\n","            if tile_data is None:\n","                continue\n","\n","            mapping_info = spatial_info['scene_mappings'][scene_id]\n","            row_offset, col_offset = mapping_info['row_offset'], mapping_info['col_offset']\n","\n","            y_slice = slice(row_offset, row_offset + TILE_SHAPE[0])\n","            x_slice = slice(col_offset, col_offset + TILE_SHAPE[1])\n","\n","            # 直接将数据“粘贴”到画布上\n","            raw_canvas[y_slice, x_slice] = tile_data.astype(np.float32)\n","\n","    print(\"原始拼接图构建完成。\")\n","\n","    # 将原始画布作为最终地图\n","    final_stitched_map = raw_canvas\n","    final_stitched_map[np.abs(final_stitched_map) < 1e-9] = np.nan\n","\n","    # --- Phase 3: 使用GeoJSON边界裁剪画布 ---\n","    print(\"\\n--- Phase 3: 使用GeoJSON边界裁剪画布 ---\")\n","    if not BOUNDARY_GEOJSON_PATH.exists():\n","        print(f\"警告: 找不到边界文件 {BOUNDARY_GEOJSON_PATH}，将跳过裁剪步骤。\")\n","        clipped_canvas = final_stitched_map\n","    else:\n","        print(f\"正在加载边界文件: {BOUNDARY_GEOJSON_PATH.name}\")\n","        boundary_gdf = gpd.read_file(BOUNDARY_GEOJSON_PATH)\n","        if boundary_gdf.crs != spatial_info['crs']:\n","            print(f\"正在将边界文件的CRS从 {boundary_gdf.crs} 转换为 {spatial_info['crs']}...\")\n","            boundary_gdf = boundary_gdf.to_crs(spatial_info['crs'])\n","\n","        print(\"正在根据边界进行裁剪...\")\n","        # 使用rasterio进行裁剪\n","        with rasterio.io.MemoryFile() as memfile:\n","            profile = {\n","                'driver': 'GTiff', 'height': final_stitched_map.shape[0], 'width': final_stitched_map.shape[1],\n","                'count': 1, 'dtype': final_stitched_map.dtype, 'crs': spatial_info['crs'],\n","                'transform': spatial_info['canvas_transform'], 'nodata': np.nan\n","            }\n","            with memfile.open(**profile) as dataset:\n","                dataset.write(final_stitched_map, 1)\n","            with memfile.open() as dataset:\n","                clipped_array, _ = mask(dataset, shapes=boundary_gdf.geometry, crop=False, invert=False, nodata=np.nan)\n","        clipped_canvas = clipped_array.squeeze()\n","        print(\"裁剪完成。\")\n","\n","    # --- Phase 4: 可视化与导出 ---\n","    print(\"\\n--- Phase 4: 生成并保存最终图像 ---\")\n","    vmin = np.nanmin(clipped_canvas)\n","    vmax = np.nanmax(clipped_canvas)\n","    print(f\"已确定颜色范围: vmin={vmin:.2f}, vmax={vmax:.2f}\")\n","\n","    aspect_ratio = canvas_width / canvas_height if canvas_height > 0 else 1\n","    fig_width = 15\n","    fig_height = fig_width / aspect_ratio\n","    fig, ax = plt.subplots(figsize=(fig_width, fig_height), dpi=300)\n","\n","    cmap = plt.get_cmap(CMAP).copy()\n","    cmap.set_bad(color='white', alpha=1.0) # 将NaN值设置为白色\n","\n","    im = ax.imshow(clipped_canvas, cmap=cmap, vmin=vmin, vmax=vmax)\n","    ax.axis('off')\n","    fig.suptitle(f\"Singapore Predicted UTCI at 12 PM (Raw Stitched - {CMAP.capitalize()})\", fontsize=16, y=0.95)\n","\n","    cbar = fig.colorbar(im, ax=ax, fraction=0.046 * (1/aspect_ratio), pad=0.04)\n","    cbar.set_label('Predicted Universal Thermal Climate Index (UTCI)')\n","\n","    plt.tight_layout(rect=[0, 0, 1, 0.93])\n","\n","    output_filepath = OUTPUT_DIR / f\"singapore_utci_12pm_raw_stitched_clipped.png\"\n","    plt.savefig(output_filepath, bbox_inches='tight', pad_inches=0.1)\n","    plt.show()\n","    plt.close(fig)\n","    print(f\"最终图像已保存至: {output_filepath}\")\n","\n","    total_duration = time.time() - total_start_time\n","    print(f\"\\n所有任务已完成！总耗时: {total_duration:.2f} 秒。\")\n","\n","if __name__ == \"__main__\":\n","    # 在某些环境（如macOS或Linux的某些配置）中，'fork'是必要的\n","    # 如果遇到问题，可以尝试移除此段\n","    try:\n","        multiprocessing.set_start_method('fork', force=True)\n","    except (ValueError, RuntimeError):\n","        # 如果系统不支持'fork'或已设置，则忽略\n","        pass\n","\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1rTm7H3JxZs6mWiqUmcI6HqLoe2fqNhVv"},"id":"oqtVm161AkzJ","executionInfo":{"status":"ok","timestamp":1753842573422,"user_tz":-480,"elapsed":86403,"user":{"displayName":"weilin xin","userId":"14777990448514190568"}},"outputId":"0d1750e9-fbad-43d8-f37f-9ba77f1eb629"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["#UTCI-Scale Analysis"],"metadata":{"id":"oGhHinFqU_K_"}},{"cell_type":"markdown","source":["###Data Collection"],"metadata":{"id":"PRP0m_1rVEsy"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from tqdm import tqdm\n","import time\n","from skimage.measure import block_reduce\n","import warnings\n","\n","# =============================================================================\n","# 1. Configuration Area\n","# =============================================================================\n","print(\"--- 1. 配置路径和参数 ---\")\n","\n","# ⭐ 请确保此基础路径与您的环境一致\n","DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","\n","# --- 输入路径 ---\n","# 存放裁剪后全时序地图的文件夹\n","MASKED_MAP_DIR = DRIVE_BASE_PATH / \"Result/0000Contrast_result/Scale_Analysis/full_timeseries\"\n","\n","# --- 输出路径 ---\n","# 为每个尺度的数据创建一个新的专用文件夹\n","PER_SCALE_OUTPUT_DIR = DRIVE_BASE_PATH / \"Result/0000Contrast_result/Scale_Analysis/per_scale_data\"\n","PER_SCALE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# --- 参数 ---\n","BASE_RESOLUTION = 4.0  # 基础分辨率 (米)\n","NUM_HOURS = 12         # UTCI数据的小时数\n","\n","# --- 尺度序列生成 ---\n","# 生成一个从8米到4000米，包含20个元素的几何序列\n","# np.geomspace比线性序列更适合尺度分析\n","scales_geom = np.geomspace(8, 4000, num=20)\n","\n","# 将每个尺度值四舍五入到最接近的4的倍数，以确保聚合因子为整数\n","# 并确保唯一性，避免重复处理\n","SCALES = sorted(list(set(np.round(scales_geom / BASE_RESOLUTION) * BASE_RESOLUTION)))\n","SCALES = [int(s) for s in SCALES] # 转换为整数\n","\n","print(\"\\n--- 将要处理的尺度序列 (米) ---\")\n","print(SCALES)\n","print(f\"总共将生成 {len(SCALES)} 个尺度的文件。\")\n","\n","\n","# =============================================================================\n","# 2. Main Execution Logic\n","# =============================================================================\n","\n","def generate_per_scale_data():\n","    \"\"\"\n","    主函数，加载4米分辨率数据，然后循环遍历每个尺度，\n","    聚合特征和UTCI时间序列，并为每个尺度保存一个单独的.pkl文件。\n","    \"\"\"\n","    total_start_time = time.time()\n","\n","    # --- 阶段 2.1: 加载裁剪后的4米分辨率数据 ---\n","    print(\"\\n--- 2.1. 加载裁剪后的4米分辨率地图 ---\")\n","    utci_path = MASKED_MAP_DIR / \"masked_utci_timeseries.npy\"\n","    features_path = MASKED_MAP_DIR / \"masked_features.npz\"\n","\n","    if not utci_path.exists() or not features_path.exists():\n","        raise FileNotFoundError(f\"错误：在 {MASKED_MAP_DIR} 中找不到必要的输入文件。请先运行之前的脚本。\")\n","\n","    utci_map_3d_4m = np.load(utci_path)\n","    features_4m = np.load(features_path)\n","    tree_height_map_4m = features_4m['tree_height']\n","    landcover_map_4m = features_4m['landcover']\n","    building_height_map_4m = features_4m['building_height']\n","    print(\"数据加载成功。\")\n","    print(f\"UTCI地图形状: {utci_map_3d_4m.shape}\")\n","\n","    # --- 阶段 2.2: 创建用于计算占比的二元特征蒙版 (一次性计算) ---\n","    print(\"\\n--- 2.2. 创建静态的二元特征蒙版 ---\")\n","    # 使用nan_to_num处理NaN值，以便进行逻辑运算\n","    tree_height_no_nan = np.nan_to_num(tree_height_map_4m)\n","    building_height_no_nan = np.nan_to_num(building_height_map_4m)\n","    landcover_no_nan = np.nan_to_num(landcover_map_4m)\n","\n","    # 创建一个字典来存储所有的二元蒙版\n","    masks = {\n","        'tree_proportion': (tree_height_no_nan > 0).astype(np.float32),\n","        'building_proportion': (building_height_no_nan > 0).astype(np.float32),\n","        'st1_proportion': (landcover_no_nan == 1).astype(np.float32), # 假设1是某个地表类型\n","        'st2_proportion': (landcover_no_nan == 2).astype(np.float32),\n","        'st3_proportion': (landcover_no_nan == 3).astype(np.float32),\n","        'st5_proportion': (landcover_no_nan == 5).astype(np.float32),\n","    }\n","\n","    # 关键一步：将原始数据中的无效区域(NaN)在蒙版中也设为NaN\n","    # 这样在计算均值时，block_reduce可以正确地忽略这些区域\n","    # np.isnan()可以检查原始数组中的NaN位置\n","    invalid_mask = np.isnan(landcover_map_4m)\n","    for key in masks:\n","        masks[key][invalid_mask] = np.nan\n","    print(\"特征蒙版创建完成。\")\n","\n","\n","    # --- 阶段 2.3: 循环聚合每个尺度 ---\n","    print(f\"\\n--- 2.3. 开始循环聚合，并将结果保存到: {PER_SCALE_OUTPUT_DIR} ---\")\n","\n","    # 忽略在处理全为NaN的块时遇到的RuntimeWarning\n","    with warnings.catch_warnings():\n","        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n","\n","        for scale in tqdm(SCALES, desc=\"聚合尺度进度\"):\n","            factor = int(scale / BASE_RESOLUTION)\n","            block_shape = (factor, factor)\n","\n","            # --- a) 聚合静态特征 ---\n","            # 高度均值\n","            mean_tree_height = block_reduce(tree_height_map_4m, block_shape, func=np.nanmean)\n","            mean_building_height = block_reduce(building_height_map_4m, block_shape, func=np.nanmean)\n","\n","            # 占比\n","            aggregated_proportions = {}\n","            for key, mask_array in masks.items():\n","                aggregated_proportions[key] = block_reduce(mask_array, block_shape, func=np.nanmean)\n","\n","            # --- b) 聚合逐时UTCI ---\n","            aggregated_utci_list = []\n","            for hour in range(NUM_HOURS):\n","                utci_slice_4m = utci_map_3d_4m[:, :, hour]\n","                agg_utci_slice = block_reduce(utci_slice_4m, block_shape, func=np.nanmean)\n","                aggregated_utci_list.append(agg_utci_slice)\n","\n","            # 将12个聚合后的2D数组堆叠成一个3D数组\n","            aggregated_utci_3d = np.stack(aggregated_utci_list, axis=-1)\n","\n","            # --- c) 组织并保存当前尺度的数据 ---\n","            scale_data_output = {\n","                'scale_meters': scale,\n","                'base_resolution_meters': BASE_RESOLUTION,\n","                'aggregation_factor': factor,\n","                'features': {\n","                    'mean_tree_height': mean_tree_height,\n","                    'mean_building_height': mean_building_height,\n","                    **aggregated_proportions # 使用字典解包合并比例数据\n","                },\n","                'utci_timeseries': aggregated_utci_3d\n","            }\n","\n","            # 定义输出文件名，使用zfill补零以方便文件排序\n","            output_filename = f\"scale_data_{str(scale).zfill(4)}m.pkl\"\n","            output_path = PER_SCALE_OUTPUT_DIR / output_filename\n","\n","            with open(output_path, 'wb') as f:\n","                pd.to_pickle(scale_data_output, f)\n","\n","    total_duration = time.time() - total_start_time\n","    print(f\"\\n🎉 全部完成！所有尺度的聚合数据已成功保存。总耗时: {total_duration:.2f} 秒。\")\n","\n","\n","# =============================================================================\n","# 3. Execution\n","# =============================================================================\n","if __name__ == \"__main__\":\n","    generate_per_scale_data()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYXnYk3kVGFK","executionInfo":{"status":"ok","timestamp":1756379634984,"user_tz":-480,"elapsed":608341,"user":{"displayName":"weilin xin","userId":"14777990448514190568"}},"outputId":"91a2171f-1bbb-4c9a-9773-e33a264ea4d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- 1. 配置路径和参数 ---\n","\n","--- 将要处理的尺度序列 (米) ---\n","[8, 12, 16, 20, 28, 40, 56, 80, 108, 152, 212, 292, 404, 564, 780, 1080, 1500, 2080, 2884, 4000]\n","总共将生成 20 个尺度的文件。\n","\n","--- 2.1. 加载裁剪后的4米分辨率地图 ---\n","数据加载成功。\n","UTCI地图形状: (8000, 13500, 12)\n","\n","--- 2.2. 创建静态的二元特征蒙版 ---\n","特征蒙版创建完成。\n","\n","--- 2.3. 开始循环聚合，并将结果保存到: /content/drive/MyDrive/Colab Notebooks/Graph Data Process/Result/0000Contrast_result/Scale_Analysis/per_scale_data ---\n"]},{"output_type":"stream","name":"stderr","text":["聚合尺度进度: 100%|██████████| 20/20 [08:59<00:00, 26.95s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🎉 全部完成！所有尺度的聚合数据已成功保存。总耗时: 604.66 秒。\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pickle\n","from pathlib import Path\n","from tqdm import tqdm\n","import os\n","import time\n","import geopandas as gpd\n","import rasterio\n","from rasterio.features import rasterize\n","import concurrent.futures\n","from skimage.measure import block_reduce\n","import warnings\n","\n","# =============================================================================\n","# 1. MASTER CONFIGURATION\n","# =============================================================================\n","print(\"--- 1. 配置主路径和待处理变量 ---\")\n","\n","# ⭐ 1. 设置您的项目基础路径 (Set your project base path)\n","DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","\n","# ⭐ 2. 将所有需要处理的目标变量添加到此列表中\n","#    (Add all target variables you want to process to this list)\n","#    脚本将为列表中的每个变量自动执行完整的处理流程。\n","VARIABLES_TO_PROCESS = ['air_temp', 'wind_speed', 'relative_humidity', 'MRT']\n","\n","# ⭐ 3. (可选) 配置并行处理的核心数\n","#    None 表示使用所有可用的核心\n","MAX_WORKERS = None\n","\n","# =============================================================================\n","# 2. Part 1: Stitching and Masking Function (拼接与裁剪函数)\n","# =============================================================================\n","\n","def preprocess_variable(target_variable, base_path):\n","    \"\"\"\n","    为指定的目标变量执行数据拼接和裁剪。\n","    此函数整合了之前处理UTCI的两个脚本的核心逻辑。\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*80)\n","    print(f\"▶️ [Phase 1] 开始为 '{target_variable}' 进行拼接与裁剪...\")\n","    print(\"=\"*80)\n","\n","    # --- 动态路径配置 ---\n","    prediction_dir = base_path / f\"Result/0000Contrast_result/Final_effectindex/Validation/cleaned_predictions_{target_variable}\"\n","    output_dir = base_path / f\"Result/0000Contrast_result/Scale_Analysis/full_timeseries_{target_variable}\"\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # 静态文件路径\n","    spatial_cache_path = base_path / \"Result/0000Contrast_result/Final_effectindex/Final_Result/Final_Result_air_temp/spatial_info_cache.pkl\"\n","    input_feature_dir = base_path / \"Alldata/inputnew\"\n","    boundary_geojson_path = base_path / \"CASE_DATA\" / \"MasterPlan2019SubzoneBoundaryNoSeaGEOJSON.geojson\"\n","\n","    # --- 2.1: Stitching (数据拼接) ---\n","    print(f\"--- [1/2] 正在拼接 '{target_variable}' 的瓦片数据 ---\")\n","    stitching_start_time = time.time()\n","\n","    if not prediction_dir.exists():\n","        print(f\"❌ 错误: 找不到预测目录 {prediction_dir}。请确保已为 '{target_variable}' 生成了预测数据。\")\n","        return False\n","\n","    with open(spatial_cache_path, 'rb') as f:\n","        spatial_info = pickle.load(f)\n","    canvas_height, canvas_width = spatial_info['canvas_height'], spatial_info['canvas_width']\n","\n","    variable_canvas_3d = np.full((canvas_height, canvas_width, 12), np.nan, dtype=np.float32)\n","    tree_height_canvas = np.full((canvas_height, canvas_width), np.nan, dtype=np.float32)\n","    landcover_canvas = np.full((canvas_height, canvas_width), np.nan, dtype=np.float32)\n","    building_height_canvas = np.full((canvas_height, canvas_width), np.nan, dtype=np.float32)\n","\n","    prediction_files = {p.stem.split('_')[-1] for p in prediction_dir.glob(\"prediction_*.npy\")}\n","    input_files = {p.stem.split('_')[-1] for p in input_feature_dir.glob(\"Input_*.npy\")}\n","    common_scene_ids = sorted(list(prediction_files & input_files & spatial_info['scene_mappings'].keys()))\n","\n","    def process_tile(scene_id):\n","        mapping_info = spatial_info['scene_mappings'][scene_id]\n","        y_slice = slice(mapping_info['row_offset'], mapping_info['row_offset'] + 250)\n","        x_slice = slice(mapping_info['col_offset'], mapping_info['col_offset'] + 250)\n","\n","        pred_path = prediction_dir / f\"prediction_{scene_id}.npy\"\n","        variable_canvas_3d[y_slice, x_slice, :] = np.load(pred_path)\n","\n","        input_path = input_feature_dir / f\"Input_{scene_id}.npy\"\n","        input_data = np.load(input_path)\n","        tree_height_canvas[y_slice, x_slice] = input_data[:, :, 0]\n","        landcover_canvas[y_slice, x_slice] = input_data[:, :, 1]\n","        building_height_canvas[y_slice, x_slice] = input_data[:, :, 2]\n","\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n","        list(tqdm(executor.map(process_tile, common_scene_ids), total=len(common_scene_ids), desc=f\"Stitching {target_variable}\"))\n","\n","    print(f\"拼接完成，耗时: {time.time() - stitching_start_time:.2f} 秒。\")\n","\n","    # --- 2.2: Masking (数据裁剪) ---\n","    print(f\"--- [2/2] 正在应用地理蒙版到 '{target_variable}' ---\")\n","    masking_start_time = time.time()\n","\n","    boundary_gdf = gpd.read_file(boundary_geojson_path).to_crs(spatial_info['crs'])\n","    boundary_mask = rasterize(\n","        shapes=[(geom, 1) for geom in boundary_gdf.geometry],\n","        out_shape=(canvas_height, canvas_width),\n","        transform=spatial_info['canvas_transform'],\n","        fill=0,\n","        dtype='uint8'\n","    ).astype(bool)\n","\n","    is_building_mask = building_height_canvas > 0\n","    variable_valid_mask = boundary_mask & ~is_building_mask\n","\n","    masked_tree_height = np.where(boundary_mask, tree_height_canvas, np.nan)\n","    masked_landcover = np.where(boundary_mask, landcover_canvas, np.nan)\n","    masked_building_height = np.where(boundary_mask, building_height_canvas, np.nan)\n","    masked_variable_3d = np.where(variable_valid_mask[:, :, np.newaxis], variable_canvas_3d, np.nan)\n","\n","    # --- 保存结果 ---\n","    variable_output_path = output_dir / \"masked_variable_timeseries.npy\"\n","    np.save(variable_output_path, masked_variable_3d)\n","\n","    features_output_path = output_dir / \"masked_features.npz\"\n","    np.savez_compressed(features_output_path, tree_height=masked_tree_height,\n","                        landcover=masked_landcover, building_height=masked_building_height)\n","\n","    print(f\"裁剪完成，耗时: {time.time() - masking_start_time:.2f} 秒。\")\n","    print(f\"✅ [Phase 1] '{target_variable}' 的预处理数据已保存至: {output_dir}\")\n","    return True\n","\n","# =============================================================================\n","# 3. Part 2: Multi-Scale Aggregation Function (多尺度聚合函数)\n","# =============================================================================\n","\n","def aggregate_to_multiscale(target_variable, base_path):\n","    \"\"\"\n","    为指定的目标变量执行多尺度数据聚合。\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*80)\n","    print(f\"▶️ [Phase 2] 开始为 '{target_variable}' 进行多尺度聚合...\")\n","    print(\"=\"*80)\n","\n","    # --- 动态路径配置 ---\n","    input_dir = base_path / f\"Result/0000Contrast_result/Scale_Analysis/full_timeseries_{target_variable}\"\n","    output_dir = base_path / f\"Result/0000Contrast_result/Scale_Analysis/per_scale_data_{target_variable}\"\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # --- 参数 ---\n","    scale_series_m = np.geomspace(8, 4000, 20)\n","    scale_series_m = np.unique((np.round(scale_series_m / 4) * 4).astype(int))\n","    base_resolution = 4.0\n","\n","    # --- 3.1: 加载数据 ---\n","    variable_path = input_dir / \"masked_variable_timeseries.npy\"\n","    features_path = input_dir / \"masked_features.npz\"\n","\n","    variable_map_3d_4m = np.load(variable_path)\n","    features_4m = np.load(features_path)\n","    tree_height_map_4m = features_4m['tree_height']\n","    landcover_map_4m = features_4m['landcover']\n","    building_height_map_4m = features_4m['building_height']\n","\n","    # --- 3.2: 创建蒙版 ---\n","    masks = {\n","        'tree_proportion': (np.nan_to_num(tree_height_map_4m) > 0).astype(np.float32),\n","        'building_proportion': (np.nan_to_num(building_height_map_4m) > 0).astype(np.float32),\n","        'st1_proportion': (np.nan_to_num(landcover_map_4m) == 1).astype(np.float32),\n","        'st2_proportion': (np.nan_to_num(landcover_map_4m) == 2).astype(np.float32),\n","        'st3_proportion': (np.nan_to_num(landcover_map_4m) == 3).astype(np.float32),\n","        'st5_proportion': (np.nan_to_num(landcover_map_4m) == 5).astype(np.float32)\n","    }\n","    for key in masks:\n","        masks[key][np.isnan(landcover_map_4m)] = np.nan\n","\n","    # --- 3.3: 循环聚合 ---\n","    with warnings.catch_warnings():\n","        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n","        for scale in tqdm(scale_series_m, desc=f\"Aggregating {target_variable}\"):\n","            factor = int(scale / base_resolution)\n","            block_shape = (factor, factor)\n","\n","            # 聚合静态特征\n","            aggregated_features = {\n","                'mean_tree_height': block_reduce(tree_height_map_4m, block_shape, func=np.nanmean),\n","                'mean_building_height': block_reduce(building_height_map_4m, block_shape, func=np.nanmean)\n","            }\n","            for key, mask_array in masks.items():\n","                aggregated_features[key] = block_reduce(mask_array, block_shape, func=np.nanmean)\n","\n","            # 聚合目标变量的时间序列\n","            agg_variable_list = [block_reduce(variable_map_3d_4m[:, :, hour], block_shape, func=np.nanmean) for hour in range(12)]\n","            aggregated_variable_3d = np.stack(agg_variable_list, axis=-1)\n","\n","            # --- 组织并保存结果 ---\n","            scale_data_output = {\n","                'variable_name': target_variable,\n","                'scale_meters': scale,\n","                'variable_timeseries': aggregated_variable_3d,\n","                'features': aggregated_features\n","            }\n","\n","            output_filename = output_dir / f\"scale_data_{scale:04d}m.pkl\"\n","            with open(output_filename, 'wb') as f:\n","                pickle.dump(scale_data_output, f)\n","\n","    print(f\"✅ [Phase 2] '{target_variable}' 的多尺度数据已保存至: {output_dir}\")\n","    return True\n","\n","# =============================================================================\n","# 4. Master Orchestration (主控流程)\n","# =============================================================================\n","\n","if __name__ == \"__main__\":\n","    main_start_time = time.time()\n","    print(\"🚀 开始执行多因子数据生成流水线...\")\n","    print(f\"将要处理的变量: {', '.join(VARIABLES_TO_PROCESS)}\")\n","\n","    for variable in VARIABLES_TO_PROCESS:\n","        # --- Phase 1 ---\n","        success_phase1 = preprocess_variable(target_variable=variable, base_path=DRIVE_BASE_PATH)\n","\n","        if not success_phase1:\n","            print(f\"❌ 由于预处理阶段失败，已跳过 '{variable}' 的后续步骤。\")\n","            continue\n","\n","        # --- Phase 2 ---\n","        success_phase2 = aggregate_to_multiscale(target_variable=variable, base_path=DRIVE_BASE_PATH)\n","\n","        if not success_phase2:\n","            print(f\"❌ '{variable}' 的多尺度聚合阶段出现问题。\")\n","            continue\n","\n","    total_duration = time.time() - main_start_time\n","    print(\"\\n\" + \"*\"*80)\n","    print(f\"🎉 所有变量的数据生成流程全部完成！总耗时: {total_duration / 60:.2f} 分钟。\")\n","    print(\"现在您可以为每个变量运行后续的分析脚本了。\")\n","    print(\"*\"*80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756457437714,"user_tz":-480,"elapsed":3801735,"user":{"displayName":"weilin xin","userId":"14777990448514190568"}},"outputId":"e7e1e878-d7a7-4aa3-9710-e29a35ae2c13","id":"N6R3kkptwFS0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- 1. 配置主路径和待处理变量 ---\n","🚀 开始执行多因子数据生成流水线...\n","将要处理的变量: air_temp, wind_speed, relative_humidity, MRT\n","\n","================================================================================\n","▶️ [Phase 1] 开始为 'air_temp' 进行拼接与裁剪...\n","================================================================================\n","--- [1/2] 正在拼接 'air_temp' 的瓦片数据 ---\n"]},{"output_type":"stream","name":"stderr","text":["Stitching air_temp: 100%|██████████| 936/936 [02:58<00:00,  5.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["拼接完成，耗时: 185.31 秒。\n","--- [2/2] 正在应用地理蒙版到 'air_temp' ---\n","裁剪完成，耗时: 42.59 秒。\n","✅ [Phase 1] 'air_temp' 的预处理数据已保存至: /content/drive/MyDrive/Colab Notebooks/Graph Data Process/Result/0000Contrast_result/Scale_Analysis/full_timeseries_air_temp\n","\n","================================================================================\n","▶️ [Phase 2] 开始为 'air_temp' 进行多尺度聚合...\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Aggregating air_temp: 100%|██████████| 20/20 [06:18<00:00, 18.92s/it]\n"]},{"output_type":"stream","name":"stdout","text":["✅ [Phase 2] 'air_temp' 的多尺度数据已保存至: /content/drive/MyDrive/Colab Notebooks/Graph Data Process/Result/0000Contrast_result/Scale_Analysis/per_scale_data_air_temp\n","\n","================================================================================\n","▶️ [Phase 1] 开始为 'wind_speed' 进行拼接与裁剪...\n","================================================================================\n","--- [1/2] 正在拼接 'wind_speed' 的瓦片数据 ---\n"]},{"output_type":"stream","name":"stderr","text":["Stitching wind_speed: 100%|██████████| 936/936 [09:42<00:00,  1.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["拼接完成，耗时: 583.67 秒。\n","--- [2/2] 正在应用地理蒙版到 'wind_speed' ---\n","裁剪完成，耗时: 42.97 秒。\n","✅ [Phase 1] 'wind_speed' 的预处理数据已保存至: /content/drive/MyDrive/Colab Notebooks/Graph Data Process/Result/0000Contrast_result/Scale_Analysis/full_timeseries_wind_speed\n","\n","================================================================================\n","▶️ [Phase 2] 开始为 'wind_speed' 进行多尺度聚合...\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Aggregating wind_speed: 100%|██████████| 20/20 [06:15<00:00, 18.76s/it]\n"]},{"output_type":"stream","name":"stdout","text":["✅ [Phase 2] 'wind_speed' 的多尺度数据已保存至: /content/drive/MyDrive/Colab Notebooks/Graph Data Process/Result/0000Contrast_result/Scale_Analysis/per_scale_data_wind_speed\n","\n","================================================================================\n","▶️ [Phase 1] 开始为 'relative_humidity' 进行拼接与裁剪...\n","================================================================================\n","--- [1/2] 正在拼接 'relative_humidity' 的瓦片数据 ---\n"]},{"output_type":"stream","name":"stderr","text":["Stitching relative_humidity: 100%|██████████| 936/936 [19:42<00:00,  1.26s/it]\n"]},{"output_type":"stream","name":"stdout","text":["拼接完成，耗时: 1183.30 秒。\n","--- [2/2] 正在应用地理蒙版到 'relative_humidity' ---\n","裁剪完成，耗时: 52.99 秒。\n","✅ [Phase 1] 'relative_humidity' 的预处理数据已保存至: /content/drive/MyDrive/Colab Notebooks/Graph Data Process/Result/0000Contrast_result/Scale_Analysis/full_timeseries_relative_humidity\n","\n","================================================================================\n","▶️ [Phase 2] 开始为 'relative_humidity' 进行多尺度聚合...\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Aggregating relative_humidity: 100%|██████████| 20/20 [06:16<00:00, 18.83s/it]\n"]},{"output_type":"stream","name":"stdout","text":["✅ [Phase 2] 'relative_humidity' 的多尺度数据已保存至: /content/drive/MyDrive/Colab Notebooks/Graph Data Process/Result/0000Contrast_result/Scale_Analysis/per_scale_data_relative_humidity\n","\n","================================================================================\n","▶️ [Phase 1] 开始为 'MRT' 进行拼接与裁剪...\n","================================================================================\n","--- [1/2] 正在拼接 'MRT' 的瓦片数据 ---\n"]},{"output_type":"stream","name":"stderr","text":["Stitching MRT: 100%|██████████| 936/936 [00:44<00:00, 20.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["拼接完成，耗时: 45.71 秒。\n","--- [2/2] 正在应用地理蒙版到 'MRT' ---\n","裁剪完成，耗时: 51.17 秒。\n","✅ [Phase 1] 'MRT' 的预处理数据已保存至: /content/drive/MyDrive/Colab Notebooks/Graph Data Process/Result/0000Contrast_result/Scale_Analysis/full_timeseries_MRT\n","\n","================================================================================\n","▶️ [Phase 2] 开始为 'MRT' 进行多尺度聚合...\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Aggregating MRT: 100%|██████████| 20/20 [07:01<00:00, 21.09s/it]"]},{"output_type":"stream","name":"stdout","text":["✅ [Phase 2] 'MRT' 的多尺度数据已保存至: /content/drive/MyDrive/Colab Notebooks/Graph Data Process/Result/0000Contrast_result/Scale_Analysis/per_scale_data_MRT\n","\n","********************************************************************************\n","🎉 所有变量的数据生成流程全部完成！总耗时: 63.36 分钟。\n","现在您可以为每个变量运行后续的分析脚本了。\n","********************************************************************************\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["###Quantify the thermal effects of different landscape features at different spatiotemporal scales"],"metadata":{"id":"I-bZQz-GYqWo"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from tqdm import tqdm\n","import time\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","import warnings\n","\n","# =============================================================================\n","# 1. Configuration Area\n","# =============================================================================\n","print(\"--- 1. Configuring Paths and Parameters ---\")\n","\n","# ⭐ Please ensure this base path is correct for your environment\n","DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","\n","# --- Input Paths ---\n","# Directory for the aggregated data per scale (generated by the previous script)\n","PER_SCALE_DATA_DIR = DRIVE_BASE_PATH / \"Result/0000Contrast_result/Scale_Analysis/per_scale_data_UTCI\"\n","\n","# --- Output Paths ---\n","# Create new directories for the analysis results\n","REGRESSION_PLOTS_DIR = DRIVE_BASE_PATH / \"Result/0000Contrast_result/Scale_Analysis/per_scale_data_UTCI/regression_plots_english\"\n","SUMMARY_TABLES_DIR = DRIVE_BASE_PATH / \"Result/0000Contrast_result/Scale_Analysis/per_scale_data_UTCI/summary_tables\"\n","REGRESSION_PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n","SUMMARY_TABLES_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# --- Analysis Parameters ---\n","# Dictionary to map feature names from code to pretty English labels for plots\n","FEATURE_DISPLAY_NAMES = {\n","    'mean_tree_height': 'Mean Tree Height (m)',\n","    'mean_building_height': 'Mean Building Height (m)',\n","    'tree_proportion': 'Tree Proportion',\n","    'building_proportion': 'Building Proportion',\n","    'st1_proportion': 'Paved Surface Prop.',\n","    'st2_proportion': 'Tiled/Brick Prop.',\n","    'st3_proportion': 'Loam/Soil Prop.',\n","    'st5_proportion': 'Water Body Prop.'\n","}\n","# Use the keys of the dictionary as the list of features to analyze\n","FEATURES_TO_ANALYZE = list(FEATURE_DISPLAY_NAMES.keys())\n","\n","# Hours to analyze (0 = first hour, 11 = 12th hour)\n","HOURS_TO_ANALYZE = range(12) # Analyze all 12 hours\n","\n","# Plotting parameter: To avoid overly dense scatter plots, sample a subset of points.\n","# The regression itself will use ALL valid data points. Set to None to plot all points.\n","MAX_POINTS_FOR_SCATTER = 50000\n","\n","# =============================================================================\n","# 2. Main Analysis Logic\n","# =============================================================================\n","\n","def analyze_scale_effects():\n","    \"\"\"\n","    Main function to loop through all combinations of scale, feature, and hour,\n","    perform linear regression, save the resulting plots, and generate a final summary table.\n","    \"\"\"\n","    total_start_time = time.time()\n","\n","    scale_data_files = sorted(list(PER_SCALE_DATA_DIR.glob(\"scale_data_*.pkl\")))\n","\n","    if not scale_data_files:\n","        raise FileNotFoundError(f\"Error: No data files found in {PER_SCALE_DATA_DIR}. Please run the data generation script first.\")\n","\n","    print(f\"--- Found {len(scale_data_files)} scale files to analyze ---\")\n","\n","    all_results = []\n","\n","    # Calculate total iterations for a single, fine-grained progress bar\n","    total_iterations = len(scale_data_files) * len(FEATURES_TO_ANALYZE) * len(HOURS_TO_ANALYZE)\n","\n","    # Create the tqdm progress bar to wrap all loops\n","    with tqdm(total=total_iterations, desc=\"Overall Progress (by combination)\") as pbar:\n","        # --- 1. Outermost Loop: Iterate over each scale ---\n","        for data_file in scale_data_files:\n","            scale_data = pd.read_pickle(data_file)\n","            scale_m = scale_data['scale_meters']\n","\n","            # --- 2. Middle Loop: Iterate over each feature ---\n","            for feature_name in FEATURES_TO_ANALYZE:\n","                x_2d = scale_data['features'][feature_name]\n","                x_flat = x_2d.flatten()\n","\n","                # --- 3. Innermost Loop: Iterate over each hour ---\n","                for hour in HOURS_TO_ANALYZE:\n","                    try:\n","                        y_2d = scale_data['utci_timeseries'][:, :, hour]\n","                        y_flat = y_2d.flatten()\n","\n","                        # --- Data Cleaning and Filtering (UPDATED LOGIC) ---\n","                        # Remove pairs with NaN values AND where the feature value is zero.\n","                        valid_indices = ~np.isnan(x_flat) & ~np.isnan(y_flat) & (x_flat != 0)\n","                        x_valid = x_flat[valid_indices].reshape(-1, 1)\n","                        y_valid = y_flat[valid_indices]\n","\n","                        if len(y_valid) < 10: # Skip if too few valid data points\n","                            pbar.update(1) # Still need to update the progress bar\n","                            continue\n","\n","                        # --- Perform Linear Regression ---\n","                        model = LinearRegression()\n","                        model.fit(x_valid, y_valid)\n","\n","                        slope = model.coef_[0]\n","                        intercept = model.intercept_\n","                        r_squared = model.score(x_valid, y_valid)\n","                        equation = f\"y = {slope:.4f}x + {intercept:.2f}\"\n","\n","                        # --- Store Results ---\n","                        result_row = {\n","                            'Scale_Meters': scale_m,\n","                            'Feature': feature_name,\n","                            'Hour': hour + 8, # Assuming time starts from 8:00\n","                            'Slope_CE': slope,\n","                            'Intercept': intercept,\n","                            'R_Squared': r_squared,\n","                            'Equation': equation\n","                        }\n","                        all_results.append(result_row)\n","\n","                        # --- Plotting and Saving (UPDATED with ENGLISH LABELS) ---\n","                        plt.figure(figsize=(6, 6))\n","\n","                        if MAX_POINTS_FOR_SCATTER and len(y_valid) > MAX_POINTS_FOR_SCATTER:\n","                            sample_indices = np.random.choice(len(y_valid), MAX_POINTS_FOR_SCATTER, replace=False)\n","                            x_sample, y_sample = x_valid[sample_indices], y_valid[sample_indices]\n","                        else:\n","                            x_sample, y_sample = x_valid, y_valid\n","\n","                        plt.scatter(x_sample, y_sample, alpha=0.3, s=10, label=\"Data Points (Sampled)\")\n","\n","                        line_x = np.array([x_valid.min(), x_valid.max()]).reshape(-1, 1)\n","                        line_y = model.predict(line_x)\n","                        plt.plot(line_x, line_y, color='red', linewidth=2, label=\"Linear Regression Line\")\n","\n","                        # Use the display name from the dictionary\n","                        display_feature_name = FEATURE_DISPLAY_NAMES.get(feature_name, feature_name)\n","\n","                        plt.title(f\"UTCI vs {display_feature_name}\\n(Scale: {scale_m}m, Time: {hour+8}:00)\", fontsize=16)\n","                        plt.xlabel(f\"Feature: {display_feature_name}\", fontsize=12)\n","                        plt.ylabel(\"UTCI (°C)\", fontsize=12)\n","                        plt.grid(True, linestyle='--', alpha=0.6)\n","\n","                        # 在图上标注公式和R² (移除了bbox背景框)\n","                        text_str = f\"{equation}\\n$R^2 = {r_squared:.4f}$\"\n","                        plt.text(0.95, 0.95, text_str, transform=plt.gca().transAxes,\n","                                 fontsize=14, verticalalignment='top', horizontalalignment='right')\n","\n","                        plt.legend()\n","\n","                        plot_filename = f\"scale_{str(scale_m).zfill(4)}m_feature_{feature_name}_hour_{str(hour+8).zfill(2)}.png\"\n","                        plot_path = REGRESSION_PLOTS_DIR / plot_filename\n","                        plt.savefig(plot_path, dpi=100, bbox_inches='tight')\n","                        plt.close()\n","\n","                    except Exception as e:\n","                        print(f\"Error processing Scale {scale_m}m, Feature {feature_name}, Hour {hour+8}: {e}\")\n","\n","                    # Update the progress bar after each combination is processed or skipped\n","                    pbar.update(1)\n","\n","    # --- Final Summary ---\n","    print(\"\\n--- Analysis complete, generating final summary table ---\")\n","    summary_df = pd.DataFrame(all_results)\n","\n","    summary_csv_path = SUMMARY_TABLES_DIR / \"CE_summary_results.csv\"\n","    summary_df.to_csv(summary_csv_path, index=False)\n","\n","    total_duration = time.time() - total_start_time\n","    print(f\"\\n🎉 Analysis pipeline finished successfully!\")\n","    print(f\"--> All regression plots saved to: {REGRESSION_PLOTS_DIR}\")\n","    print(f\"--> Final summary table saved to: {summary_csv_path}\")\n","    print(f\"--> Total elapsed time: {total_duration:.2f} seconds.\")\n","\n","\n","# =============================================================================\n","# 3. Execution\n","# =============================================================================\n","if __name__ == \"__main__\":\n","    with warnings.catch_warnings():\n","        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n","        warnings.simplefilter(\"ignore\", category=UserWarning)\n","        analyze_scale_effects()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkG_qhdHYrjg","executionInfo":{"status":"ok","timestamp":1756382828324,"user_tz":-480,"elapsed":1561006,"user":{"displayName":"weilin xin","userId":"14777990448514190568"}},"outputId":"b3ac132d-a69d-42a7-d692-28c965664089"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- 1. Configuring Paths and Parameters ---\n","--- Found 20 scale files to analyze ---\n"]},{"output_type":"stream","name":"stderr","text":["Overall Progress (by combination): 100%|██████████| 1920/1920 [25:55<00:00,  1.23it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","--- Analysis complete, generating final summary table ---\n","\n","🎉 Analysis pipeline finished successfully!\n","--> All regression plots saved to: /content/drive/MyDrive/Colab Notebooks/Graph Data Process/Result/0000Contrast_result/Scale_Analysis/per_scale_data/regression_plots_english\n","--> Final summary table saved to: /content/drive/MyDrive/Colab Notebooks/Graph Data Process/Result/0000Contrast_result/Scale_Analysis/per_scale_data/summary_tables/CE_summary_results.csv\n","--> Total elapsed time: 1555.90 seconds.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["#Scaling Law"],"metadata":{"id":"kgRYmEFHXo14"}},{"cell_type":"markdown","source":["###Dominant mode selection"],"metadata":{"id":"vgNmJMOcG2ZR"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import numpy as np\n","from pathlib import Path\n","from matplotlib.lines import Line2D\n","\n","# =============================================================================\n","# 1. 主配置区域 (MASTER CONFIGURATION)\n","# =============================================================================\n","print(\"--- 1. 配置绘图参数 ---\")\n","\n","# ⭐ 1. 设置您的项目基础路径 (请确保与分析脚本一致)\n","DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","\n","# ⭐ 2. 设置分段数目 (用于定位正确的输入文件)\n","NUM_SEGMENTS = 2\n","\n","# ⭐ 3. 设置本次可视化的目标特征\n","TARGET_FEATURE = 'tree_proportion'\n","\n","FEATURES_TO_PROCESS = [\n","    'mean_tree_height', 'mean_building_height', 'tree_proportion',\n","    'building_proportion', 'st1_proportion', 'st2_proportion',\n","    'st3_proportion', 'st5_proportion'\n","]\n","# ⭐ 4. (新) 设置Beta值的选择模式\n","#    'significance': 优先选择P-value更小 (更显著) 的那一段的beta。\n","#    'points':       优先选择数据点数量更多的那一段的beta。\n","BETA_SELECTION_MODE = 'significance'\n","\n","# ⭐ 5. 设置统计显著性的P-value阈值\n","P_VALUE_THRESHOLD = 0.05\n","\n","# ⭐ 6. 设置是否显示不可靠点 (不显著或不健康) 的标记\n","#    True:  显示所有点, 并用红色圆圈标记出不可靠的点。\n","#    False: 直接不绘制不可靠的点 (折线会在此处断开)。\n","SHOW_UNRELIABLE_POINTS = True\n","\n","# --- 路径配置 ---\n","MASTER_STATS_PATH = DRIVE_BASE_PATH / f\"Result/0000Contrast_result/Scale_Analysis/diurnal_breakpoint_analysis/segments_{NUM_SEGMENTS}/master_summary_stats.csv\"\n","OUTPUT_PLOT_PATH = DRIVE_BASE_PATH / f\"Result/0000Contrast_result/Scale_Analysis/diurnal_breakpoint_analysis/segments_{NUM_SEGMENTS}/{TARGET_FEATURE}_dominant_beta_trends_{BETA_SELECTION_MODE}_mode.png\"\n","\n","# =============================================================================\n","# 2. 绘图执行流程\n","# =============================================================================\n","if __name__ == \"__main__\":\n","    print(f\"--- 2. 开始为特征 '{TARGET_FEATURE}' 生成主导Beta趋势图 ---\")\n","    print(f\"   - 选择模式: '{BETA_SELECTION_MODE}'\")\n","\n","    if not MASTER_STATS_PATH.exists():\n","        print(f\"❌ 错误: 找不到汇总数据文件: {MASTER_STATS_PATH}\")\n","    else:\n","        # 1. 加载并筛选数据\n","        df = pd.read_csv(MASTER_STATS_PATH)\n","        feature_df = df[df['Feature'] == TARGET_FEATURE].copy()\n","        print(f\"INFO: 已加载并筛选出 {len(feature_df)} 条与 '{TARGET_FEATURE}' 相关的数据。\")\n","\n","        if feature_df.empty:\n","            print(f\"⚠️ 警告: 在汇总文件中未找到特征 '{TARGET_FEATURE}' 的数据，无法绘图。\")\n","        else:\n","            # 2. (新) 根据选择模式，确定每个小时的主导Beta值\n","            p1 = feature_df['P-value_1'].fillna(1.0)\n","            p2 = feature_df['P-value_2'].fillna(1.0)\n","            n1 = feature_df['n_points_1'].fillna(0)\n","            n2 = feature_df['n_points_2'].fillna(0)\n","\n","            if BETA_SELECTION_MODE == 'significance':\n","                # 条件: p1更显著; 或p值相等时, n1的点更多\n","                condition = (p1 < p2) | ((p1 == p2) & (n1 >= n2))\n","            elif BETA_SELECTION_MODE == 'points':\n","                # 条件: n1的点更多; 或点数相等时, p1更显著\n","                condition = (n1 > n2) | ((n1 == n2) & (p1 <= p2))\n","            else:\n","                raise ValueError(\"BETA_SELECTION_MODE 必须是 'significance' 或 'points'\")\n","\n","            feature_df['selected_beta'] = np.where(condition, feature_df['beta_1'], feature_df['beta_2'])\n","            feature_df['selected_p_value'] = np.where(condition, p1, p2)\n","            feature_df['selected_n_points'] = np.where(condition, n1, n2)\n","\n","            # 3. 准备绘图\n","            plt.style.use('seaborn-v0_8-whitegrid')\n","            fig, ax = plt.subplots(1, 1, figsize=(14, 8)) # 单个图\n","            fig.suptitle(f\"'{TARGET_FEATURE}' 的主导Beta日变化趋势 (模式: {BETA_SELECTION_MODE})\", fontsize=20)\n","            variables = feature_df['Variable'].unique()\n","            colors = cm.get_cmap('tab10', len(variables))\n","\n","            # 4. 循环绘制每个影响因子的曲线\n","            for j, variable in enumerate(variables):\n","                variable_df = feature_df[feature_df['Variable'] == variable].copy()\n","\n","                # 统一的不可靠点判断逻辑\n","                unreliable_mask = (variable_df['selected_p_value'] >= P_VALUE_THRESHOLD) | (variable_df['selected_n_points'] < 3)\n","\n","                # 根据开关决定绘图方式\n","                if SHOW_UNRELIABLE_POINTS:\n","                    ax.plot(variable_df['Hour'], variable_df['selected_beta'], marker='o', linestyle='-', color=colors(j), label=variable, markersize=5)\n","                    unreliable_points = variable_df[unreliable_mask]\n","                    if not unreliable_points.empty:\n","                        ax.scatter(unreliable_points['Hour'], unreliable_points['selected_beta'],\n","                                   s=150, facecolors='none', edgecolors='red', linewidth=1.5, zorder=10)\n","                else:\n","                    reliable_points_df = variable_df[~unreliable_mask]\n","                    ax.plot(reliable_points_df['Hour'], reliable_points_df['selected_beta'], marker='o', linestyle='-', color=colors(j), label=variable, markersize=5)\n","\n","            # 5. 格式化图表和图例\n","            ax.set_ylabel(\"主导Beta值 (Dominant Beta)\", fontsize=12)\n","            ax.grid(True, which=\"both\", linestyle=':', alpha=0.7)\n","\n","            handles, labels = ax.get_legend_handles_labels()\n","            if SHOW_UNRELIABLE_POINTS and any(unreliable_mask):\n","                unreliable_marker = Line2D([0], [0], marker='o', color='w', label=f'不可靠点 (p >= {P_VALUE_THRESHOLD} 或 n < 3)',\n","                                      markerfacecolor='none', markeredgecolor='red', markersize=10, markeredgewidth=1.5)\n","                handles.append(unreliable_marker)\n","            ax.legend(handles=handles, title=\"影响因子 (Variable)\")\n","\n","            # 6. 格式化并保存\n","            plt.xlabel(\"小时 (Hour of Day)\", fontsize=12)\n","            plt.xticks(range(8, 20))\n","            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","\n","            plt.savefig(OUTPUT_PLOT_PATH, dpi=200, bbox_inches='tight')\n","            plt.show()\n","            print(f\"✅ 绘图完成！图表已保存至: {OUTPUT_PLOT_PATH}\")\n","            plt.close(fig)\n"],"metadata":{"id":"FNcBdl2fGr0c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Final_Data_Generation"],"metadata":{"id":"mTKiZ-eIMiTv"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","\n","# =============================================================================\n","# 1. 主配置区域 (MASTER CONFIGURATION)\n","# =============================================================================\n","print(\"--- 1. 配置数据生成参数 ---\")\n","\n","# ⭐ 1. 设置您的项目基础路径 (请确保与分析脚本一致)\n","DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","\n","# ⭐ 2. 设置分段数目 (用于定位正确的输入文件)\n","NUM_SEGMENTS = 2\n","\n","# ⭐ 3. 设置统计显著性的P-value阈值\n","P_VALUE_THRESHOLD = 0.05\n","\n","# --- 路径配置 ---\n","# 输入的汇总数据文件\n","MASTER_STATS_PATH = DRIVE_BASE_PATH / f\"Result/0000Contrast_result/Scale_Analysis/diurnal_breakpoint_analysis/segments_{NUM_SEGMENTS}/master_summary_stats.csv\"\n","\n","# 输出的最终清洗后的数据表\n","OUTPUT_CSV_PATH = DRIVE_BASE_PATH / f\"Result/0000Contrast_result/Scale_Analysis/diurnal_breakpoint_analysis/segments_{NUM_SEGMENTS}/final_clean_beta_table_by_stage.csv\"\n","\n","# =============================================================================\n","# 2. 数据处理与生成流程\n","# =============================================================================\n","if __name__ == \"__main__\":\n","    print(f\"--- 2. 开始生成最终清洗数据表 ---\")\n","\n","    if not MASTER_STATS_PATH.exists():\n","        print(f\"❌ 错误: 找不到汇总数据文件: {MASTER_STATS_PATH}\")\n","    else:\n","        # 1. 加载数据\n","        df = pd.read_csv(MASTER_STATS_PATH)\n","        print(f\"INFO: 已成功加载 {len(df)} 条原始数据。\")\n","\n","        # 检查必需的列是否存在\n","        required_cols = ['Stage', 'breakpoint_1']\n","        if not all(col in df.columns for col in required_cols):\n","            raise ValueError(f\"错误: 输入的CSV文件中缺少必需的列。请确保包含: {required_cols}\")\n","\n","        # 2. 根据 'Stage' 列，确定每个小时要使用的数据段\n","        condition = df['Stage'] == 1\n","        print(\"INFO: 将根据 'Stage' 列直接选择数据段。\")\n","\n","        # 使用该条件直接选取对应的指标\n","        df['selected_beta'] = np.where(condition, df['beta_1'], df['beta_2'])\n","        df['selected_p_value'] = np.where(condition, df['P-value_1'], df['P-value_2'])\n","        df['selected_r2'] = np.where(condition, df['R2_1'], df['R2_2'])\n","        df['selected_n_points'] = np.where(condition, df['n_points_1'], df['n_points_2'])\n","\n","        print(\"INFO: 已根据 'Stage' 列确定每小时的主导Beta值。\")\n","\n","        # 3. 执行简化的健康度检查\n","        unreliable_mask_standard = (df['selected_p_value'].fillna(1.0) >= P_VALUE_THRESHOLD) | \\\n","                                   (df['selected_n_points'].fillna(0) < 3)\n","\n","        is_unreliable = unreliable_mask_standard\n","\n","        print(f\"INFO: 健康度检查完成。共发现 {is_unreliable.sum()} 个不可靠的数据点。\")\n","\n","        # 4. 数据清洗：将不可靠数据的核心指标设为空值\n","        columns_to_nullify = ['selected_beta', 'selected_p_value', 'selected_r2']\n","        df.loc[is_unreliable, columns_to_nullify] = np.nan\n","\n","        # 5. 格式化最终表格\n","        # [核心修改] 在最终输出中添加 'breakpoint_1' 列\n","        final_df = df[['Variable', 'Feature', 'Hour', 'Stage', 'Model', 'breakpoint_1']].copy()\n","        final_df['Final_Beta'] = df['selected_beta']\n","        final_df['Final_P_Value'] = df['selected_p_value']\n","        final_df['Final_R_Squared'] = df['selected_r2']\n","\n","        # 重命名 breakpoint_1 为更通用的名称\n","        final_df.rename(columns={'breakpoint_1': 'Characteristic_Scale'}, inplace=True)\n","\n","\n","        # 6. 保存最终的CSV文件\n","        final_df.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8-sig')\n","        print(f\"\\n✅ 最终清洗数据表已成功生成！\")\n","        print(f\"   文件已保存至: {OUTPUT_CSV_PATH}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ibm1GAtq5ud","executionInfo":{"status":"ok","timestamp":1757750279852,"user_tz":-480,"elapsed":74,"user":{"displayName":"weilin xin","userId":"14777990448514190568"}},"outputId":"2295e928-b36b-4993-bdaa-9feb38ce61e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- 1. 配置数据生成参数 ---\n","--- 2. 开始生成最终清洗数据表 ---\n","INFO: 已成功加载 480 条原始数据。\n","INFO: 将根据 'Stage' 列直接选择数据段。\n","INFO: 已根据 'Stage' 列确定每小时的主导Beta值。\n","INFO: 健康度检查完成。共发现 53 个不可靠的数据点。\n","\n","✅ 最终清洗数据表已成功生成！\n","   文件已保存至: /content/drive/MyDrive/Colab Notebooks/Graph Data Process/Result/0000Contrast_result/Scale_Analysis/diurnal_breakpoint_analysis/segments_2/final_clean_beta_table_by_stage.csv\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","\n","# =============================================================================\n","# 1. 主配置区域 (MASTER CONFIGURATION)\n","# =============================================================================\n","print(\"--- 1. 配置数据聚合参数 ---\")\n","\n","# ⭐ 1. 设置您的项目基础路径 (请确保与分析脚本一致)\n","DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","\n","# ⭐ 2. 设置分段数目 (用于定位正确的输入文件)\n","NUM_SEGMENTS = 2\n","\n","# --- 路径配置 ---\n","# [核心修改] 输入文件直接指向基于 \"Stage\" 生成的文件\n","CLEAN_BETA_TABLE_PATH = DRIVE_BASE_PATH / f\"Result/0000Contrast_result/Scale_Analysis/diurnal_breakpoint_analysis/segments_{NUM_SEGMENTS}/final_clean_beta_table_by_stage.csv\"\n","\n","# 输入文件2: 每小时气候均值\n","HOURLY_CLIMATE_AVG_PATH = DRIVE_BASE_PATH / f\"Result/0000Contrast_result/Scale_Analysis/diurnal_breakpoint_analysis/segments_{NUM_SEGMENTS}/hourly_climate_averages.csv\"\n","\n","# 最终输出文件: 聚合后的绘图专用数据\n","AGGREGATED_OUTPUT_PATH = DRIVE_BASE_PATH / f\"Result/0000Contrast_result/Scale_Analysis/diurnal_breakpoint_analysis/segments_{NUM_SEGMENTS}/aggregated_data_for_plotting.csv\"\n","\n","# --- 变量列表 ---\n","CLIMATE_VARIABLES = ['UTCI', 'air_temp', 'relative_humidity', 'MRT', 'wind_speed']\n","\n","# =============================================================================\n","# 2. 数据聚合流程\n","# =============================================================================\n","if __name__ == \"__main__\":\n","    print(f\"--- 2. 开始执行数据聚合流程 ---\")\n","\n","    try:\n","        # --- 阶段一: 数据加载与合并 ---\n","        print(\"   - 正在加载输入文件...\")\n","        if not CLEAN_BETA_TABLE_PATH.exists() or not HOURLY_CLIMATE_AVG_PATH.exists():\n","            raise FileNotFoundError(\"找不到必要的输入文件，请先确保之前的脚本已成功运行。\")\n","\n","        beta_df = pd.read_csv(CLEAN_BETA_TABLE_PATH)\n","        climate_df = pd.read_csv(HOURLY_CLIMATE_AVG_PATH)\n","\n","        # 检查必需的 'Model' 列是否存在\n","        if 'Model' not in beta_df.columns:\n","            raise ValueError(\"错误: 输入的CSV文件中缺少必需的 'Model' 列。\")\n","\n","        # 合并成一个完整的基础数据表\n","        base_df = pd.merge(beta_df, climate_df, on='Hour', how='left')\n","        print(f\"   - 数据加载与合并完成，基础表包含 {len(base_df)} 行。\")\n","\n","        # --- 阶段二: 核心聚合计算 ---\n","        print(\"   - 开始按 (Variable, Feature) 分组进行聚合计算...\")\n","\n","        aggregated_results = []\n","        climate_avg_cols = [f'avg_{v}' for v in CLIMATE_VARIABLES]\n","\n","        # 按40个组合进行分组\n","        for (variable, feature), group in base_df.groupby(['Variable', 'Feature']):\n","\n","            # [核心修改] 提取该组合的Model类型 (因为组内都相同，取第一个即可)\n","            group_model = group['Model'].iloc[0]\n","\n","            # 计算 Beta 均值和方差 (自动忽略NaN)\n","            beta_mean = group['Final_Beta'].mean()\n","            beta_variance = group['Final_Beta'].var()\n","\n","            # 计算 P-value 均值\n","            p_value_mean = group['Final_P_Value'].mean()\n","\n","            # 计算5个相关性数值\n","            correlations = group[['Final_Beta'] + climate_avg_cols].corr(method='pearson')\n","\n","            # 准备要存储的数据行\n","            result_row = {\n","                'Variable': variable,\n","                'Feature': feature,\n","                'Model': group_model, # [核心修改] 添加Model信息\n","                'Beta_Mean': beta_mean,\n","                'Beta_Variance': beta_variance,\n","                'P_Value_Mean': p_value_mean,\n","            }\n","\n","            # 提取并添加5个相关性值\n","            for var_name in CLIMATE_VARIABLES:\n","                col_name = f'avg_{var_name}'\n","                corr_value = correlations.loc['Final_Beta', col_name]\n","                result_row[f'Corr_{var_name}'] = corr_value\n","\n","            aggregated_results.append(result_row)\n","\n","        print(f\"   - 聚合计算完成，共处理了 {len(aggregated_results)} 个组合。\")\n","\n","        # --- 阶段三: 结果整合与输出 ---\n","        final_df = pd.DataFrame(aggregated_results)\n","\n","        # [核心修改] 保证列的顺序，并加入 'Model' 列\n","        final_cols = ['Variable', 'Feature', 'Model', 'Beta_Mean', 'Beta_Variance', 'P_Value_Mean'] + [f'Corr_{v}' for v in CLIMATE_VARIABLES]\n","        final_df = final_df[final_cols]\n","\n","        final_df.to_csv(AGGREGATED_OUTPUT_PATH, index=False, encoding='utf-8-sig')\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"🎉 终极绘图数据源已成功生成！\")\n","        print(f\"   文件已保存至: {AGGREGATED_OUTPUT_PATH.name}\")\n","        print(\"=\"*80)\n","\n","    except Exception as e:\n","        print(f\"\\n❌ 程序运行中发生错误: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z00Q_CqTsZ3s","executionInfo":{"status":"ok","timestamp":1757747124857,"user_tz":-480,"elapsed":6,"user":{"displayName":"weilin xin","userId":"14777990448514190568"}},"outputId":"8ffd0427-040a-4f37-82c1-ac8613b1b572"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- 1. 配置数据聚合参数 ---\n","--- 2. 开始执行数据聚合流程 ---\n","   - 正在加载输入文件...\n","   - 数据加载与合并完成，基础表包含 480 行。\n","   - 开始按 (Variable, Feature) 分组进行聚合计算...\n","   - 聚合计算完成，共处理了 40 个组合。\n","\n","================================================================================\n","🎉 终极绘图数据源已成功生成！\n","   文件已保存至: aggregated_data_for_plotting.csv\n","================================================================================\n"]}]}]}