{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"hFgAnmRRzU92"},"outputs":[],"source":["pip install torch_geometric torchprofile"]},{"cell_type":"markdown","source":["##Data Loader"],"metadata":{"id":"szgoBzKA1xql"}},{"cell_type":"code","source":["# ===========================================================\n","# 0. 导入 & 数据加载 (只运行一次)\n","# ===========================================================\n","import pickle\n","from pathlib import Path\n","import os\n","import time\n","from torch_geometric.data import Data # 导入 Data 以进行类型检查\n","\n","print(\"--- 正在启动数据加载单元格 ---\")\n","\n","# --- 1. 定义关键路径 ---\n","# (确保这些库已在 STGCN/ASTGCN 单元格中导入，或者在这里导入)\n","DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","DATA_SUBDIR = Path(\"Result/Sequential_13Hour_Data\")\n","DATA_FILENAME = \"graph_seq_20230503_SeqH7to19_NpyH8fill0.0.pkl\"\n","DATA_PATH = DRIVE_BASE_PATH / DATA_SUBDIR / DATA_FILENAME\n","\n","# --- 2. 一次性加载数据到全局变量 ---\n","GLOBAL_LOADED_DATA = None\n","print(f\"正在从 {DATA_PATH} 加载数据...\")\n","load_start_time = time.time()\n","\n","try:\n","    if not DATA_PATH.exists():\n","        raise FileNotFoundError(f\"数据文件在指定路径未找到: {DATA_PATH}\")\n","\n","    with open(DATA_PATH, \"rb\") as f:\n","        # 这个变量将在整个 Colab 运行时中可用\n","        GLOBAL_LOADED_DATA = pickle.load(f)\n","\n","    load_duration = time.time() - load_start_time\n","    print(f\"数据加载成功! 耗时: {load_duration:.2f} 秒。\")\n","\n","    # 基本的数据验证\n","    if (not GLOBAL_LOADED_DATA or\n","        not isinstance(GLOBAL_LOADED_DATA, list) or\n","        not GLOBAL_LOADED_DATA[0] or\n","        not isinstance(GLOBAL_LOADED_DATA[0], list)):\n","        print(\"警告：加载的数据为空或格式不正确！\")\n","    else:\n","        print(f\"数据类型: {type(GLOBAL_LOADED_DATA)}, 总序列数: {len(GLOBAL_LOADED_DATA)}\")\n","        print(f\"第一条序列的类型: {type(GLOBAL_LOADED_DATA[0])}, 长度: {len(GLOBAL_LOADED_DATA[0])}\")\n","\n","except Exception as e:\n","    print(f\"加载数据时发生严重错误: {e}\")\n","    import traceback\n","    traceback.print_exc()\n","\n","print(\"--- 数据加载单元格执行完毕 ---\")"],"metadata":{"id":"N5scjkpe1y7z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l4oT7hkt2Coz"},"source":["##Ours"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NbgqUP33vpGM"},"outputs":[],"source":["# ===========================================================\n","# 0. 环境 & 依赖\n","# ===========================================================\n","import os\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch_geometric.nn import RGCNConv # ⚡ USE RGCNConv\n","from torch_geometric.data import Data, Batch # Batch很重要\n","from torch_geometric.loader import DataLoader\n","# import matplotlib.pyplot as plt # Not used in the final reporting directly, can be commented if not needed elsewhere\n","import pickle\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.metrics import r2_score\n","import gc\n","import math\n","from datetime import datetime as dt_datetime, timedelta # For time feature\n","from pathlib import Path\n","import time # Added for timing\n","import json # Added for report export\n","import torchprofile # Added for FLOPS calculation\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","torch.cuda.ipc_collect()\n","\n","\n","# ===========================================================\n","# 1. 特征生成 & 辅助模块\n","# ===========================================================\n","\n","def generate_time_features_for_sequence(base_dt_obj, num_steps):\n","    time_features_list = []\n","    for i in range(num_steps):\n","        current_dt = base_dt_obj + timedelta(hours=i)\n","        hour_norm = current_dt.hour / 23.0\n","        day_of_year_norm = current_dt.timetuple().tm_yday / (366.0 if current_dt.year % 4 == 0 and (current_dt.year % 100 != 0 or current_dt.year % 400 == 0) else 365.0)\n","        hour_sin = math.sin(2 * math.pi * hour_norm)\n","        hour_cos = math.cos(2 * math.pi * hour_norm)\n","        doy_sin = math.sin(2 * math.pi * day_of_year_norm)\n","        doy_cos = math.cos(2 * math.pi * day_of_year_norm)\n","        time_features_list.append(torch.tensor([hour_sin, hour_cos, doy_sin, doy_cos], dtype=torch.float32))\n","    return torch.stack(time_features_list)\n","\n","\n","class MLPEncoder(nn.Module):\n","    def __init__(self, in_dim, out_dim, hid_dim=None, dropout_rate=0.1):\n","        super().__init__()\n","        if hid_dim is None:\n","            hid_dim = max(min(in_dim, out_dim), (in_dim + out_dim) // 2)\n","            if hid_dim == 0 and out_dim > 0 : hid_dim = out_dim\n","            if hid_dim == 0 and in_dim > 0 : hid_dim = in_dim\n","            if hid_dim == 0 : hid_dim = 1\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(in_dim, hid_dim),\n","            nn.ReLU(),\n","            nn.LayerNorm(hid_dim),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hid_dim, out_dim)\n","        )\n","    def forward(self, x):\n","        return self.mlp(x)\n","\n","class RGCNModule(nn.Module):\n","    def __init__(self, rgcn_input_dim, rgcn_hidden_dim, rgcn_output_dim, num_relations, dropout_rate=0.5):\n","        super().__init__()\n","        self.rgcn_input_dim = rgcn_input_dim\n","        self.rgcn_hidden_dim = rgcn_hidden_dim\n","        self.rgcn_output_dim = rgcn_output_dim\n","        self.num_relations = num_relations\n","\n","        self.conv1 = RGCNConv(rgcn_input_dim, rgcn_hidden_dim, num_relations)\n","        self.bn1 = nn.BatchNorm1d(rgcn_hidden_dim)\n","        self.prelu1 = nn.PReLU(rgcn_hidden_dim)\n","\n","        self.conv2 = RGCNConv(rgcn_hidden_dim, rgcn_hidden_dim, num_relations)\n","        self.bn2 = nn.BatchNorm1d(rgcn_hidden_dim)\n","        self.prelu2 = nn.PReLU(rgcn_hidden_dim)\n","\n","        self.conv3 = RGCNConv(rgcn_hidden_dim, rgcn_output_dim, num_relations)\n","        self.bn3 = nn.BatchNorm1d(rgcn_output_dim)\n","        self.prelu3 = nn.PReLU(rgcn_output_dim)\n","\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x, edge_index, edge_attr):\n","        if edge_attr is None or edge_attr.shape[1] < 5:\n","            raise ValueError(\"RGCNModule: edge_attr is missing or has insufficient columns for edge_type.\")\n","        edge_type = edge_attr[:, 4].long()\n","\n","        x = self.conv1(x, edge_index, edge_type=edge_type)\n","        x = self.bn1(x)\n","        x = self.prelu1(x)\n","        x = self.dropout(x)\n","\n","        x = self.conv2(x, edge_index, edge_type=edge_type)\n","        x = self.bn2(x)\n","        x = self.prelu2(x)\n","        x = self.dropout(x)\n","\n","        x = self.conv3(x, edge_index, edge_type=edge_type)\n","        x = self.bn3(x)\n","        x = self.prelu3(x)\n","        return x\n","\n","# ===========================================================\n","# 2. RGCN-LSTM 模型定义\n","# ===========================================================\n","class RGCNLSTMModelWithHourlyHeads(nn.Module): # Renamed from RGCNGRUModelWithHourlyHeads\n","    def __init__(self,\n","                 static_node_in_dim,\n","                 global_env_in_dim,\n","                 time_in_dim,\n","                 global_env_emb_dim,\n","                 time_emb_dim,\n","                 rgcn_hidden_dim,\n","                 rgcn_output_dim,\n","                 num_relations,\n","                 lstm_hidden_dim, # Renamed from gru_hidden_dim\n","                 fusion_mlp_output_dim=None,\n","                 fusion_mlp_hidden_dim=None,\n","                 dropout_rate_fusion_mlp=0.1,\n","                 num_lstm_layers=1, # Renamed from num_gru_layers\n","                 T_pred_horizon=12,\n","                 dropout_rate_encoders=0.1,\n","                 dropout_rate_rgcn=0.3,\n","                 dropout_rate_lstm=0.2, # Renamed from dropout_rate_gru\n","                 mlp_prediction_hidden_dim=64,\n","                 dropout_rate_pred_head=0.2\n","                ):\n","        super().__init__()\n","        self.T_pred_horizon = T_pred_horizon\n","        self.static_node_in_dim = static_node_in_dim\n","        self.global_env_in_dim = global_env_in_dim\n","        self.time_in_dim = time_in_dim\n","        self.num_relations = num_relations\n","        self.rgcn_output_dim = rgcn_output_dim\n","        self.lstm_hidden_dim = lstm_hidden_dim # Storing for component profiling & consistency\n","\n","\n","        self.global_env_encoder = MLPEncoder(global_env_in_dim, global_env_emb_dim, dropout_rate=dropout_rate_encoders)\n","        self.time_encoder = MLPEncoder(time_in_dim, time_emb_dim, dropout_rate=dropout_rate_encoders)\n","        # MLP to encode GCN output to match LSTM hidden dim for h0, c0\n","        self.h0_c0_from_rgcn_encoder = MLPEncoder(rgcn_output_dim, lstm_hidden_dim, dropout_rate=dropout_rate_encoders)\n","\n","        self.rgcn_module_for_h0 = RGCNModule(static_node_in_dim, rgcn_hidden_dim, rgcn_output_dim, num_relations, dropout_rate_rgcn)\n","        self.rgcn_module_for_sequence = RGCNModule(static_node_in_dim, rgcn_hidden_dim, rgcn_output_dim, num_relations, dropout_rate_rgcn)\n","\n","        concatenated_feature_dim = rgcn_output_dim + global_env_emb_dim + time_emb_dim\n","        actual_fusion_mlp_output_dim = fusion_mlp_output_dim if fusion_mlp_output_dim is not None else concatenated_feature_dim\n","        self.fusion_mlp_input_dim = concatenated_feature_dim\n","\n","        self.fusion_mlp = MLPEncoder(\n","            in_dim=concatenated_feature_dim,\n","            out_dim=actual_fusion_mlp_output_dim,\n","            hid_dim=fusion_mlp_hidden_dim,\n","            dropout_rate=dropout_rate_fusion_mlp\n","        )\n","\n","        lstm_input_size_actual = actual_fusion_mlp_output_dim\n","        self.lstm_input_dim = lstm_input_size_actual # Store for component profiling\n","\n","        self.lstm = nn.LSTM( # Changed from nn.GRU\n","            input_size=lstm_input_size_actual,\n","            hidden_size=lstm_hidden_dim,\n","            num_layers=num_lstm_layers,\n","            batch_first=True,\n","            dropout=dropout_rate_lstm if num_lstm_layers > 1 else 0.0\n","        )\n","\n","        self.hourly_prediction_heads = nn.ModuleList()\n","        for _ in range(T_pred_horizon):\n","            self.hourly_prediction_heads.append(\n","                nn.Sequential(\n","                    nn.Linear(lstm_hidden_dim, mlp_prediction_hidden_dim), # Input from LSTM\n","                    nn.ReLU(),\n","                    nn.Dropout(dropout_rate_pred_head),\n","                    nn.Linear(mlp_prediction_hidden_dim, 1)\n","                )\n","            )\n","\n","        self.register_buffer('node_feat_mean', torch.zeros(static_node_in_dim))\n","        self.register_buffer('node_feat_std', torch.ones(static_node_in_dim))\n","\n","\n","    def forward(self, list_of_batched_timesteps: list, timeline_time_features: torch.Tensor, device: torch.device):\n","        pyg_batch_7am = list_of_batched_timesteps[0].to(device)\n","        normalized_x_7am = (pyg_batch_7am.x - self.node_feat_mean) / (self.node_feat_std + 1e-8)\n","\n","        rgcn_output_7am = self.rgcn_module_for_h0(\n","            normalized_x_7am,\n","            pyg_batch_7am.edge_index,\n","            pyg_batch_7am.edge_attr\n","        )\n","        # For LSTM, h0 and c0 are needed. We'll derive h0 from GCN and initialize c0 to zeros.\n","        h0_features_for_lstm_nodes = self.h0_c0_from_rgcn_encoder(rgcn_output_7am)\n","\n","        h0_for_lstm = h0_features_for_lstm_nodes.unsqueeze(0) # Shape: (1, N_nodes, lstm_hidden_dim)\n","        c0_for_lstm = torch.zeros_like(h0_for_lstm) # Shape: (1, N_nodes, lstm_hidden_dim)\n","\n","        if self.lstm.num_layers > 1:\n","            h0_for_lstm = h0_for_lstm.repeat(self.lstm.num_layers, 1, 1)\n","            c0_for_lstm = c0_for_lstm.repeat(self.lstm.num_layers, 1, 1)\n","\n","        initial_hidden_state = (h0_for_lstm, c0_for_lstm)\n","\n","        all_lstm_input_features_over_time = [] # Renamed\n","        for t_pred_idx in range(self.T_pred_horizon):\n","            pyg_batch_this_timestep = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","            normalized_x = (pyg_batch_this_timestep.x - self.node_feat_mean) / (self.node_feat_std + 1e-8)\n","\n","            rgcn_output_nodes_t = self.rgcn_module_for_sequence(\n","                normalized_x,\n","                pyg_batch_this_timestep.edge_index,\n","                pyg_batch_this_timestep.edge_attr\n","            )\n","\n","            global_env_feat_t_unencoded = pyg_batch_this_timestep.graph_global_env_features\n","            expected_num_graphs_in_batch_t = pyg_batch_this_timestep.num_graphs\n","            expected_global_features_dim = self.global_env_encoder.mlp[0].in_features\n","            if not (global_env_feat_t_unencoded.shape == (expected_num_graphs_in_batch_t, expected_global_features_dim)):\n","                if global_env_feat_t_unencoded.ndim == 1 and \\\n","                   global_env_feat_t_unencoded.shape[0] == expected_num_graphs_in_batch_t * expected_global_features_dim:\n","                    global_env_feat_t_unencoded = global_env_feat_t_unencoded.view(expected_num_graphs_in_batch_t, expected_global_features_dim)\n","                elif global_env_feat_t_unencoded.numel() == expected_num_graphs_in_batch_t * expected_global_features_dim:\n","                    global_env_feat_t_unencoded = global_env_feat_t_unencoded.view(expected_num_graphs_in_batch_t, expected_global_features_dim)\n","                else:\n","                    print(f\"Warning: LSTM Input Time {t_pred_idx}: Correcting global_env_feat shape from {global_env_feat_t_unencoded.shape} to ({expected_num_graphs_in_batch_t}, {expected_global_features_dim}) with zeros due to mismatch.\")\n","                    global_env_feat_t_unencoded = torch.zeros(expected_num_graphs_in_batch_t, expected_global_features_dim, device=device)\n","            global_env_emb_t = self.global_env_encoder(global_env_feat_t_unencoded)\n","            global_env_emb_t_expanded = global_env_emb_t[pyg_batch_this_timestep.batch]\n","\n","            current_raw_time_feat_for_timestep_t = timeline_time_features[t_pred_idx, :].to(device)\n","            current_emb_time_feat_for_timestep_t = self.time_encoder(current_raw_time_feat_for_timestep_t)\n","            num_nodes_in_pyg_batch = pyg_batch_this_timestep.num_nodes\n","            time_emb_t_expanded_to_nodes = current_emb_time_feat_for_timestep_t.unsqueeze(0).expand(num_nodes_in_pyg_batch, -1)\n","\n","            concatenated_features_for_timestep_t = torch.cat([rgcn_output_nodes_t, global_env_emb_t_expanded, time_emb_t_expanded_to_nodes], dim=-1)\n","            fused_features_for_timestep_t = self.fusion_mlp(concatenated_features_for_timestep_t)\n","            all_lstm_input_features_over_time.append(fused_features_for_timestep_t)\n","\n","        stacked_lstm_input_features = torch.stack(all_lstm_input_features_over_time, dim=1) # Renamed\n","\n","        if initial_hidden_state[0].shape[1] != stacked_lstm_input_features.shape[0]: # Check h0's N_nodes\n","            print(f\"CRITICAL WARNING: Node count mismatch for LSTM h0 ({initial_hidden_state[0].shape[1]}) and LSTM input sequence ({stacked_lstm_input_features.shape[0]}).\")\n","            if initial_hidden_state[0].shape[1] > stacked_lstm_input_features.shape[0]:\n","                h0_adj = initial_hidden_state[0][:, :stacked_lstm_input_features.shape[0], :]\n","                c0_adj = initial_hidden_state[1][:, :stacked_lstm_input_features.shape[0], :]\n","                initial_hidden_state = (h0_adj, c0_adj)\n","\n","        lstm_out, _ = self.lstm(stacked_lstm_input_features, initial_hidden_state) # Changed from self.gru\n","\n","        all_hourly_final_predictions_scaled = []\n","        for t in range(self.T_pred_horizon):\n","            lstm_out_t = lstm_out[:, t, :] # Use lstm_out\n","            prediction_t_scaled = self.hourly_prediction_heads[t](lstm_out_t)\n","            all_hourly_final_predictions_scaled.append(prediction_t_scaled.squeeze(-1))\n","\n","        predictions_scaled = torch.stack(all_hourly_final_predictions_scaled, dim=1)\n","        return predictions_scaled\n","\n","# ===========================================================\n","# 3. 评估指标函数 (在原始尺度上计算指标) - NO CHANGES\n","# ===========================================================\n","def mse_loss_masked(predictions_scaled, targets_scaled, mask):\n","    expanded_mask = mask.unsqueeze(1).expand_as(targets_scaled)\n","    valid_targets_mask = ~torch.isnan(targets_scaled)\n","    final_mask = expanded_mask & valid_targets_mask\n","    if final_mask.sum() == 0:\n","        return torch.tensor(0.0, device=predictions_scaled.device, requires_grad=True)\n","    loss = F.mse_loss(predictions_scaled[final_mask], targets_scaled[final_mask])\n","    return loss\n","\n","def calculate_hourly_metrics(predictions_scaled, targets_scaled, node_masks, target_mean, target_std):\n","    target_mean_cpu = target_mean.cpu()\n","    target_std_cpu = target_std.cpu()\n","    preds_unscaled = predictions_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","    targets_unscaled = targets_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","\n","    num_nodes_total, T_horizon = preds_unscaled.shape\n","    hourly_metrics_dict = {}\n","\n","    preds_np = preds_unscaled.numpy()\n","    targets_np = targets_unscaled.numpy()\n","    mask_np = node_masks.cpu().numpy()\n","\n","    for t in range(T_horizon):\n","        preds_t_all_nodes = preds_np[:, t]\n","        targets_t_all_nodes = targets_np[:, t]\n","        preds_t_on_loss_nodes = preds_t_all_nodes[mask_np]\n","        targets_t_on_loss_nodes = targets_t_all_nodes[mask_np]\n","        valid_target_data_mask_t = ~np.isnan(targets_t_on_loss_nodes)\n","        preds_t_final_valid = preds_t_on_loss_nodes[valid_target_data_mask_t]\n","        targets_t_final_valid = targets_t_on_loss_nodes[valid_target_data_mask_t]\n","\n","        if preds_t_final_valid.shape[0] < 2:\n","            hourly_metrics_dict[t] = {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count': 0}\n","            continue\n","        mse = np.mean((preds_t_final_valid - targets_t_final_valid)**2)\n","        mae = np.mean(np.abs(preds_t_final_valid - targets_t_final_valid))\n","        rmse = np.sqrt(mse)\n","        try:\n","            r2 = r2_score(targets_t_final_valid, preds_t_final_valid)\n","        except ValueError:\n","            r2 = np.nan\n","        hourly_metrics_dict[t] = {'mse': mse, 'mae': mae, 'rmse': rmse, 'r2': r2, 'count': preds_t_final_valid.shape[0]}\n","    return hourly_metrics_dict\n","\n","# ===========================================================\n","# 4. 训练与评估循环 (适配y归一化) - NO CHANGES\n","# ===========================================================\n","def train_epoch(model, loader, optimizer, device, timeline_time_features,\n","                node_feat_mean, node_feat_std, target_mean, target_std):\n","    model.train()\n","    total_loss_scaled = 0\n","    num_sequences_processed = 0\n","    model.node_feat_mean = node_feat_mean.to(device)\n","    model.node_feat_std = node_feat_std.to(device)\n","\n","    target_mean_dev = target_mean.to(device)\n","    target_std_dev = target_std.to(device)\n","\n","    epoch_start_time = time.time()\n","    for list_of_batched_timesteps in loader:\n","        optimizer.zero_grad()\n","        predictions_batch_scaled = model(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","\n","        targets_list_for_loss_scaled = []\n","        first_predicted_timestep_batch = list_of_batched_timesteps[1].to(device)\n","        mask_for_loss = ~first_predicted_timestep_batch.building_mask\n","\n","        for t_pred_idx in range(model.T_pred_horizon):\n","            current_target_timestep_batch = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","            targets_t_nodes_original = current_target_timestep_batch.y.squeeze()\n","            targets_t_nodes_scaled = (targets_t_nodes_original - target_mean_dev) / (target_std_dev + 1e-8)\n","            targets_list_for_loss_scaled.append(targets_t_nodes_scaled)\n","\n","        targets_batch_scaled = torch.stack(targets_list_for_loss_scaled, dim=1)\n","        loss = mse_loss_masked(predictions_batch_scaled, targets_batch_scaled, mask_for_loss)\n","        num_sequences_in_this_super_batch = list_of_batched_timesteps[0].num_graphs\n","\n","        if not torch.isnan(loss) and not torch.isinf(loss) and loss.item() > 0 :\n","            loss.backward()\n","            optimizer.step()\n","            total_loss_scaled += loss.item() * num_sequences_in_this_super_batch\n","        num_sequences_processed += num_sequences_in_this_super_batch\n","\n","    epoch_duration = time.time() - epoch_start_time\n","    avg_loss = total_loss_scaled / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","    return avg_loss, epoch_duration\n","\n","def evaluate_epoch(model, loader, device, timeline_time_features,\n","                   node_feat_mean, node_feat_std, target_mean, target_std, epoch_type=\"Eval\"):\n","    model.eval()\n","    all_batch_predictions_scaled = []\n","    all_batch_targets_scaled = []\n","    all_batch_masks_for_metrics = []\n","    total_loss_scaled = 0\n","    num_sequences_processed = 0\n","    model.node_feat_mean = node_feat_mean.to(device)\n","    model.node_feat_std = node_feat_std.to(device)\n","\n","    target_mean_dev = target_mean.to(device)\n","    target_std_dev = target_std.to(device)\n","\n","    eval_start_time = time.time()\n","    with torch.no_grad():\n","        for list_of_batched_timesteps in loader:\n","            predictions_batch_scaled = model(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","            targets_list_original = []\n","            first_predicted_timestep_batch = list_of_batched_timesteps[1].to(device)\n","            mask_for_metrics = ~first_predicted_timestep_batch.building_mask\n","\n","            for t_pred_idx in range(model.T_pred_horizon):\n","                current_target_timestep_batch = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","                targets_t_nodes_original = current_target_timestep_batch.y.squeeze()\n","                targets_list_original.append(targets_t_nodes_original)\n","\n","            targets_batch_original = torch.stack(targets_list_original, dim=1)\n","            targets_batch_scaled_for_loss = (targets_batch_original - target_mean_dev) / (target_std_dev + 1e-8)\n","            loss = mse_loss_masked(predictions_batch_scaled, targets_batch_scaled_for_loss, mask_for_metrics)\n","            num_sequences_in_this_super_batch = list_of_batched_timesteps[0].num_graphs\n","\n","            if not torch.isnan(loss) and not torch.isinf(loss):\n","                total_loss_scaled += loss.item() * num_sequences_in_this_super_batch\n","            num_sequences_processed += num_sequences_in_this_super_batch\n","\n","            all_batch_predictions_scaled.append(predictions_batch_scaled.cpu())\n","            all_batch_targets_scaled.append(targets_batch_scaled_for_loss.cpu())\n","            all_batch_masks_for_metrics.append(mask_for_metrics.cpu())\n","\n","    eval_duration = time.time() - eval_start_time\n","    avg_loss_scaled = total_loss_scaled / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","\n","    if not all_batch_predictions_scaled:\n","        empty_metrics = {t: {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0} for t in range(model.T_pred_horizon)}\n","        return avg_loss_scaled, empty_metrics, eval_duration\n","\n","    final_predictions_scaled = torch.cat(all_batch_predictions_scaled, dim=0)\n","    final_targets_scaled = torch.cat(all_batch_targets_scaled, dim=0)\n","    final_masks_for_metrics = torch.cat(all_batch_masks_for_metrics, dim=0)\n","\n","    hourly_metrics_original_scale = calculate_hourly_metrics(final_predictions_scaled, final_targets_scaled,\n","                                                             final_masks_for_metrics, target_mean.cpu(), target_std.cpu())\n","    return avg_loss_scaled, hourly_metrics_original_scale, eval_duration\n","\n","# ===========================================================\n","# 5. 主训练流程 (RGCN-LSTM)\n","# ===========================================================\n","\n","def calculate_aggregated_metrics_report(hourly_metrics_dict, T_pred_horizon):\n","    metrics_to_aggregate = ['r2', 'mse', 'mae', 'rmse']\n","    aggregated_report = {}\n","    for metric_name in metrics_to_aggregate:\n","        values = [hourly_metrics_dict[t][metric_name] for t in range(T_pred_horizon) if t in hourly_metrics_dict and not np.isnan(hourly_metrics_dict[t][metric_name])]\n","        if values:\n","            aggregated_report[f'avg_{metric_name}'] = np.mean(values)\n","            aggregated_report[f'std_{metric_name}'] = np.std(values)\n","        else:\n","            aggregated_report[f'avg_{metric_name}'] = np.nan\n","            aggregated_report[f'std_{metric_name}'] = np.nan\n","    return aggregated_report\n","\n","\n","def main_training_rgcn_lstm_hourly_heads( # Renamed\n","    all_sequences_data: list,\n","    config: dict,\n","    time_features_for_dataset: torch.Tensor\n","):\n","    train_start_time = time.time()\n","    report_data = {'config': config}\n","\n","    seed = config.get('seed', 42)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"使用设备: {device}\")\n","    report_data['device'] = str(device)\n","\n","    T_PRED_HORIZON = config.get('T_pred_horizon', 12)\n","    expected_input_len = T_PRED_HORIZON + 1\n","    valid_sequences_data = [seq for seq in all_sequences_data if isinstance(seq, list) and len(seq) == expected_input_len]\n","    if len(valid_sequences_data) != len(all_sequences_data):\n","        print(f\"警告: 从 {len(all_sequences_data)} 个序列中筛选出 {len(valid_sequences_data)} 个长度为 {expected_input_len} 的有效序列。\")\n","    if not valid_sequences_data:\n","        raise ValueError(f\"没有找到长度为 {expected_input_len} 的有效序列数据。\")\n","    all_sequences_data = valid_sequences_data\n","\n","    # Dataset split\n","    num_total_sequences = len(all_sequences_data)\n","    indices = np.random.permutation(num_total_sequences)\n","    train_split_ratio = config.get('train_split_ratio', 0.7)\n","    val_split_ratio = config.get('val_split_ratio', 0.2)\n","    train_size = int(train_split_ratio * num_total_sequences)\n","    val_size = int(val_split_ratio * num_total_sequences)\n","    train_indices = indices[:train_size]\n","    val_indices = indices[train_size : train_size + val_size]\n","    test_indices = indices[train_size + val_size :]\n","    train_dataset = [all_sequences_data[i] for i in train_indices]\n","    val_dataset   = [all_sequences_data[i] for i in val_indices]\n","    test_dataset  = [all_sequences_data[i] for i in test_indices]\n","    report_data['dataset_split'] = {'total_sequences': num_total_sequences, 'train_size': len(train_dataset), 'val_size': len(val_dataset), 'test_size': len(test_dataset)}\n","\n","    # Scaler calculation\n","    all_train_node_features_list = []\n","    all_train_target_values_list_for_scaling = []\n","    for seq in train_dataset:\n","        for i_step, graph_data in enumerate(seq):\n","            if hasattr(graph_data, 'x') and graph_data.x is not None: all_train_node_features_list.append(graph_data.x)\n","            if i_step > 0 and hasattr(graph_data, 'y') and graph_data.y is not None:\n","                y_original = graph_data.y.squeeze(); current_mask_for_loss = ~graph_data.building_mask\n","                valid_target_indices = current_mask_for_loss & ~torch.isnan(y_original)\n","                if valid_target_indices.sum() > 0: all_train_target_values_list_for_scaling.append(y_original[valid_target_indices])\n","    if not all_train_node_features_list: raise ValueError(\"训练数据中未找到节点特征 'x'，无法计算scaler！\")\n","    all_train_node_features_tensor = torch.cat(all_train_node_features_list, dim=0)\n","    node_feat_mean = torch.mean(all_train_node_features_tensor, dim=0); node_feat_std = torch.std(all_train_node_features_tensor, dim=0)\n","    node_feat_std[node_feat_std < 1e-8] = 1.0\n","    scaler_path_x = Path(config['results_dir']) / \"node_feature_scaler_rgcn_lstm.pth\" # Renamed\n","    torch.save({'mean': node_feat_mean, 'std': node_feat_std}, scaler_path_x); print(f\"节点特征x scaler已保存到: {scaler_path_x}\")\n","    if not all_train_target_values_list_for_scaling:\n","        target_mean = torch.tensor(0.0); target_std = torch.tensor(1.0)\n","    else:\n","        all_train_target_values_tensor = torch.cat(all_train_target_values_list_for_scaling, dim=0)\n","        target_mean = torch.mean(all_train_target_values_tensor.float()); target_std = torch.std(all_train_target_values_tensor.float())\n","        if target_std < 1e-8: target_std = torch.tensor(1.0)\n","    target_scaler_path = Path(config['results_dir']) / \"target_scaler_rgcn_lstm.pth\" # Renamed\n","    torch.save({'mean': target_mean, 'std': target_std}, target_scaler_path); print(f\"目标值y scaler已保存到: {target_scaler_path}\")\n","\n","    # DataLoaders\n","    batch_size = config.get('batch_size', 8); num_workers = config.get('num_workers', 0)\n","    pin_memory_flag = config.get('pin_memory', False) and device.type == 'cuda'\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=num_workers, pin_memory=pin_memory_flag)\n","    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n","    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n","\n","    # Model Initialization\n","    sample_graph_7am_for_dims = all_sequences_data[0][0]\n","    static_node_in_dim = sample_graph_7am_for_dims.x.shape[1]\n","    global_env_in_dim = sample_graph_7am_for_dims.graph_global_env_features.shape[0] if sample_graph_7am_for_dims.graph_global_env_features.ndim == 1 else sample_graph_7am_for_dims.graph_global_env_features.shape[1]\n","    time_in_dim = time_features_for_dataset.shape[1]\n","    num_relations = config.get('num_relations', 5)\n","\n","    model = RGCNLSTMModelWithHourlyHeads( # Use RGCNLSTMModel\n","        static_node_in_dim=static_node_in_dim, global_env_in_dim=global_env_in_dim, time_in_dim=time_in_dim,\n","        global_env_emb_dim=config.get('global_env_emb_dim', 16), time_emb_dim=config.get('time_emb_dim', 8),\n","        rgcn_hidden_dim=config.get('gcn_hidden_dim', 128), rgcn_output_dim=config.get('gcn_output_dim', 128),\n","        num_relations=num_relations,\n","        lstm_hidden_dim=config.get('gru_hidden_dim', 128), # Use existing config key for hidden_dim\n","        fusion_mlp_output_dim=config.get('fusion_mlp_output_dim', 128),\n","        fusion_mlp_hidden_dim=config.get('fusion_mlp_hidden_dim', 64),\n","        dropout_rate_fusion_mlp=config.get('dropout_rate_fusion_mlp', 0.2),\n","        num_lstm_layers=config.get('num_gru_layers', 1), # Use existing config key for num_layers\n","        T_pred_horizon=T_PRED_HORIZON,\n","        dropout_rate_encoders=config.get('dropout_rate_encoders', 0.1),\n","        dropout_rate_rgcn=config.get('dropout_rate_gcn', 0.3),\n","        dropout_rate_lstm=config.get('dropout_rate_gru', 0.2), # Use existing config key for dropout\n","        mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim', 64),\n","        dropout_rate_pred_head=config.get('dropout_rate_pred_head', 0.2)\n","    ).to(device)\n","    model.node_feat_mean = node_feat_mean.to(device); model.node_feat_std = node_feat_std.to(device)\n","    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"模型总参数量 (RGCN-LSTM): {total_params:,}\"); report_data['model_total_parameters'] = total_params\n","\n","    # ===== Component FLOPS Calculation =====\n","    print(\"\\nCalculating MACs for model components (approximate FLOPS):\")\n","    report_data['component_gmacs'] = {}\n","    dummy_nodes_component = 2500\n","    dummy_edges_component = 60000\n","    dummy_batch_global_comp = 1\n","\n","    model.eval()\n","\n","    # 1. RGCNModule (No change)\n","    try:\n","        rgcn_module_to_profile = model.rgcn_module_for_h0\n","        dummy_x_rgcn = torch.randn(dummy_nodes_component, rgcn_module_to_profile.rgcn_input_dim, device=device)\n","        dummy_ei_rgcn = torch.randint(0, dummy_nodes_component, (2, dummy_edges_component), device=device)\n","        dummy_ea_rgcn = torch.randn(dummy_edges_component, 5, device=device)\n","        dummy_ea_rgcn[:, 4] = torch.randint(0, model.num_relations, (dummy_edges_component,), device=device).float()\n","        macs_rgcn = torchprofile.profile_macs(rgcn_module_to_profile, args=(dummy_x_rgcn, dummy_ei_rgcn, dummy_ea_rgcn))\n","        report_data['component_gmacs']['rgcn_module'] = macs_rgcn / 1e9\n","        print(f\"  RGCNModule GMACs: {macs_rgcn / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling RGCNModule: {e}\"); report_data['component_gmacs']['rgcn_module'] = \"Error\"\n","\n","    # 2. LSTM Layer (Manual MAC Calculation)\n","    print(f\"  Manually Calculating MACs for LSTM Layer:\")\n","    try:\n","        lstm_layer = model.lstm # Changed from model.gru\n","        N_nodes = dummy_nodes_component\n","        L_seq = T_PRED_HORIZON\n","        H_in = lstm_layer.input_size\n","        H_hidden = lstm_layer.hidden_size\n","        num_layers = lstm_layer.num_layers\n","\n","        macs_lstm_manual = 0\n","        # For a single layer LSTM: MACs ≈ N * L * 4 * (H_in * H_hidden + H_hidden^2)\n","        # (Input, Forget, Cell, Output gates each have similar complexity to a GRU gate part)\n","        macs_lstm_manual = N_nodes * L_seq * 4 * (H_in * H_hidden + H_hidden * H_hidden) # For the first layer\n","        if num_layers > 1:\n","            # Subsequent (num_layers - 1) layers: N * L * 4 * (H_hidden * H_hidden + H_hidden^2)\n","            # (because input to subsequent layers is H_hidden from the layer below)\n","            macs_lstm_manual += N_nodes * L_seq * (num_layers - 1) * 4 * (H_hidden * H_hidden + H_hidden * H_hidden)\n","\n","        gmacs_lstm_manual = macs_lstm_manual / 1e9\n","        report_data['component_gmacs']['lstm_layer'] = gmacs_lstm_manual # Renamed from gru_layer\n","        report_data['component_gmacs']['lstm_layer_profiling_notes'] = \"Manually calculated based on formula.\"\n","        print(f\"  LSTM Parameters: input_size={H_in}, hidden_size={H_hidden}, num_layers={num_layers}\")\n","        print(f\"  Used for calculation: N_nodes={N_nodes}, L_seq={L_seq}\")\n","        print(f\"  LSTM Layer GMACs (Manual): {gmacs_lstm_manual:.4f} (for sequence length {L_seq})\")\n","\n","    except Exception as e:\n","        print(f\"  Error manually calculating LSTM Layer MACs: {e}\")\n","        report_data['component_gmacs']['lstm_layer'] = \"Error\" # Renamed\n","        report_data['component_gmacs']['lstm_layer_profiling_notes'] = f\"Error during manual calculation: {str(e)}\"\n","\n","    # 3. Fusion MLP (MLPEncoder) - No change\n","    try:\n","        fusion_mlp_to_profile = model.fusion_mlp\n","        dummy_input_fusion_mlp = torch.randn(dummy_nodes_component, model.fusion_mlp_input_dim, device=device)\n","        macs_fusion_mlp = torchprofile.profile_macs(fusion_mlp_to_profile, args=(dummy_input_fusion_mlp,))\n","        report_data['component_gmacs']['fusion_mlp'] = macs_fusion_mlp / 1e9\n","        print(f\"  Fusion MLP GMACs: {macs_fusion_mlp / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Fusion MLP: {e}\"); report_data['component_gmacs']['fusion_mlp'] = \"Error\"\n","\n","    # 4. Prediction Head (one MLP from ModuleList) - Input dim is now lstm_hidden_dim\n","    try:\n","        pred_head_to_profile = model.hourly_prediction_heads[0]\n","        dummy_input_pred_head = torch.randn(dummy_nodes_component, model.lstm_hidden_dim, device=device) # Uses lstm_hidden_dim\n","        macs_pred_head = torchprofile.profile_macs(pred_head_to_profile, args=(dummy_input_pred_head,))\n","        report_data['component_gmacs']['prediction_head_mlp'] = macs_pred_head / 1e9\n","        print(f\"  Prediction Head MLP (single hour) GMACs: {macs_pred_head / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Prediction Head: {e}\"); report_data['component_gmacs']['prediction_head_mlp'] = \"Error\"\n","\n","    # 5. Global Environment Encoder (MLPEncoder) - No change\n","    try:\n","        encoder_to_profile = model.global_env_encoder\n","        dummy_input_encoder = torch.randn(dummy_batch_global_comp, model.global_env_in_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['global_env_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  Global Env Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Global Env Encoder: {e}\"); report_data['component_gmacs']['global_env_encoder_mlp'] = \"Error\"\n","\n","    # 6. Time Encoder (MLPEncoder) - No change\n","    try:\n","        encoder_to_profile = model.time_encoder\n","        dummy_input_encoder = torch.randn(dummy_batch_global_comp, model.time_in_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['time_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  Time Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Time Encoder: {e}\"); report_data['component_gmacs']['time_encoder_mlp'] = \"Error\"\n","\n","    # 7. H0/C0 from RGCN Encoder (MLPEncoder) - Output dim is now lstm_hidden_dim\n","    try:\n","        encoder_to_profile = model.h0_c0_from_rgcn_encoder # Renamed internal variable\n","        dummy_input_encoder = torch.randn(dummy_nodes_component, model.rgcn_output_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['h0_c0_from_rgcn_encoder_mlp'] = macs_encoder / 1e9 # Renamed key\n","        print(f\"  H0/C0 from RGCN Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling H0/C0 from RGCN Encoder: {e}\"); report_data['component_gmacs']['h0_c0_from_rgcn_encoder_mlp'] = \"Error\"\n","\n","    model.train()\n","    # ===== End Component FLOPS Calculation =====\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config.get('lr', 0.001), weight_decay=config.get('weight_decay', 1e-5))\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=config.get('scheduler_patience', 20), verbose=True)\n","\n","    best_val_loss_scaled = float('inf'); best_val_hourly_metrics_original = None; best_epoch = 0\n","    patience_counter = 0; max_epochs = config.get('max_epochs', 300); early_stopping_patience = config.get('early_stopping_patience', 45)\n","    model_save_path = Path(config['results_dir']) / f\"best_rgcnlstm_hourly_heads_model_seed{seed}.pth\" # Renamed\n","    timeline_time_features_on_device = time_features_for_dataset.to(device)\n","    target_mean_cpu = target_mean.cpu(); target_std_cpu = target_std.cpu()\n","    target_mean_on_device = target_mean.to(device); target_std_on_device = target_std.to(device)\n","    epoch_times = []\n","\n","    for epoch in range(1, max_epochs + 1):\n","        train_loss_scaled, epoch_duration = train_epoch(model, train_loader, optimizer, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_on_device, target_std_on_device)\n","        epoch_times.append(epoch_duration)\n","        val_loss_scaled, val_hourly_metrics_original, _ = evaluate_epoch(model, val_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Validation\")\n","        scheduler.step(val_loss_scaled)\n","        print(f\"Epoch {epoch:03d} | Train Scaled MSE: {train_loss_scaled:.4f} | Val Scaled MSE: {val_loss_scaled:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f} | Epoch Time: {epoch_duration:.2f}s\")\n","        _print_hourly_metrics_summary(\"Val\", val_hourly_metrics_original, T_PRED_HORIZON, indent=\"                     \")\n","        if val_loss_scaled < best_val_loss_scaled:\n","            best_val_loss_scaled = val_loss_scaled; best_val_hourly_metrics_original = val_hourly_metrics_original; best_epoch = epoch\n","            patience_counter = 0; torch.save(model.state_dict(), model_save_path)\n","            print(f\"                     ---> Best model saved (Epoch: {epoch}, Val Scaled MSE: {best_val_loss_scaled:.4f})\")\n","        else:\n","            patience_counter += 1\n","        if patience_counter >= early_stopping_patience:\n","            print(f\"Early stopping at epoch {epoch} due to no improvement.\"); break\n","\n","    total_training_duration = time.time() - train_start_time\n","    report_data['total_training_time_seconds'] = total_training_duration\n","    report_data['average_epoch_time_seconds'] = np.mean(epoch_times) if epoch_times else np.nan\n","    report_data['num_epochs_trained'] = epoch; report_data['best_validation_epoch'] = best_epoch\n","    report_data['best_validation_scaled_mse'] = best_val_loss_scaled\n","\n","    model_for_eval = RGCNLSTMModelWithHourlyHeads( # Use RGCNLSTMModel\n","        static_node_in_dim=static_node_in_dim, global_env_in_dim=global_env_in_dim, time_in_dim=time_in_dim,\n","        global_env_emb_dim=config.get('global_env_emb_dim', 16), time_emb_dim=config.get('time_emb_dim', 8),\n","        rgcn_hidden_dim=config.get('gcn_hidden_dim', 128), rgcn_output_dim=config.get('gcn_output_dim', 128),\n","        num_relations=num_relations,\n","        lstm_hidden_dim=config.get('gru_hidden_dim', 128),\n","        fusion_mlp_output_dim=config.get('fusion_mlp_output_dim', 128),\n","        fusion_mlp_hidden_dim=config.get('fusion_mlp_hidden_dim', 64),\n","        dropout_rate_fusion_mlp=config.get('dropout_rate_fusion_mlp', 0.2),\n","        num_lstm_layers=config.get('num_gru_layers', 1), T_pred_horizon=T_PRED_HORIZON,\n","        dropout_rate_encoders=config.get('dropout_rate_encoders', 0.1),\n","        dropout_rate_rgcn=config.get('dropout_rate_gcn', 0.3),\n","        dropout_rate_lstm=config.get('dropout_rate_gru', 0.2),\n","        mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim', 64),\n","        dropout_rate_pred_head=config.get('dropout_rate_pred_head', 0.2)\n","    ).to(device)\n","    try: model_for_eval.load_state_dict(torch.load(model_save_path, map_location=device))\n","    except Exception as e: print(f\"无法加载最佳模型 ({e})，将使用训练循环结束时的模型。\"); model_for_eval = model\n","\n","    print(\"\\n评估最佳模型在训练集上...\"); best_model_train_loss_scaled, best_model_train_hourly_metrics, train_eval_duration = evaluate_epoch(model_for_eval, train_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Best Model on Train\")\n","    report_data['best_model_train_set_metrics_hourly'] = best_model_train_hourly_metrics; report_data['best_model_train_set_metrics_aggregated'] = calculate_aggregated_metrics_report(best_model_train_hourly_metrics, T_PRED_HORIZON)\n","    report_data['best_model_train_set_eval_time_seconds'] = train_eval_duration; _print_hourly_metrics_summary(\"最佳模型训练集\", best_model_train_hourly_metrics, T_PRED_HORIZON)\n","\n","    report_data['best_model_validation_set_metrics_hourly'] = best_val_hourly_metrics_original\n","    if best_val_hourly_metrics_original: report_data['best_model_validation_set_metrics_aggregated'] = calculate_aggregated_metrics_report(best_val_hourly_metrics_original, T_PRED_HORIZON)\n","    else:\n","        _, reeval_val_metrics, val_eval_duration = evaluate_epoch(model_for_eval, val_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Best Model on Val (Re-eval)\")\n","        report_data['best_model_validation_set_metrics_hourly'] = reeval_val_metrics\n","        report_data['best_model_validation_set_metrics_aggregated'] = calculate_aggregated_metrics_report(reeval_val_metrics, T_PRED_HORIZON)\n","        report_data['best_model_validation_set_eval_time_seconds'] = val_eval_duration\n","    _print_hourly_metrics_summary(\"最佳模型验证集\", report_data['best_model_validation_set_metrics_hourly'], T_PRED_HORIZON)\n","\n","\n","    print(\"\\n评估最佳模型在测试集上...\"); test_loss_scaled, test_hourly_metrics_original, test_inference_duration = evaluate_epoch(model_for_eval, test_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Test\")\n","    report_data['test_set_inference_time_seconds'] = test_inference_duration; report_data['best_model_test_set_metrics_hourly'] = test_hourly_metrics_original; report_data['best_model_test_set_metrics_aggregated'] = calculate_aggregated_metrics_report(test_hourly_metrics_original, T_PRED_HORIZON)\n","    print(\"\\n\" + \"=\"*20 + \" 最终测试集评估结果 (RGCN-LSTM) \" + \"=\"*20); print(f\"平均测试 Scaled MSE: {test_loss_scaled:.4f}\") # Renamed\n","    _print_hourly_metrics_summary(\"测试集\", test_hourly_metrics_original, T_PRED_HORIZON)\n","\n","    agg_test = report_data['best_model_test_set_metrics_aggregated']\n","    print(f\"平均测试 MSE (Orig) : {agg_test.get('avg_mse', np.nan):.4f} (Std: {agg_test.get('std_mse', np.nan):.4f})\")\n","    print(f\"平均测试 R2 (Orig)  : {agg_test.get('avg_r2', np.nan):.4f} (Std: {agg_test.get('std_r2', np.nan):.4f})\")\n","    print(f\"平均测试 MAE (Orig) : {agg_test.get('avg_mae', np.nan):.4f} (Std: {agg_test.get('std_mae', np.nan):.4f})\")\n","    print(f\"平均测试 RMSE (Orig): {agg_test.get('avg_rmse', np.nan):.4f} (Std: {agg_test.get('std_rmse', np.nan):.4f})\")\n","    print(\"=\"*70)\n","\n","    report_file_path = Path(config['results_dir']) / f\"training_report_lstm_seed{seed}.json\" # Renamed\n","    try:\n","        class NpEncoder(json.JSONEncoder):\n","            def default(self, obj):\n","                if isinstance(obj, np.integer): return int(obj)\n","                if isinstance(obj, np.floating): return float(obj)\n","                if isinstance(obj, np.ndarray): return obj.tolist()\n","                if isinstance(obj, torch.Tensor): return obj.tolist()\n","                if isinstance(obj, Path): return str(obj)\n","                return super(NpEncoder, self).default(obj)\n","        with open(report_file_path, 'w') as f: json.dump(report_data, f, indent=4, cls=NpEncoder)\n","        print(f\"训练报告已保存到: {report_file_path}\")\n","    except Exception as e: print(f\"保存训练报告失败: {e}\")\n","\n","    return model_for_eval, node_feat_mean, node_feat_std, target_mean, target_std\n","\n","\n","def _print_hourly_metrics_summary(set_name, hourly_metrics, T_pred_horizon, indent=\"  \"):\n","    if hourly_metrics is None:\n","        print(f\"{indent}{set_name} metrics not available.\")\n","        return\n","    print(f\"\\n{indent}每小时 {set_name} 指标 (Original Scale):\")\n","    header_printed = False\n","    for hour_idx in range(T_pred_horizon):\n","        metrics = hourly_metrics.get(hour_idx, {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0})\n","        if not header_printed:\n","            print(f\"{indent}  Hour | {'R2':>13s} | {'MSE':>14s} | {'MAE':>14s} | {'RMSE':>15s} | {'Count':>7s}\")\n","            header_printed = True\n","        print(f\"{indent}  {hour_idx:02d}   | {metrics.get('r2', np.nan):13.4f} | {metrics.get('mse', np.nan):14.4f} | {metrics.get('mae', np.nan):14.4f} | {metrics.get('rmse', np.nan):15.4f} | {metrics.get('count', 0):7d}\")\n","    aggregated = calculate_aggregated_metrics_report(hourly_metrics, T_pred_horizon)\n","    print(f\"{indent}  Aggregated Avg R2   : {aggregated.get('avg_r2', np.nan):.4f} (Std: {aggregated.get('std_r2', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MSE  : {aggregated.get('avg_mse', np.nan):.4f} (Std: {aggregated.get('std_mse', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MAE  : {aggregated.get('avg_mae', np.nan):.4f} (Std: {aggregated.get('std_mae', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg RMSE : {aggregated.get('avg_rmse', np.nan):.4f} (Std: {aggregated.get('std_rmse', np.nan):.4f})\")\n","\n","\n","# ===========================================================\n","# 6. 主执行块\n","# ===========================================================\n","if __name__ == \"__main__\":\n","    gc.collect()\n","    if torch.cuda.is_available(): torch.cuda.empty_cache()\n","\n","    DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","    if not DRIVE_BASE_PATH.exists(): DRIVE_BASE_PATH.mkdir(parents=True, exist_ok=True)\n","\n","    DATA_SUBDIR = Path(\"Result/Sequential_12Hour_Data\") # Assuming same data structure\n","    DATA_FILENAME = \"graph_seq_20230503_SeqH7to19_NpyH8fill0.0.pkl\" # Using same dummy data name\n","    RESULTS_SUBDIR = Path(\"Result/Final_RGCNLSTM1\") # Renamed output subdir\n","    RESULTS_SAVE_DIR = DRIVE_BASE_PATH / RESULTS_SUBDIR\n","    os.makedirs(RESULTS_SAVE_DIR, exist_ok=True)\n","    DATA_PATH = DRIVE_BASE_PATH / DATA_SUBDIR / DATA_FILENAME\n","\n","    DATA_YEAR = 2023; DATA_MONTH = 5; DATA_DAY = 3\n","    START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES = 8; PREDICTION_HORIZON = 12\n","    # Config keys for GRU ('gru_hidden_dim', 'num_gru_layers', 'dropout_rate_gru')\n","    # will be used to provide values for the LSTM layer to maintain parameter consistency as requested.\n","    training_config = {\n","        'seed': 42, 'batch_size': 8, 'lr': 0.001, 'weight_decay': 1e-5,\n","        'max_epochs': 1000, 'scheduler_patience': 20, 'early_stopping_patience': 45, # Shortened for quick test\n","        'T_pred_horizon': PREDICTION_HORIZON, 'results_dir': str(RESULTS_SAVE_DIR),\n","        'global_env_emb_dim': 16, 'time_emb_dim': 8,\n","        'gcn_hidden_dim': 128, 'gcn_output_dim': 128, # Example smaller dims for test\n","        'num_relations': 5,\n","        'gru_hidden_dim': 128, # This will be lstm_hidden_dim\n","        'num_gru_layers': 1,  # This will be num_lstm_layers\n","        'mlp_prediction_hidden_dim': 64,\n","        'fusion_mlp_output_dim': 128, 'fusion_mlp_hidden_dim': 64,\n","        'dropout_rate_fusion_mlp': 0.2, 'dropout_rate_encoders': 0.1,\n","        'dropout_rate_gcn': 0.3,\n","        'dropout_rate_gru': 0.2,  # This will be dropout_rate_lstm\n","        'dropout_rate_pred_head': 0.2, 'use_amp': False, 'enable_profiler': False, 'num_workers': 0,\n","        'pin_memory': False, 'train_split_ratio': 0.7, 'val_split_ratio': 0.2, 'h0_from_first_step': True\n","    }\n","\n","    all_graph_sequences_loaded = None\n","    try:\n","        if not DATA_PATH.exists(): raise FileNotFoundError(f\"数据文件在指定路径未找到: {DATA_PATH}\")\n","        with open(DATA_PATH, \"rb\") as f: all_graph_sequences_loaded = pickle.load(f)\n","        if not all_graph_sequences_loaded or not isinstance(all_graph_sequences_loaded, list) or not all_graph_sequences_loaded[0] or not isinstance(all_graph_sequences_loaded[0], list):\n","            raise ValueError(\"加载的数据格式不正确。\")\n","        expected_len_per_sequence = training_config['T_pred_horizon'] + 1\n","        processed_sequences = []\n","        for i, seq in enumerate(all_graph_sequences_loaded):\n","            if not isinstance(seq, list) or len(seq) != expected_len_per_sequence: continue\n","            valid_seq = True\n","            for step_idx, graph_step_data in enumerate(seq):\n","                if not isinstance(graph_step_data, Data) or not hasattr(graph_step_data, 'x') or graph_step_data.x is None or \\\n","                   not hasattr(graph_step_data, 'edge_index') or graph_step_data.edge_index is None or \\\n","                   not hasattr(graph_step_data, 'edge_attr') or graph_step_data.edge_attr is None or \\\n","                   graph_step_data.edge_attr.shape[1] < 5 or \\\n","                   not hasattr(graph_step_data, 'graph_global_env_features') or \\\n","                   (step_idx > 0 and (not hasattr(graph_step_data, 'y') or graph_step_data.y is None)):\n","                    valid_seq = False; break\n","                if step_idx > 0 and isinstance(graph_step_data.y, torch.Tensor) and graph_step_data.y.ndim == 1:\n","                    graph_step_data.y = graph_step_data.y.unsqueeze(1)\n","            if valid_seq: processed_sequences.append(seq)\n","        if not processed_sequences: raise ValueError(f\"数据处理后没有长度为 {expected_len_per_sequence} 的有效序列。\")\n","        all_graph_sequences = processed_sequences\n","        print(f\"成功加载并处理 {len(all_graph_sequences)} 个空间窗口的序列数据。\")\n","    except Exception as e: print(f\"加载或验证数据时发生错误: {e}\"); all_graph_sequences = None\n","\n","    if all_graph_sequences:\n","        base_datetime_for_timeline = dt_datetime(DATA_YEAR, DATA_MONTH, DATA_DAY, START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES)\n","        time_features_for_dataset_timeline = generate_time_features_for_sequence(base_datetime_for_timeline, training_config['T_pred_horizon'])\n","        # Call the renamed main training function\n","        trained_model, final_node_mean, final_node_std, final_target_mean, final_target_std = main_training_rgcn_lstm_hourly_heads(\n","            all_graph_sequences, training_config, time_features_for_dataset_timeline\n","        )\n","        print(\"RGCN-LSTM 模型训练和评估完成!\") # Updated print\n","    else:\n","        print(\"由于数据加载失败或数据为空，训练流程未启动。\")"]},{"cell_type":"markdown","metadata":{"id":"bUuXd9vb2AUZ"},"source":["##RGCN+GRU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZQOYc0FvicY"},"outputs":[],"source":["# ===========================================================\n","# 0. 环境 & 依赖\n","# ===========================================================\n","import os\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch_geometric.nn import RGCNConv # ⚡ USE RGCNConv\n","from torch_geometric.data import Data, Batch # Batch很重要\n","from torch_geometric.loader import DataLoader\n","# import matplotlib.pyplot as plt # Not used in the final reporting directly, can be commented if not needed elsewhere\n","import pickle\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.metrics import r2_score\n","import gc\n","import math\n","from datetime import datetime as dt_datetime, timedelta # For time feature\n","from pathlib import Path\n","import time # Added for timing\n","import json # Added for report export\n","import torchprofile # Added for FLOPS calculation\n","\n","torch.cuda.empty_cache()\n","torch.cuda.ipc_collect()\n","gc.collect()\n","\n","# ===========================================================\n","# 1. 特征生成 & 辅助模块\n","# ===========================================================\n","\n","def generate_time_features_for_sequence(base_dt_obj, num_steps):\n","    time_features_list = []\n","    for i in range(num_steps):\n","        current_dt = base_dt_obj + timedelta(hours=i)\n","        hour_norm = current_dt.hour / 23.0\n","        day_of_year_norm = current_dt.timetuple().tm_yday / (366.0 if current_dt.year % 4 == 0 and (current_dt.year % 100 != 0 or current_dt.year % 400 == 0) else 365.0)\n","        hour_sin = math.sin(2 * math.pi * hour_norm)\n","        hour_cos = math.cos(2 * math.pi * hour_norm)\n","        doy_sin = math.sin(2 * math.pi * day_of_year_norm)\n","        doy_cos = math.cos(2 * math.pi * day_of_year_norm)\n","        time_features_list.append(torch.tensor([hour_sin, hour_cos, doy_sin, doy_cos], dtype=torch.float32))\n","    return torch.stack(time_features_list)\n","\n","\n","class MLPEncoder(nn.Module):\n","    def __init__(self, in_dim, out_dim, hid_dim=None, dropout_rate=0.1):\n","        super().__init__()\n","        if hid_dim is None:\n","            hid_dim = max(min(in_dim, out_dim), (in_dim + out_dim) // 2)\n","            if hid_dim == 0 and out_dim > 0 : hid_dim = out_dim\n","            if hid_dim == 0 and in_dim > 0 : hid_dim = in_dim\n","            if hid_dim == 0 : hid_dim = 1\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(in_dim, hid_dim),\n","            nn.ReLU(),\n","            nn.LayerNorm(hid_dim),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hid_dim, out_dim)\n","        )\n","    def forward(self, x):\n","        return self.mlp(x)\n","\n","class RGCNModule(nn.Module): # Renamed from GCNModule\n","    def __init__(self, rgcn_input_dim, rgcn_hidden_dim, rgcn_output_dim, num_relations, dropout_rate=0.5):\n","        super().__init__()\n","        self.rgcn_input_dim = rgcn_input_dim\n","        self.rgcn_hidden_dim = rgcn_hidden_dim\n","        self.rgcn_output_dim = rgcn_output_dim\n","        self.num_relations = num_relations\n","\n","        self.conv1 = RGCNConv(rgcn_input_dim, rgcn_hidden_dim, num_relations)\n","        self.bn1 = nn.BatchNorm1d(rgcn_hidden_dim)\n","        self.prelu1 = nn.PReLU(rgcn_hidden_dim)\n","\n","        self.conv2 = RGCNConv(rgcn_hidden_dim, rgcn_hidden_dim, num_relations)\n","        self.bn2 = nn.BatchNorm1d(rgcn_hidden_dim)\n","        self.prelu2 = nn.PReLU(rgcn_hidden_dim)\n","\n","        self.conv3 = RGCNConv(rgcn_hidden_dim, rgcn_output_dim, num_relations)\n","        self.bn3 = nn.BatchNorm1d(rgcn_output_dim)\n","        self.prelu3 = nn.PReLU(rgcn_output_dim)\n","\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x, edge_index, edge_attr): # Changed edge_weight to edge_attr\n","        if edge_attr is None or edge_attr.shape[1] < 5:\n","            raise ValueError(\"RGCNModule: edge_attr is missing or has insufficient columns for edge_type.\")\n","        edge_type = edge_attr[:, 4].long()\n","\n","        x = self.conv1(x, edge_index, edge_type=edge_type)\n","        x = self.bn1(x)\n","        x = self.prelu1(x)\n","        x = self.dropout(x)\n","\n","        x = self.conv2(x, edge_index, edge_type=edge_type)\n","        x = self.bn2(x)\n","        x = self.prelu2(x)\n","        x = self.dropout(x)\n","\n","        x = self.conv3(x, edge_index, edge_type=edge_type)\n","        x = self.bn3(x)\n","        x = self.prelu3(x)\n","        return x\n","\n","# ===========================================================\n","# 2. RGCN-GRU 模型定义 (包含Fusion MLP, 预测归一化的y)\n","# ===========================================================\n","class RGCNGRUModelWithHourlyHeads(nn.Module): # Renamed\n","    def __init__(self,\n","                 static_node_in_dim,\n","                 global_env_in_dim,\n","                 time_in_dim,\n","                 global_env_emb_dim,\n","                 time_emb_dim,\n","                 rgcn_hidden_dim,\n","                 rgcn_output_dim,\n","                 num_relations,\n","                 gru_hidden_dim,\n","                 fusion_mlp_output_dim=None,\n","                 fusion_mlp_hidden_dim=None,\n","                 dropout_rate_fusion_mlp=0.1,\n","                 num_gru_layers=1,\n","                 T_pred_horizon=12,\n","                 dropout_rate_encoders=0.1,\n","                 dropout_rate_rgcn=0.3,\n","                 dropout_rate_gru=0.2,\n","                 mlp_prediction_hidden_dim=64,\n","                 dropout_rate_pred_head=0.2\n","                ):\n","        super().__init__()\n","        self.T_pred_horizon = T_pred_horizon\n","        self.static_node_in_dim = static_node_in_dim\n","        self.global_env_in_dim = global_env_in_dim\n","        self.time_in_dim = time_in_dim\n","        self.num_relations = num_relations\n","        self.rgcn_output_dim = rgcn_output_dim # Store for component profiling\n","        self.gru_hidden_dim = gru_hidden_dim # Store for component profiling\n","\n","\n","        self.global_env_encoder = MLPEncoder(global_env_in_dim, global_env_emb_dim, dropout_rate=dropout_rate_encoders)\n","        self.time_encoder = MLPEncoder(time_in_dim, time_emb_dim, dropout_rate=dropout_rate_encoders)\n","        self.h0_from_rgcn_encoder = MLPEncoder(rgcn_output_dim, gru_hidden_dim, dropout_rate=dropout_rate_encoders)\n","\n","        self.rgcn_module_for_h0 = RGCNModule(static_node_in_dim, rgcn_hidden_dim, rgcn_output_dim, num_relations, dropout_rate_rgcn)\n","        self.rgcn_module_for_sequence = RGCNModule(static_node_in_dim, rgcn_hidden_dim, rgcn_output_dim, num_relations, dropout_rate_rgcn)\n","\n","        concatenated_feature_dim = rgcn_output_dim + global_env_emb_dim + time_emb_dim\n","        actual_fusion_mlp_output_dim = fusion_mlp_output_dim if fusion_mlp_output_dim is not None else concatenated_feature_dim\n","        self.fusion_mlp_input_dim = concatenated_feature_dim # Store for component profiling\n","\n","        self.fusion_mlp = MLPEncoder(\n","            in_dim=concatenated_feature_dim,\n","            out_dim=actual_fusion_mlp_output_dim,\n","            hid_dim=fusion_mlp_hidden_dim,\n","            dropout_rate=dropout_rate_fusion_mlp\n","        )\n","\n","        gru_input_size_actual = actual_fusion_mlp_output_dim\n","        self.gru_input_dim = gru_input_size_actual # Store for component profiling\n","\n","        self.gru = nn.GRU(\n","            input_size=gru_input_size_actual,\n","            hidden_size=gru_hidden_dim,\n","            num_layers=num_gru_layers,\n","            batch_first=True,\n","            dropout=dropout_rate_gru if num_gru_layers > 1 else 0.0\n","        )\n","\n","        self.hourly_prediction_heads = nn.ModuleList()\n","        for _ in range(T_pred_horizon):\n","            self.hourly_prediction_heads.append(\n","                nn.Sequential(\n","                    nn.Linear(gru_hidden_dim, mlp_prediction_hidden_dim),\n","                    nn.ReLU(),\n","                    nn.Dropout(dropout_rate_pred_head),\n","                    nn.Linear(mlp_prediction_hidden_dim, 1)\n","                )\n","            )\n","\n","        self.register_buffer('node_feat_mean', torch.zeros(static_node_in_dim))\n","        self.register_buffer('node_feat_std', torch.ones(static_node_in_dim))\n","\n","\n","    def forward(self, list_of_batched_timesteps: list, timeline_time_features: torch.Tensor, device: torch.device):\n","        pyg_batch_7am = list_of_batched_timesteps[0].to(device)\n","        normalized_x_7am = (pyg_batch_7am.x - self.node_feat_mean) / (self.node_feat_std + 1e-8)\n","\n","        rgcn_output_7am = self.rgcn_module_for_h0(\n","            normalized_x_7am,\n","            pyg_batch_7am.edge_index,\n","            pyg_batch_7am.edge_attr\n","        )\n","        h0_features_for_gru_nodes = self.h0_from_rgcn_encoder(rgcn_output_7am)\n","        h0_for_gru = h0_features_for_gru_nodes.unsqueeze(0)\n","        if self.gru.num_layers > 1:\n","            h0_for_gru = h0_for_gru.repeat(self.gru.num_layers, 1, 1)\n","\n","        all_gru_input_features_over_time = []\n","        for t_pred_idx in range(self.T_pred_horizon):\n","            pyg_batch_this_timestep = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","            normalized_x = (pyg_batch_this_timestep.x - self.node_feat_mean) / (self.node_feat_std + 1e-8)\n","\n","            rgcn_output_nodes_t = self.rgcn_module_for_sequence(\n","                normalized_x,\n","                pyg_batch_this_timestep.edge_index,\n","                pyg_batch_this_timestep.edge_attr\n","            )\n","\n","            global_env_feat_t_unencoded = pyg_batch_this_timestep.graph_global_env_features\n","            expected_num_graphs_in_batch_t = pyg_batch_this_timestep.num_graphs\n","            expected_global_features_dim = self.global_env_encoder.mlp[0].in_features\n","            if not (global_env_feat_t_unencoded.shape == (expected_num_graphs_in_batch_t, expected_global_features_dim)):\n","                if global_env_feat_t_unencoded.ndim == 1 and \\\n","                   global_env_feat_t_unencoded.shape[0] == expected_num_graphs_in_batch_t * expected_global_features_dim:\n","                    global_env_feat_t_unencoded = global_env_feat_t_unencoded.view(expected_num_graphs_in_batch_t, expected_global_features_dim)\n","                elif global_env_feat_t_unencoded.numel() == expected_num_graphs_in_batch_t * expected_global_features_dim:\n","                    global_env_feat_t_unencoded = global_env_feat_t_unencoded.view(expected_num_graphs_in_batch_t, expected_global_features_dim)\n","                else:\n","                    print(f\"Warning: GRU Input Time {t_pred_idx}: Correcting global_env_feat shape from {global_env_feat_t_unencoded.shape} to ({expected_num_graphs_in_batch_t}, {expected_global_features_dim}) with zeros due to mismatch.\")\n","                    global_env_feat_t_unencoded = torch.zeros(expected_num_graphs_in_batch_t, expected_global_features_dim, device=device)\n","            global_env_emb_t = self.global_env_encoder(global_env_feat_t_unencoded)\n","            global_env_emb_t_expanded = global_env_emb_t[pyg_batch_this_timestep.batch]\n","\n","            current_raw_time_feat_for_timestep_t = timeline_time_features[t_pred_idx, :].to(device)\n","            current_emb_time_feat_for_timestep_t = self.time_encoder(current_raw_time_feat_for_timestep_t)\n","            num_nodes_in_pyg_batch = pyg_batch_this_timestep.num_nodes\n","            time_emb_t_expanded_to_nodes = current_emb_time_feat_for_timestep_t.unsqueeze(0).expand(num_nodes_in_pyg_batch, -1)\n","\n","            concatenated_features_for_timestep_t = torch.cat([rgcn_output_nodes_t, global_env_emb_t_expanded, time_emb_t_expanded_to_nodes], dim=-1)\n","            fused_features_for_timestep_t = self.fusion_mlp(concatenated_features_for_timestep_t)\n","            all_gru_input_features_over_time.append(fused_features_for_timestep_t)\n","\n","        stacked_gru_input_features = torch.stack(all_gru_input_features_over_time, dim=1)\n","\n","        if h0_for_gru.shape[1] != stacked_gru_input_features.shape[0]:\n","            print(f\"CRITICAL WARNING: Node count mismatch for GRU h0 ({h0_for_gru.shape[1]}) and GRU input sequence ({stacked_gru_input_features.shape[0]}).\")\n","            if h0_for_gru.shape[1] > stacked_gru_input_features.shape[0]:\n","                h0_for_gru = h0_for_gru[:, :stacked_gru_input_features.shape[0], :]\n","\n","        gru_out, _ = self.gru(stacked_gru_input_features, h0_for_gru)\n","\n","        all_hourly_final_predictions_scaled = []\n","        for t in range(self.T_pred_horizon):\n","            gru_out_t = gru_out[:, t, :]\n","            prediction_t_scaled = self.hourly_prediction_heads[t](gru_out_t)\n","            all_hourly_final_predictions_scaled.append(prediction_t_scaled.squeeze(-1))\n","\n","        predictions_scaled = torch.stack(all_hourly_final_predictions_scaled, dim=1)\n","        return predictions_scaled\n","\n","# ===========================================================\n","# 3. 评估指标函数 (在原始尺度上计算指标)\n","# ===========================================================\n","def mse_loss_masked(predictions_scaled, targets_scaled, mask):\n","    expanded_mask = mask.unsqueeze(1).expand_as(targets_scaled)\n","    valid_targets_mask = ~torch.isnan(targets_scaled)\n","    final_mask = expanded_mask & valid_targets_mask\n","    if final_mask.sum() == 0:\n","        return torch.tensor(0.0, device=predictions_scaled.device, requires_grad=True)\n","    loss = F.mse_loss(predictions_scaled[final_mask], targets_scaled[final_mask])\n","    return loss\n","\n","def calculate_hourly_metrics(predictions_scaled, targets_scaled, node_masks, target_mean, target_std):\n","    target_mean_cpu = target_mean.cpu()\n","    target_std_cpu = target_std.cpu()\n","    preds_unscaled = predictions_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","    targets_unscaled = targets_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","\n","    num_nodes_total, T_horizon = preds_unscaled.shape\n","    hourly_metrics_dict = {}\n","\n","    preds_np = preds_unscaled.numpy()\n","    targets_np = targets_unscaled.numpy()\n","    mask_np = node_masks.cpu().numpy()\n","\n","    for t in range(T_horizon):\n","        preds_t_all_nodes = preds_np[:, t]\n","        targets_t_all_nodes = targets_np[:, t]\n","        preds_t_on_loss_nodes = preds_t_all_nodes[mask_np]\n","        targets_t_on_loss_nodes = targets_t_all_nodes[mask_np]\n","        valid_target_data_mask_t = ~np.isnan(targets_t_on_loss_nodes)\n","        preds_t_final_valid = preds_t_on_loss_nodes[valid_target_data_mask_t]\n","        targets_t_final_valid = targets_t_on_loss_nodes[valid_target_data_mask_t]\n","\n","        if preds_t_final_valid.shape[0] < 2:\n","            hourly_metrics_dict[t] = {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count': 0}\n","            continue\n","        mse = np.mean((preds_t_final_valid - targets_t_final_valid)**2)\n","        mae = np.mean(np.abs(preds_t_final_valid - targets_t_final_valid))\n","        rmse = np.sqrt(mse)\n","        try:\n","            r2 = r2_score(targets_t_final_valid, preds_t_final_valid)\n","        except ValueError:\n","            r2 = np.nan\n","        hourly_metrics_dict[t] = {'mse': mse, 'mae': mae, 'rmse': rmse, 'r2': r2, 'count': preds_t_final_valid.shape[0]}\n","    return hourly_metrics_dict\n","\n","# ===========================================================\n","# 4. 训练与评估循环 (适配y归一化)\n","# ===========================================================\n","def train_epoch(model, loader, optimizer, device, timeline_time_features,\n","                node_feat_mean, node_feat_std, target_mean, target_std):\n","    model.train()\n","    total_loss_scaled = 0\n","    num_sequences_processed = 0\n","    model.node_feat_mean = node_feat_mean.to(device)\n","    model.node_feat_std = node_feat_std.to(device)\n","\n","    target_mean_dev = target_mean.to(device)\n","    target_std_dev = target_std.to(device)\n","\n","    epoch_start_time = time.time()\n","    for list_of_batched_timesteps in loader:\n","        optimizer.zero_grad()\n","        predictions_batch_scaled = model(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","\n","        targets_list_for_loss_scaled = []\n","        first_predicted_timestep_batch = list_of_batched_timesteps[1].to(device)\n","        mask_for_loss = ~first_predicted_timestep_batch.building_mask\n","\n","        for t_pred_idx in range(model.T_pred_horizon):\n","            current_target_timestep_batch = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","            targets_t_nodes_original = current_target_timestep_batch.y.squeeze()\n","            targets_t_nodes_scaled = (targets_t_nodes_original - target_mean_dev) / (target_std_dev + 1e-8)\n","            targets_list_for_loss_scaled.append(targets_t_nodes_scaled)\n","\n","        targets_batch_scaled = torch.stack(targets_list_for_loss_scaled, dim=1)\n","        loss = mse_loss_masked(predictions_batch_scaled, targets_batch_scaled, mask_for_loss)\n","        num_sequences_in_this_super_batch = list_of_batched_timesteps[0].num_graphs\n","\n","        if not torch.isnan(loss) and not torch.isinf(loss) and loss.item() > 0 :\n","            loss.backward()\n","            optimizer.step()\n","            total_loss_scaled += loss.item() * num_sequences_in_this_super_batch\n","        num_sequences_processed += num_sequences_in_this_super_batch\n","\n","    epoch_duration = time.time() - epoch_start_time\n","    avg_loss = total_loss_scaled / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","    return avg_loss, epoch_duration\n","\n","def evaluate_epoch(model, loader, device, timeline_time_features,\n","                   node_feat_mean, node_feat_std, target_mean, target_std, epoch_type=\"Eval\"):\n","    model.eval()\n","    all_batch_predictions_scaled = []\n","    all_batch_targets_scaled = []\n","    all_batch_masks_for_metrics = []\n","    total_loss_scaled = 0\n","    num_sequences_processed = 0\n","    model.node_feat_mean = node_feat_mean.to(device)\n","    model.node_feat_std = node_feat_std.to(device)\n","\n","    target_mean_dev = target_mean.to(device)\n","    target_std_dev = target_std.to(device)\n","\n","    eval_start_time = time.time()\n","    with torch.no_grad():\n","        for list_of_batched_timesteps in loader:\n","            predictions_batch_scaled = model(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","            targets_list_original = []\n","            first_predicted_timestep_batch = list_of_batched_timesteps[1].to(device)\n","            mask_for_metrics = ~first_predicted_timestep_batch.building_mask\n","\n","            for t_pred_idx in range(model.T_pred_horizon):\n","                current_target_timestep_batch = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","                targets_t_nodes_original = current_target_timestep_batch.y.squeeze()\n","                targets_list_original.append(targets_t_nodes_original)\n","\n","            targets_batch_original = torch.stack(targets_list_original, dim=1)\n","            targets_batch_scaled_for_loss = (targets_batch_original - target_mean_dev) / (target_std_dev + 1e-8)\n","            loss = mse_loss_masked(predictions_batch_scaled, targets_batch_scaled_for_loss, mask_for_metrics)\n","            num_sequences_in_this_super_batch = list_of_batched_timesteps[0].num_graphs\n","\n","            if not torch.isnan(loss) and not torch.isinf(loss):\n","                total_loss_scaled += loss.item() * num_sequences_in_this_super_batch\n","            num_sequences_processed += num_sequences_in_this_super_batch\n","\n","            all_batch_predictions_scaled.append(predictions_batch_scaled.cpu())\n","            all_batch_targets_scaled.append(targets_batch_scaled_for_loss.cpu())\n","            all_batch_masks_for_metrics.append(mask_for_metrics.cpu())\n","\n","    eval_duration = time.time() - eval_start_time\n","    avg_loss_scaled = total_loss_scaled / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","\n","    if not all_batch_predictions_scaled:\n","        empty_metrics = {t: {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0} for t in range(model.T_pred_horizon)}\n","        return avg_loss_scaled, empty_metrics, eval_duration\n","\n","    final_predictions_scaled = torch.cat(all_batch_predictions_scaled, dim=0)\n","    final_targets_scaled = torch.cat(all_batch_targets_scaled, dim=0)\n","    final_masks_for_metrics = torch.cat(all_batch_masks_for_metrics, dim=0)\n","\n","    hourly_metrics_original_scale = calculate_hourly_metrics(final_predictions_scaled, final_targets_scaled,\n","                                                             final_masks_for_metrics, target_mean.cpu(), target_std.cpu())\n","    return avg_loss_scaled, hourly_metrics_original_scale, eval_duration\n","\n","# ===========================================================\n","# 5. 主训练流程 (集成y归一化, 使用RGCN, 添加报告)\n","# ===========================================================\n","\n","def calculate_aggregated_metrics_report(hourly_metrics_dict, T_pred_horizon):\n","    metrics_to_aggregate = ['r2', 'mse', 'mae', 'rmse']\n","    aggregated_report = {}\n","    for metric_name in metrics_to_aggregate:\n","        values = [hourly_metrics_dict[t][metric_name] for t in range(T_pred_horizon) if t in hourly_metrics_dict and not np.isnan(hourly_metrics_dict[t][metric_name])]\n","        if values:\n","            aggregated_report[f'avg_{metric_name}'] = np.mean(values)\n","            aggregated_report[f'std_{metric_name}'] = np.std(values)\n","        else:\n","            aggregated_report[f'avg_{metric_name}'] = np.nan\n","            aggregated_report[f'std_{metric_name}'] = np.nan\n","    return aggregated_report\n","\n","\n","def main_training_rgcn_gru_hourly_heads(\n","    all_sequences_data: list,\n","    config: dict,\n","    time_features_for_dataset: torch.Tensor\n","):\n","    train_start_time = time.time()\n","    report_data = {'config': config}\n","\n","    seed = config.get('seed', 42)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"使用设备: {device}\")\n","    report_data['device'] = str(device)\n","\n","    T_PRED_HORIZON = config.get('T_pred_horizon', 12)\n","    expected_input_len = T_PRED_HORIZON + 1\n","    valid_sequences_data = [seq for seq in all_sequences_data if isinstance(seq, list) and len(seq) == expected_input_len]\n","    if len(valid_sequences_data) != len(all_sequences_data):\n","        print(f\"警告: 从 {len(all_sequences_data)} 个序列中筛选出 {len(valid_sequences_data)} 个长度为 {expected_input_len} 的有效序列。\")\n","    if not valid_sequences_data:\n","        raise ValueError(f\"没有找到长度为 {expected_input_len} 的有效序列数据。\")\n","    all_sequences_data = valid_sequences_data\n","\n","    # Dataset split (same as before)\n","    num_total_sequences = len(all_sequences_data)\n","    indices = np.random.permutation(num_total_sequences)\n","    train_split_ratio = config.get('train_split_ratio', 0.7)\n","    val_split_ratio = config.get('val_split_ratio', 0.2)\n","    train_size = int(train_split_ratio * num_total_sequences)\n","    val_size = int(val_split_ratio * num_total_sequences)\n","    train_indices = indices[:train_size]\n","    val_indices = indices[train_size : train_size + val_size]\n","    test_indices = indices[train_size + val_size :]\n","    train_dataset = [all_sequences_data[i] for i in train_indices]\n","    val_dataset   = [all_sequences_data[i] for i in val_indices]\n","    test_dataset  = [all_sequences_data[i] for i in test_indices]\n","    report_data['dataset_split'] = {'total_sequences': num_total_sequences, 'train_size': len(train_dataset), 'val_size': len(val_dataset), 'test_size': len(test_dataset)}\n","\n","    # Scaler calculation (same as before)\n","    all_train_node_features_list = []\n","    all_train_target_values_list_for_scaling = []\n","    for seq in train_dataset:\n","        for i_step, graph_data in enumerate(seq):\n","            if hasattr(graph_data, 'x') and graph_data.x is not None: all_train_node_features_list.append(graph_data.x)\n","            if i_step > 0 and hasattr(graph_data, 'y') and graph_data.y is not None:\n","                y_original = graph_data.y.squeeze(); current_mask_for_loss = ~graph_data.building_mask\n","                valid_target_indices = current_mask_for_loss & ~torch.isnan(y_original)\n","                if valid_target_indices.sum() > 0: all_train_target_values_list_for_scaling.append(y_original[valid_target_indices])\n","    if not all_train_node_features_list: raise ValueError(\"训练数据中未找到节点特征 'x'，无法计算scaler！\")\n","    all_train_node_features_tensor = torch.cat(all_train_node_features_list, dim=0)\n","    node_feat_mean = torch.mean(all_train_node_features_tensor, dim=0); node_feat_std = torch.std(all_train_node_features_tensor, dim=0)\n","    node_feat_std[node_feat_std < 1e-8] = 1.0\n","    scaler_path_x = Path(config['results_dir']) / \"node_feature_scaler_rgcn_gru.pth\"\n","    torch.save({'mean': node_feat_mean, 'std': node_feat_std}, scaler_path_x); print(f\"节点特征x scaler已保存到: {scaler_path_x}\")\n","    if not all_train_target_values_list_for_scaling:\n","        target_mean = torch.tensor(0.0); target_std = torch.tensor(1.0)\n","    else:\n","        all_train_target_values_tensor = torch.cat(all_train_target_values_list_for_scaling, dim=0)\n","        target_mean = torch.mean(all_train_target_values_tensor.float()); target_std = torch.std(all_train_target_values_tensor.float())\n","        if target_std < 1e-8: target_std = torch.tensor(1.0)\n","    target_scaler_path = Path(config['results_dir']) / \"target_scaler_rgcn_gru.pth\"\n","    torch.save({'mean': target_mean, 'std': target_std}, target_scaler_path); print(f\"目标值y scaler已保存到: {target_scaler_path}\")\n","\n","    # DataLoaders (same as before)\n","    batch_size = config.get('batch_size', 8); num_workers = config.get('num_workers', 0)\n","    pin_memory_flag = config.get('pin_memory', False) and device.type == 'cuda'\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=num_workers, pin_memory=pin_memory_flag)\n","    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n","    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n","\n","    # Model Initialization (same as before)\n","    sample_graph_7am_for_dims = all_sequences_data[0][0]\n","    static_node_in_dim = sample_graph_7am_for_dims.x.shape[1]\n","    global_env_in_dim = sample_graph_7am_for_dims.graph_global_env_features.shape[0] if sample_graph_7am_for_dims.graph_global_env_features.ndim == 1 else sample_graph_7am_for_dims.graph_global_env_features.shape[1]\n","    time_in_dim = time_features_for_dataset.shape[1]\n","    num_relations = config.get('num_relations', 5)\n","    model = RGCNGRUModelWithHourlyHeads( static_node_in_dim=static_node_in_dim, global_env_in_dim=global_env_in_dim, time_in_dim=time_in_dim,\n","                                        global_env_emb_dim=config.get('global_env_emb_dim', 16), time_emb_dim=config.get('time_emb_dim', 8),\n","                                         rgcn_hidden_dim=config.get('gcn_hidden_dim', 128), rgcn_output_dim=config.get('gcn_output_dim', 128),\n","                                         num_relations=num_relations, gru_hidden_dim=config.get('gru_hidden_dim', 128),\n","                                         fusion_mlp_output_dim=config.get('fusion_mlp_output_dim', 128), fusion_mlp_hidden_dim=config.get('fusion_mlp_hidden_dim', 64), dropout_rate_fusion_mlp=config.get('dropout_rate_fusion_mlp', 0.2), num_gru_layers=config.get('num_gru_layers', 1), T_pred_horizon=T_PRED_HORIZON, dropout_rate_encoders=config.get('dropout_rate_encoders', 0.1), dropout_rate_rgcn=config.get('dropout_rate_gcn', 0.3), dropout_rate_gru=config.get('dropout_rate_gru', 0.2), mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim', 64), dropout_rate_pred_head=config.get('dropout_rate_pred_head', 0.2) ).to(device)\n","    model.node_feat_mean = node_feat_mean.to(device); model.node_feat_std = node_feat_std.to(device)\n","    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"模型总参数量: {total_params:,}\"); report_data['model_total_parameters'] = total_params\n","\n","    # ===== Component FLOPS Calculation =====\n","    print(\"\\nCalculating MACs for model components (approximate FLOPS):\")\n","    report_data['component_gmacs'] = {}\n","    dummy_nodes_component = 2500  # Representative number of nodes for a single graph component\n","    dummy_edges_component = 60000 # Representative number of edges\n","    dummy_batch_global_comp = 1 # For components that process per-graph or per-batch features\n","\n","    model.eval() # Ensure model is in eval mode for profiling if it has dropout/batchnorm\n","\n","    # 1. RGCNModule\n","    try:\n","        rgcn_module_to_profile = model.rgcn_module_for_h0 # Or _for_sequence\n","        dummy_x_rgcn = torch.randn(dummy_nodes_component, rgcn_module_to_profile.rgcn_input_dim, device=device)\n","        dummy_ei_rgcn = torch.randint(0, dummy_nodes_component, (2, dummy_edges_component), device=device)\n","        dummy_ea_rgcn = torch.randn(dummy_edges_component, 5, device=device) # Assuming 5 edge features\n","        dummy_ea_rgcn[:, 4] = torch.randint(0, model.num_relations, (dummy_edges_component,), device=device).float()\n","        macs_rgcn = torchprofile.profile_macs(rgcn_module_to_profile, args=(dummy_x_rgcn, dummy_ei_rgcn, dummy_ea_rgcn))\n","        report_data['component_gmacs']['rgcn_module'] = macs_rgcn / 1e9\n","        print(f\"  RGCNModule GMACs: {macs_rgcn / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling RGCNModule: {e}\"); report_data['component_gmacs']['rgcn_module'] = \"Error\"\n","\n","    # 2. GRU Layer\n","    print(f\"  Manually Calculating MACs for GRU Layer:\")\n","    try:\n","        gru_layer = model.gru\n","        N_nodes = dummy_nodes_component # Number of nodes (batch items for GRU)\n","        L_seq = T_PRED_HORIZON        # Sequence length\n","        H_in = gru_layer.input_size\n","        H_hidden = gru_layer.hidden_size\n","        num_layers = gru_layer.num_layers\n","\n","        macs_gru_manual = 0\n","        if num_layers == 1:\n","            # For a single layer GRU: MACs ≈ N * L * 3 * (H_in * H_hidden + H_hidden^2)\n","            macs_gru_manual = N_nodes * L_seq * 3 * (H_in * H_hidden + H_hidden * H_hidden)\n","        else:\n","            # For multi-layer GRU:\n","            # First layer: N * L * 3 * (H_in * H_hidden + H_hidden^2)\n","            # Subsequent (num_layers - 1) layers: N * L * 3 * (H_hidden * H_hidden + H_hidden^2)\n","            #   (because input to subsequent layers is H_hidden)\n","            macs_gru_manual = N_nodes * L_seq * 3 * (H_in * H_hidden + H_hidden * H_hidden)\n","            if num_layers > 1:\n","                macs_gru_manual += N_nodes * L_seq * (num_layers - 1) * 3 * (H_hidden * H_hidden + H_hidden * H_hidden)\n","\n","        gmacs_gru_manual = macs_gru_manual / 1e9\n","        report_data['component_gmacs']['gru_layer'] = gmacs_gru_manual\n","        report_data['component_gmacs']['gru_layer_profiling_notes'] = \"Manually calculated based on formula.\"\n","        print(f\"  GRU Parameters: input_size={H_in}, hidden_size={H_hidden}, num_layers={num_layers}\")\n","        print(f\"  Used for calculation: N_nodes={N_nodes}, L_seq={L_seq}\")\n","        print(f\"  GRU Layer GMACs (Manual): {gmacs_gru_manual:.4f} (for sequence length {L_seq})\")\n","\n","    except Exception as e:\n","        print(f\"  Error manually calculating GRU Layer MACs: {e}\")\n","        report_data['component_gmacs']['gru_layer'] = \"Error\"\n","        report_data['component_gmacs']['gru_layer_profiling_notes'] = f\"Error during manual calculation: {str(e)}\"\n","\n","    # 3. Fusion MLP (MLPEncoder)\n","    try:\n","        fusion_mlp_to_profile = model.fusion_mlp\n","        dummy_input_fusion_mlp = torch.randn(dummy_nodes_component, model.fusion_mlp_input_dim, device=device)\n","        macs_fusion_mlp = torchprofile.profile_macs(fusion_mlp_to_profile, args=(dummy_input_fusion_mlp,))\n","        report_data['component_gmacs']['fusion_mlp'] = macs_fusion_mlp / 1e9\n","        print(f\"  Fusion MLP GMACs: {macs_fusion_mlp / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Fusion MLP: {e}\"); report_data['component_gmacs']['fusion_mlp'] = \"Error\"\n","\n","    # 4. Prediction Head (one MLP from ModuleList)\n","    try:\n","        pred_head_to_profile = model.hourly_prediction_heads[0]\n","        dummy_input_pred_head = torch.randn(dummy_nodes_component, model.gru_hidden_dim, device=device)\n","        macs_pred_head = torchprofile.profile_macs(pred_head_to_profile, args=(dummy_input_pred_head,))\n","        report_data['component_gmacs']['prediction_head_mlp'] = macs_pred_head / 1e9\n","        print(f\"  Prediction Head MLP (single hour) GMACs: {macs_pred_head / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Prediction Head: {e}\"); report_data['component_gmacs']['prediction_head_mlp'] = \"Error\"\n","\n","    # 5. Global Environment Encoder (MLPEncoder)\n","    try:\n","        encoder_to_profile = model.global_env_encoder\n","        dummy_input_encoder = torch.randn(dummy_batch_global_comp, model.global_env_in_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['global_env_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  Global Env Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Global Env Encoder: {e}\"); report_data['component_gmacs']['global_env_encoder_mlp'] = \"Error\"\n","\n","    # 6. Time Encoder (MLPEncoder)\n","    try:\n","        encoder_to_profile = model.time_encoder\n","        dummy_input_encoder = torch.randn(dummy_batch_global_comp, model.time_in_dim, device=device) # Input usually (1, time_in_dim)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['time_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  Time Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Time Encoder: {e}\"); report_data['component_gmacs']['time_encoder_mlp'] = \"Error\"\n","\n","    # 7. H0 from RGCN Encoder (MLPEncoder)\n","    try:\n","        encoder_to_profile = model.h0_from_rgcn_encoder\n","        dummy_input_encoder = torch.randn(dummy_nodes_component, model.rgcn_output_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['h0_from_rgcn_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  H0 from RGCN Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling H0 from RGCN Encoder: {e}\"); report_data['component_gmacs']['h0_from_rgcn_encoder_mlp'] = \"Error\"\n","\n","    model.train() # Return to train mode if changed\n","    # ===== End Component FLOPS Calculation =====\n","\n","    # Optimizer and Scheduler (same as before)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config.get('lr', 0.001), weight_decay=config.get('weight_decay', 1e-5))\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=config.get('scheduler_patience', 20), verbose=True)\n","\n","    # Training Loop (same as before, with metric storage)\n","    best_val_loss_scaled = float('inf'); best_val_hourly_metrics_original = None; best_epoch = 0\n","    patience_counter = 0; max_epochs = config.get('max_epochs', 300); early_stopping_patience = config.get('early_stopping_patience', 45)\n","    model_save_path = Path(config['results_dir']) / f\"best_rgcngru_hourly_heads_model_seed{seed}.pth\"\n","    timeline_time_features_on_device = time_features_for_dataset.to(device)\n","    target_mean_cpu = target_mean.cpu(); target_std_cpu = target_std.cpu()\n","    target_mean_on_device = target_mean.to(device); target_std_on_device = target_std.to(device)\n","    epoch_times = []\n","\n","    for epoch in range(1, max_epochs + 1):\n","        train_loss_scaled, epoch_duration = train_epoch(model, train_loader, optimizer, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_on_device, target_std_on_device)\n","        epoch_times.append(epoch_duration)\n","        val_loss_scaled, val_hourly_metrics_original, _ = evaluate_epoch(model, val_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Validation\")\n","        scheduler.step(val_loss_scaled)\n","        print(f\"Epoch {epoch:03d} | Train Scaled MSE: {train_loss_scaled:.4f} | Val Scaled MSE: {val_loss_scaled:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f} | Epoch Time: {epoch_duration:.2f}s\")\n","        _print_hourly_metrics_summary(\"Val\", val_hourly_metrics_original, T_PRED_HORIZON, indent=\"                     \")\n","        if val_loss_scaled < best_val_loss_scaled:\n","            best_val_loss_scaled = val_loss_scaled; best_val_hourly_metrics_original = val_hourly_metrics_original; best_epoch = epoch\n","            patience_counter = 0; torch.save(model.state_dict(), model_save_path)\n","            print(f\"                     ---> Best model saved (Epoch: {epoch}, Val Scaled MSE: {best_val_loss_scaled:.4f})\")\n","        else:\n","            patience_counter += 1\n","        if patience_counter >= early_stopping_patience:\n","            print(f\"Early stopping at epoch {epoch} due to no improvement.\"); break\n","\n","    total_training_duration = time.time() - train_start_time\n","    report_data['total_training_time_seconds'] = total_training_duration\n","    report_data['average_epoch_time_seconds'] = np.mean(epoch_times) if epoch_times else np.nan\n","    report_data['num_epochs_trained'] = epoch; report_data['best_validation_epoch'] = best_epoch\n","    report_data['best_validation_scaled_mse'] = best_val_loss_scaled\n","\n","    # Evaluation with Best Model (same as before)\n","    model_for_eval = RGCNGRUModelWithHourlyHeads( static_node_in_dim=static_node_in_dim, global_env_in_dim=global_env_in_dim, time_in_dim=time_in_dim, global_env_emb_dim=config.get('global_env_emb_dim', 16), time_emb_dim=config.get('time_emb_dim', 8), rgcn_hidden_dim=config.get('gcn_hidden_dim', 128), rgcn_output_dim=config.get('gcn_output_dim', 128), num_relations=num_relations, gru_hidden_dim=config.get('gru_hidden_dim', 128), fusion_mlp_output_dim=config.get('fusion_mlp_output_dim', 128), fusion_mlp_hidden_dim=config.get('fusion_mlp_hidden_dim', 64), dropout_rate_fusion_mlp=config.get('dropout_rate_fusion_mlp', 0.2), num_gru_layers=config.get('num_gru_layers', 1), T_pred_horizon=T_PRED_HORIZON, dropout_rate_encoders=config.get('dropout_rate_encoders', 0.1), dropout_rate_rgcn=config.get('dropout_rate_gcn', 0.3), dropout_rate_gru=config.get('dropout_rate_gru', 0.2), mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim', 64), dropout_rate_pred_head=config.get('dropout_rate_pred_head', 0.2) ).to(device)\n","    try: model_for_eval.load_state_dict(torch.load(model_save_path, map_location=device))\n","    except Exception as e: print(f\"无法加载最佳模型 ({e})，将使用训练循环结束时的模型。\"); model_for_eval = model\n","\n","    print(\"\\n评估最佳模型在训练集上...\"); best_model_train_loss_scaled, best_model_train_hourly_metrics, train_eval_duration = evaluate_epoch(model_for_eval, train_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Best Model on Train\")\n","    report_data['best_model_train_set_metrics_hourly'] = best_model_train_hourly_metrics; report_data['best_model_train_set_metrics_aggregated'] = calculate_aggregated_metrics_report(best_model_train_hourly_metrics, T_PRED_HORIZON)\n","    report_data['best_model_train_set_eval_time_seconds'] = train_eval_duration; _print_hourly_metrics_summary(\"最佳模型训练集\", best_model_train_hourly_metrics, T_PRED_HORIZON)\n","\n","    report_data['best_model_validation_set_metrics_hourly'] = best_val_hourly_metrics_original\n","    if best_val_hourly_metrics_original: report_data['best_model_validation_set_metrics_aggregated'] = calculate_aggregated_metrics_report(best_val_hourly_metrics_original, T_PRED_HORIZON)\n","    else: # Fallback if training didn't run/save\n","        _, reeval_val_metrics, val_eval_duration = evaluate_epoch(model_for_eval, val_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Best Model on Val (Re-eval)\")\n","        report_data['best_model_validation_set_metrics_hourly'] = reeval_val_metrics\n","        report_data['best_model_validation_set_metrics_aggregated'] = calculate_aggregated_metrics_report(reeval_val_metrics, T_PRED_HORIZON)\n","        report_data['best_model_validation_set_eval_time_seconds'] = val_eval_duration\n","    _print_hourly_metrics_summary(\"最佳模型验证集\", report_data['best_model_validation_set_metrics_hourly'], T_PRED_HORIZON)\n","\n","\n","    print(\"\\n评估最佳模型在测试集上...\"); test_loss_scaled, test_hourly_metrics_original, test_inference_duration = evaluate_epoch(model_for_eval, test_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Test\")\n","    report_data['test_set_inference_time_seconds'] = test_inference_duration; report_data['best_model_test_set_metrics_hourly'] = test_hourly_metrics_original; report_data['best_model_test_set_metrics_aggregated'] = calculate_aggregated_metrics_report(test_hourly_metrics_original, T_PRED_HORIZON)\n","    print(\"\\n\" + \"=\"*20 + \" 最终测试集评估结果 (RGCN-GRU) \" + \"=\"*20); print(f\"平均测试 Scaled MSE: {test_loss_scaled:.4f}\")\n","    _print_hourly_metrics_summary(\"测试集\", test_hourly_metrics_original, T_PRED_HORIZON)\n","\n","    agg_test = report_data['best_model_test_set_metrics_aggregated']\n","    print(f\"平均测试 MSE (Orig) : {agg_test.get('avg_mse', np.nan):.4f} (Std: {agg_test.get('std_mse', np.nan):.4f})\")\n","    print(f\"平均测试 R2 (Orig)  : {agg_test.get('avg_r2', np.nan):.4f} (Std: {agg_test.get('std_r2', np.nan):.4f})\")\n","    print(f\"平均测试 MAE (Orig) : {agg_test.get('avg_mae', np.nan):.4f} (Std: {agg_test.get('std_mae', np.nan):.4f})\")\n","    print(f\"平均测试 RMSE (Orig): {agg_test.get('avg_rmse', np.nan):.4f} (Std: {agg_test.get('std_rmse', np.nan):.4f})\")\n","    print(\"=\"*70)\n","\n","    # Save Report (same as before)\n","    report_file_path = Path(config['results_dir']) / f\"training_report_seed{seed}.json\"\n","    try:\n","        class NpEncoder(json.JSONEncoder):\n","            def default(self, obj):\n","                if isinstance(obj, np.integer): return int(obj)\n","                if isinstance(obj, np.floating): return float(obj)\n","                if isinstance(obj, np.ndarray): return obj.tolist()\n","                if isinstance(obj, torch.Tensor): return obj.tolist()\n","                if isinstance(obj, Path): return str(obj)\n","                return super(NpEncoder, self).default(obj)\n","        with open(report_file_path, 'w') as f: json.dump(report_data, f, indent=4, cls=NpEncoder)\n","        print(f\"训练报告已保存到: {report_file_path}\")\n","    except Exception as e: print(f\"保存训练报告失败: {e}\")\n","\n","    return model_for_eval, node_feat_mean, node_feat_std, target_mean, target_std\n","\n","\n","def _print_hourly_metrics_summary(set_name, hourly_metrics, T_pred_horizon, indent=\"  \"):\n","    if hourly_metrics is None:\n","        print(f\"{indent}{set_name} metrics not available.\")\n","        return\n","    print(f\"\\n{indent}每小时 {set_name} 指标 (Original Scale):\")\n","    header_printed = False\n","    for hour_idx in range(T_pred_horizon):\n","        metrics = hourly_metrics.get(hour_idx, {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0})\n","        if not header_printed:\n","            print(f\"{indent}  Hour | {'R2':>13s} | {'MSE':>14s} | {'MAE':>14s} | {'RMSE':>15s} | {'Count':>7s}\")\n","            header_printed = True\n","        print(f\"{indent}  {hour_idx:02d}   | {metrics.get('r2', np.nan):13.4f} | {metrics.get('mse', np.nan):14.4f} | {metrics.get('mae', np.nan):14.4f} | {metrics.get('rmse', np.nan):15.4f} | {metrics.get('count', 0):7d}\")\n","    # Print aggregated for this set as well\n","    aggregated = calculate_aggregated_metrics_report(hourly_metrics, T_pred_horizon)\n","    print(f\"{indent}  Aggregated Avg R2   : {aggregated.get('avg_r2', np.nan):.4f} (Std: {aggregated.get('std_r2', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MSE  : {aggregated.get('avg_mse', np.nan):.4f} (Std: {aggregated.get('std_mse', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MAE  : {aggregated.get('avg_mae', np.nan):.4f} (Std: {aggregated.get('std_mae', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg RMSE : {aggregated.get('avg_rmse', np.nan):.4f} (Std: {aggregated.get('std_rmse', np.nan):.4f})\")\n","\n","\n","# ===========================================================\n","# 6. 主执行块\n","# ===========================================================\n","if __name__ == \"__main__\":\n","    gc.collect()\n","    if torch.cuda.is_available(): torch.cuda.empty_cache()\n","\n","    DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","    if not DRIVE_BASE_PATH.exists(): DRIVE_BASE_PATH.mkdir(parents=True, exist_ok=True)\n","\n","    DATA_SUBDIR = Path(\"Result/Sequential_12Hour_Data\")\n","    DATA_FILENAME = \"graph_seq_20230503_SeqH7to19_NpyH8fill0.0.pkl\"\n","    RESULTS_SUBDIR = Path(\"Result/Final_RGCNGRU1\")\n","    RESULTS_SAVE_DIR = DRIVE_BASE_PATH / RESULTS_SUBDIR\n","    os.makedirs(RESULTS_SAVE_DIR, exist_ok=True)\n","    DATA_PATH = DRIVE_BASE_PATH / DATA_SUBDIR / DATA_FILENAME\n","\n","    DATA_YEAR = 2023; DATA_MONTH = 5; DATA_DAY = 3\n","    START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES = 8; PREDICTION_HORIZON = 12\n","    training_config = { 'seed': 42, 'batch_size': 8, 'lr': 0.001, 'weight_decay': 1e-5, 'max_epochs': 1000, 'scheduler_patience': 20,\n","                       'early_stopping_patience': 45, 'T_pred_horizon': PREDICTION_HORIZON, 'results_dir': str(RESULTS_SAVE_DIR),\n","                        'global_env_emb_dim': 16, 'time_emb_dim': 8,\n","                        'gcn_hidden_dim': 128, 'gcn_output_dim': 128, 'num_relations': 5, 'gru_hidden_dim': 128,\n","                        'num_gru_layers': 1, 'mlp_prediction_hidden_dim': 64, 'fusion_mlp_output_dim': 128, 'fusion_mlp_hidden_dim': 64,\n","                        'dropout_rate_fusion_mlp': 0.2, 'dropout_rate_encoders': 0.1, 'dropout_rate_gcn': 0.3, 'dropout_rate_gru': 0.2,\n","                        'dropout_rate_pred_head': 0.2, 'use_amp': False, 'enable_profiler': False, 'num_workers': 0,\n","                        'pin_memory': False, 'train_split_ratio': 0.7, 'val_split_ratio': 0.2, 'h0_from_first_step': True }\n","\n","    all_graph_sequences_loaded = None\n","    try:\n","        if not DATA_PATH.exists(): raise FileNotFoundError(f\"数据文件在指定路径未找到: {DATA_PATH}\")\n","        with open(DATA_PATH, \"rb\") as f: all_graph_sequences_loaded = pickle.load(f)\n","        if not all_graph_sequences_loaded or not isinstance(all_graph_sequences_loaded, list) or not all_graph_sequences_loaded[0] or not isinstance(all_graph_sequences_loaded[0], list):\n","            raise ValueError(\"加载的数据格式不正确。\")\n","        expected_len_per_sequence = training_config['T_pred_horizon'] + 1\n","        processed_sequences = []\n","        for i, seq in enumerate(all_graph_sequences_loaded):\n","            if not isinstance(seq, list) or len(seq) != expected_len_per_sequence: continue\n","            valid_seq = True\n","            for step_idx, graph_step_data in enumerate(seq):\n","                if not isinstance(graph_step_data, Data) or not hasattr(graph_step_data, 'x') or graph_step_data.x is None or \\\n","                   not hasattr(graph_step_data, 'edge_index') or graph_step_data.edge_index is None or \\\n","                   not hasattr(graph_step_data, 'edge_attr') or graph_step_data.edge_attr is None or \\\n","                   graph_step_data.edge_attr.shape[1] < 5 or \\\n","                   not hasattr(graph_step_data, 'graph_global_env_features') or \\\n","                   (step_idx > 0 and (not hasattr(graph_step_data, 'y') or graph_step_data.y is None)):\n","                    valid_seq = False; break\n","                if step_idx > 0 and isinstance(graph_step_data.y, torch.Tensor) and graph_step_data.y.ndim == 1:\n","                    graph_step_data.y = graph_step_data.y.unsqueeze(1)\n","            if valid_seq: processed_sequences.append(seq)\n","        if not processed_sequences: raise ValueError(f\"数据处理后没有长度为 {expected_len_per_sequence} 的有效序列。\")\n","        all_graph_sequences = processed_sequences\n","        print(f\"成功加载并处理 {len(all_graph_sequences)} 个空间窗口的序列数据。\")\n","    except Exception as e: print(f\"加载或验证数据时发生错误: {e}\"); all_graph_sequences = None\n","\n","    if all_graph_sequences:\n","        base_datetime_for_timeline = dt_datetime(DATA_YEAR, DATA_MONTH, DATA_DAY, START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES)\n","        time_features_for_dataset_timeline = generate_time_features_for_sequence(base_datetime_for_timeline, training_config['T_pred_horizon'])\n","        trained_model, final_node_mean, final_node_std, final_target_mean, final_target_std = main_training_rgcn_gru_hourly_heads(all_graph_sequences, training_config, time_features_for_dataset_timeline)\n","        print(\"模型训练和评估完成!\")\n","    else:\n","        print(\"由于数据加载失败或数据为空，训练流程未启动。\")"]},{"cell_type":"markdown","metadata":{"id":"5UPE93sr2D7Z"},"source":["##RGCN+Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zsC5z78vsvb"},"outputs":[],"source":["# ===========================================================\n","# 0. 环境 & 依赖\n","# ===========================================================\n","import os\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch_geometric.nn import RGCNConv\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.loader import DataLoader\n","import pickle\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.metrics import r2_score\n","import gc\n","import math\n","from datetime import datetime as dt_datetime, timedelta\n","from pathlib import Path\n","import time\n","import json\n","import torchprofile\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","torch.cuda.ipc_collect()\n","\n","\n","# ===========================================================\n","# 1. 特征生成 & 辅助模块\n","# ===========================================================\n","\n","def generate_time_features_for_sequence(base_dt_obj, num_steps):\n","    time_features_list = []\n","    for i in range(num_steps):\n","        current_dt = base_dt_obj + timedelta(hours=i)\n","        hour_norm = current_dt.hour / 23.0\n","        day_of_year_norm = current_dt.timetuple().tm_yday / (366.0 if current_dt.year % 4 == 0 and (current_dt.year % 100 != 0 or current_dt.year % 400 == 0) else 365.0)\n","        hour_sin = math.sin(2 * math.pi * hour_norm)\n","        hour_cos = math.cos(2 * math.pi * hour_norm)\n","        doy_sin = math.sin(2 * math.pi * day_of_year_norm)\n","        doy_cos = math.cos(2 * math.pi * day_of_year_norm)\n","        time_features_list.append(torch.tensor([hour_sin, hour_cos, doy_sin, doy_cos], dtype=torch.float32))\n","    return torch.stack(time_features_list)\n","\n","\n","class MLPEncoder(nn.Module):\n","    def __init__(self, in_dim, out_dim, hid_dim=None, dropout_rate=0.1):\n","        super().__init__()\n","        if hid_dim is None:\n","            hid_dim = max(min(in_dim, out_dim), (in_dim + out_dim) // 2)\n","            if hid_dim == 0 and out_dim > 0 : hid_dim = out_dim\n","            if hid_dim == 0 and in_dim > 0 : hid_dim = in_dim\n","            if hid_dim == 0 : hid_dim = 1\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(in_dim, hid_dim),\n","            nn.ReLU(),\n","            nn.LayerNorm(hid_dim),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hid_dim, out_dim)\n","        )\n","    def forward(self, x):\n","        return self.mlp(x)\n","\n","class RGCNModule(nn.Module):\n","    def __init__(self, rgcn_input_dim, rgcn_hidden_dim, rgcn_output_dim, num_relations, dropout_rate=0.5):\n","        super().__init__()\n","        self.rgcn_input_dim = rgcn_input_dim\n","        self.rgcn_hidden_dim = rgcn_hidden_dim\n","        self.rgcn_output_dim = rgcn_output_dim\n","        self.num_relations = num_relations\n","\n","        self.conv1 = RGCNConv(rgcn_input_dim, rgcn_hidden_dim, num_relations)\n","        self.bn1 = nn.BatchNorm1d(rgcn_hidden_dim)\n","        self.prelu1 = nn.PReLU(rgcn_hidden_dim)\n","\n","        self.conv2 = RGCNConv(rgcn_hidden_dim, rgcn_hidden_dim, num_relations)\n","        self.bn2 = nn.BatchNorm1d(rgcn_hidden_dim)\n","        self.prelu2 = nn.PReLU(rgcn_hidden_dim)\n","\n","        self.conv3 = RGCNConv(rgcn_hidden_dim, rgcn_output_dim, num_relations)\n","        self.bn3 = nn.BatchNorm1d(rgcn_output_dim)\n","        self.prelu3 = nn.PReLU(rgcn_output_dim)\n","\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x, edge_index, edge_attr):\n","        if x.size(0) == 0: # Handle empty graphs if they occur\n","            return x\n","        if edge_attr is None or edge_attr.shape[1] < 5:\n","            raise ValueError(\"RGCNModule: edge_attr is missing or has insufficient columns for edge_type.\")\n","        edge_type = edge_attr[:, 4].long()\n","\n","        x = self.conv1(x, edge_index, edge_type=edge_type)\n","        if x.size(0) > 1: x = self.bn1(x)\n","        x = self.prelu1(x)\n","        x = self.dropout(x)\n","\n","        x = self.conv2(x, edge_index, edge_type=edge_type)\n","        if x.size(0) > 1: x = self.bn2(x)\n","        x = self.prelu2(x)\n","        x = self.dropout(x)\n","\n","        x = self.conv3(x, edge_index, edge_type=edge_type)\n","        if x.size(0) > 1: x = self.bn3(x)\n","        x = self.prelu3(x)\n","        return x\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=50):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        if d_model % 2 != 0: # Odd d_model case\n","             pe[:, 1::2] = torch.cos(position * div_term[:-1]) # Use div_term up to second to last\n","        else:\n","             pe[:, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)  # Shape: (max_len, d_model)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: Tensor, shape [sequence_length, batch_size, feature_dim] (for batch_first=False Transformer)\n","        self.pe shape is [max_len, feature_dim]\n","        \"\"\"\n","        # self.pe[:x.size(0), :] slices to (sequence_length, feature_dim)\n","        # .unsqueeze(1) changes it to (sequence_length, 1, feature_dim) for broadcasting\n","        x = x + self.pe[:x.size(0), :].unsqueeze(1)\n","        return self.dropout(x)\n","\n","# ===========================================================\n","# 2. RGCN-Transformer 模型定义\n","# ===========================================================\n","class RGCNTransformerModelWithHourlyHeads(nn.Module):\n","    def __init__(self,\n","                 static_node_in_dim,\n","                 global_env_in_dim,\n","                 time_in_dim,\n","                 global_env_emb_dim,\n","                 time_emb_dim,\n","                 rgcn_hidden_dim,\n","                 rgcn_output_dim,\n","                 num_relations,\n","                 transformer_d_model,\n","                 transformer_nhead,\n","                 transformer_num_encoder_layers,\n","                 transformer_num_decoder_layers,\n","                 transformer_dim_feedforward,\n","                 transformer_dropout_rate,\n","                 fusion_mlp_output_dim=None,\n","                 fusion_mlp_hidden_dim=None,\n","                 dropout_rate_fusion_mlp=0.1,\n","                 T_pred_horizon=12,\n","                 dropout_rate_encoders=0.1,\n","                 dropout_rate_rgcn=0.3,\n","                 mlp_prediction_hidden_dim=64,\n","                 dropout_rate_pred_head=0.2,\n","                 num_encoder_obs_steps=1\n","                ):\n","        super().__init__()\n","        self.T_pred_horizon = T_pred_horizon\n","        self.static_node_in_dim = static_node_in_dim\n","        self.global_env_in_dim = global_env_in_dim\n","        self.time_in_dim = time_in_dim\n","        self.num_relations = num_relations\n","        self.rgcn_output_dim = rgcn_output_dim\n","        self.num_encoder_obs_steps = num_encoder_obs_steps\n","\n","\n","        concatenated_feature_dim = rgcn_output_dim + global_env_emb_dim + time_emb_dim\n","        # If fusion_mlp_output_dim is specified, it should match transformer_d_model\n","        # If not specified, concatenated_feature_dim should ideally be transformer_d_model,\n","        # or fusion_mlp makes it transformer_d_model.\n","        if fusion_mlp_output_dim is not None and fusion_mlp_output_dim != transformer_d_model:\n","            print(f\"Warning: fusion_mlp_output_dim ({fusion_mlp_output_dim}) \"\n","                  f\"does not match transformer_d_model ({transformer_d_model}). \"\n","                  f\"Setting fusion_mlp_output_dim to transformer_d_model.\")\n","            actual_fusion_mlp_output_dim = transformer_d_model\n","        elif fusion_mlp_output_dim is None:\n","             actual_fusion_mlp_output_dim = transformer_d_model # Fusion MLP will output d_model\n","        else: # fusion_mlp_output_dim == transformer_d_model\n","            actual_fusion_mlp_output_dim = fusion_mlp_output_dim\n","\n","        self.d_model = transformer_d_model\n","        self.fusion_mlp_input_dim = concatenated_feature_dim\n","        self.transformer_input_dim = self.d_model\n","\n","\n","        self.global_env_encoder = MLPEncoder(global_env_in_dim, global_env_emb_dim, dropout_rate=dropout_rate_encoders)\n","        self.time_encoder = MLPEncoder(time_in_dim, time_emb_dim, dropout_rate=dropout_rate_encoders)\n","\n","        self.rgcn_module_for_encoder_inputs = RGCNModule(static_node_in_dim, rgcn_hidden_dim, rgcn_output_dim, num_relations, dropout_rate_rgcn)\n","        self.rgcn_module_for_decoder_inputs = RGCNModule(static_node_in_dim, rgcn_hidden_dim, rgcn_output_dim, num_relations, dropout_rate_rgcn)\n","\n","        self.fusion_mlp = MLPEncoder(\n","            in_dim=concatenated_feature_dim,\n","            out_dim=self.d_model,\n","            hid_dim=fusion_mlp_hidden_dim,\n","            dropout_rate=dropout_rate_fusion_mlp\n","        )\n","\n","        self.pos_encoder = PositionalEncoding(self.d_model, transformer_dropout_rate, max_len=T_pred_horizon + num_encoder_obs_steps + 5) # Extra buffer for max_len\n","\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=transformer_nhead,\n","                                                 dim_feedforward=transformer_dim_feedforward,\n","                                                 dropout=transformer_dropout_rate, batch_first=False)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=transformer_num_encoder_layers)\n","\n","        decoder_layer = nn.TransformerDecoderLayer(d_model=self.d_model, nhead=transformer_nhead,\n","                                                 dim_feedforward=transformer_dim_feedforward,\n","                                                 dropout=transformer_dropout_rate, batch_first=False)\n","        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=transformer_num_decoder_layers)\n","\n","        self.hourly_prediction_heads = nn.ModuleList()\n","        for _ in range(T_pred_horizon):\n","            self.hourly_prediction_heads.append(\n","                nn.Sequential(\n","                    nn.Linear(self.d_model, mlp_prediction_hidden_dim),\n","                    nn.ReLU(),\n","                    nn.Dropout(dropout_rate_pred_head),\n","                    nn.Linear(mlp_prediction_hidden_dim, 1)\n","                )\n","            )\n","\n","        self.register_buffer('node_feat_mean', torch.zeros(static_node_in_dim))\n","        self.register_buffer('node_feat_std', torch.ones(static_node_in_dim))\n","\n","    def _generate_square_subsequent_mask(self, sz, device):\n","        mask = (torch.triu(torch.ones(sz, sz, device=device)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def _process_one_timestep_input(self, pyg_batch_timestep, rgcn_module, time_feature_for_step, device, step_type=\"generic\"):\n","        normalized_x = (pyg_batch_timestep.x - self.node_feat_mean) / (self.node_feat_std + 1e-8)\n","        rgcn_output_nodes = rgcn_module(\n","            normalized_x,\n","            pyg_batch_timestep.edge_index,\n","            pyg_batch_timestep.edge_attr\n","        )\n","\n","        global_env_feat_unencoded = pyg_batch_timestep.graph_global_env_features\n","        expected_num_graphs = pyg_batch_timestep.num_graphs\n","        expected_global_dim = self.global_env_encoder.mlp[0].in_features # Get in_dim from the layer\n","\n","        if not (global_env_feat_unencoded.shape == (expected_num_graphs, expected_global_dim)):\n","             if global_env_feat_unencoded.numel() == expected_num_graphs * expected_global_dim:\n","                global_env_feat_unencoded = global_env_feat_unencoded.view(expected_num_graphs, expected_global_dim)\n","             else:\n","                # print(f\"Warning ({step_type}): Correcting global_env_feat shape from {global_env_feat_unencoded.shape} to ({expected_num_graphs}, {expected_global_dim}) with zeros.\")\n","                global_env_feat_unencoded = torch.zeros(expected_num_graphs, expected_global_dim, device=device)\n","\n","        global_env_emb = self.global_env_encoder(global_env_feat_unencoded)\n","        global_env_emb_expanded = global_env_emb[pyg_batch_timestep.batch] # Expand to node level\n","\n","        current_emb_time_feat = self.time_encoder(time_feature_for_step.to(device))\n","        num_nodes_in_batch = pyg_batch_timestep.num_nodes\n","        time_emb_expanded_to_nodes = current_emb_time_feat.unsqueeze(0).expand(num_nodes_in_batch, -1)\n","\n","        concatenated_features = torch.cat([rgcn_output_nodes, global_env_emb_expanded, time_emb_expanded_to_nodes], dim=-1)\n","        fused_features = self.fusion_mlp(concatenated_features)\n","        return fused_features\n","\n","    def forward(self, list_of_batched_timesteps: list, timeline_time_features: torch.Tensor, device: torch.device):\n","        # timeline_time_features: (T_pred_horizon, time_feat_dim) for decoder steps\n","        # list_of_batched_timesteps: Length NUM_ENCODER_OBS_STEPS + T_PRED_HORIZON\n","        # First NUM_ENCODER_OBS_STEPS are for encoder, rest for decoder targets/inputs\n","\n","        # 1. Prepare Encoder Input\n","        encoder_input_fused_features_list = []\n","        for i in range(self.num_encoder_obs_steps):\n","            pyg_batch_enc_step = list_of_batched_timesteps[i].to(device)\n","            # We need time features for encoder steps.\n","            # If timeline_time_features is only for decoder, we make a placeholder or require them to be passed.\n","            # For this iteration, let's assume timeline_time_features needs to be extended/pre-pended if obs steps > 0\n","            # For num_encoder_obs_steps=1, we'll use a placeholder or derive one.\n","            # The simplest for now is to pass a dummy time feature for these encoder steps.\n","            # A more robust solution would be to have time features for all input steps.\n","            if i < timeline_time_features.size(0) : # If timeline features also cover obs steps\n","                 current_time_feat_for_enc = timeline_time_features[i, :]\n","            else: # Fallback if timeline_time_features is only for prediction horizon\n","                 # This case needs to be handled based on how data is prepared.\n","                 # Using a zero tensor as a placeholder if obs steps are not covered by timeline_time_features\n","                 # print(f\"Warning: Using placeholder time feature for encoder step {i}\")\n","                 current_time_feat_for_enc = torch.zeros(self.time_in_dim, device=device)\n","\n","\n","            fused_enc_step = self._process_one_timestep_input(\n","                pyg_batch_enc_step, self.rgcn_module_for_encoder_inputs,\n","                current_time_feat_for_enc, device, step_type=f\"encoder_step_{i}\"\n","            ) # (N_nodes_in_batch_i, d_model)\n","            encoder_input_fused_features_list.append(fused_enc_step)\n","\n","        # Stack along sequence length for Transformer: (L_enc, N_nodes_total, d_model)\n","        src = torch.stack(encoder_input_fused_features_list, dim=0)\n","        src = self.pos_encoder(src)\n","        memory = self.transformer_encoder(src) # Shape: (L_enc, N_nodes_total, d_model)\n","\n","        # 2. Prepare Decoder Inputs (for teacher forcing during training)\n","        all_decoder_input_fused_features_list = []\n","        for t_pred_idx in range(self.T_pred_horizon):\n","            pyg_batch_this_timestep = list_of_batched_timesteps[self.num_encoder_obs_steps + t_pred_idx].to(device)\n","            # timeline_time_features is already aligned with prediction horizon\n","            current_time_feature = timeline_time_features[t_pred_idx, :]\n","\n","            fused_features_step_t = self._process_one_timestep_input(\n","                pyg_batch_this_timestep, self.rgcn_module_for_decoder_inputs,\n","                current_time_feature, device, step_type=f\"decoder_step_{t_pred_idx}\"\n","            ) # (N_nodes_in_batch_t, d_model)\n","            all_decoder_input_fused_features_list.append(fused_features_step_t)\n","\n","        tgt = torch.stack(all_decoder_input_fused_features_list, dim=0) # (T_pred_horizon, N_nodes_total, d_model)\n","        tgt = self.pos_encoder(tgt)\n","\n","        tgt_mask = self._generate_square_subsequent_mask(self.T_pred_horizon, device)\n","\n","        decoder_output = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n","        # decoder_output shape: (T_pred_horizon, N_nodes_total, d_model)\n","\n","        # 3. Prediction Heads\n","        all_hourly_final_predictions_scaled = []\n","        for t in range(self.T_pred_horizon):\n","            decoder_output_t = decoder_output[t, :, :]\n","            prediction_t_scaled = self.hourly_prediction_heads[t](decoder_output_t)\n","            all_hourly_final_predictions_scaled.append(prediction_t_scaled.squeeze(-1))\n","\n","        predictions_scaled = torch.stack(all_hourly_final_predictions_scaled, dim=1) # (N_nodes_total, T_pred_horizon)\n","        return predictions_scaled\n","\n","# ===========================================================\n","# 3. 评估指标函数 - NO CHANGES\n","# ===========================================================\n","# ... (mse_loss_masked, calculate_hourly_metrics are identical) ...\n","def mse_loss_masked(predictions_scaled, targets_scaled, mask):\n","    expanded_mask = mask.unsqueeze(1).expand_as(targets_scaled)\n","    valid_targets_mask = ~torch.isnan(targets_scaled)\n","    final_mask = expanded_mask & valid_targets_mask\n","    if final_mask.sum() == 0:\n","        return torch.tensor(0.0, device=predictions_scaled.device, requires_grad=True)\n","    loss = F.mse_loss(predictions_scaled[final_mask], targets_scaled[final_mask])\n","    return loss\n","\n","def calculate_hourly_metrics(predictions_scaled, targets_scaled, node_masks, target_mean, target_std):\n","    target_mean_cpu = target_mean.cpu()\n","    target_std_cpu = target_std.cpu()\n","    preds_unscaled = predictions_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","    targets_unscaled = targets_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","\n","    num_nodes_total, T_horizon = preds_unscaled.shape\n","    hourly_metrics_dict = {}\n","\n","    preds_np = preds_unscaled.numpy()\n","    targets_np = targets_unscaled.numpy()\n","    mask_np = node_masks.cpu().numpy()\n","\n","    for t in range(T_horizon):\n","        preds_t_all_nodes = preds_np[:, t]\n","        targets_t_all_nodes = targets_np[:, t]\n","        preds_t_on_loss_nodes = preds_t_all_nodes[mask_np]\n","        targets_t_on_loss_nodes = targets_t_all_nodes[mask_np]\n","        valid_target_data_mask_t = ~np.isnan(targets_t_on_loss_nodes)\n","        preds_t_final_valid = preds_t_on_loss_nodes[valid_target_data_mask_t]\n","        targets_t_final_valid = targets_t_on_loss_nodes[valid_target_data_mask_t]\n","\n","        if preds_t_final_valid.shape[0] < 2:\n","            hourly_metrics_dict[t] = {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count': 0}\n","            continue\n","        mse = np.mean((preds_t_final_valid - targets_t_final_valid)**2)\n","        mae = np.mean(np.abs(preds_t_final_valid - targets_t_final_valid))\n","        rmse = np.sqrt(mse)\n","        try:\n","            r2 = r2_score(targets_t_final_valid, preds_t_final_valid)\n","        except ValueError:\n","            r2 = np.nan\n","        hourly_metrics_dict[t] = {'mse': mse, 'mae': mae, 'rmse': rmse, 'r2': r2, 'count': preds_t_final_valid.shape[0]}\n","    return hourly_metrics_dict\n","\n","# ===========================================================\n","# 4. 训练与评估循环 - Train/Eval loops need to use correct target indices\n","# ===========================================================\n","def train_epoch(model, loader, optimizer, device, timeline_time_features,\n","                node_feat_mean, node_feat_std, target_mean, target_std):\n","    model.train()\n","    total_loss_scaled = 0\n","    num_sequences_processed = 0\n","    model.node_feat_mean = node_feat_mean.to(device)\n","    model.node_feat_std = node_feat_std.to(device)\n","\n","    target_mean_dev = target_mean.to(device)\n","    target_std_dev = target_std.to(device)\n","\n","    epoch_start_time = time.time()\n","    for list_of_batched_timesteps in loader: # Each element is a list of Batch objects for one super-sample\n","        optimizer.zero_grad()\n","        # timeline_time_features corresponds to the T_PRED_HORIZON steps for the decoder\n","        predictions_batch_scaled = model(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","\n","        targets_list_for_loss_scaled = []\n","        # Mask is based on the first *prediction* step's graph data\n","        # If num_encoder_obs_steps=1, this is list_of_batched_timesteps[1]\n","        # If num_encoder_obs_steps=k, this is list_of_batched_timesteps[k]\n","        first_prediction_graph_step_in_batch = list_of_batched_timesteps[model.num_encoder_obs_steps].to(device)\n","        mask_for_loss = ~first_prediction_graph_step_in_batch.building_mask\n","\n","        for t_pred_idx in range(model.T_pred_horizon):\n","            # Targets come from the prediction horizon part of the sequence\n","            current_target_timestep_batch = list_of_batched_timesteps[model.num_encoder_obs_steps + t_pred_idx].to(device)\n","            targets_t_nodes_original = current_target_timestep_batch.y.squeeze()\n","            targets_t_nodes_scaled = (targets_t_nodes_original - target_mean_dev) / (target_std_dev + 1e-8)\n","            targets_list_for_loss_scaled.append(targets_t_nodes_scaled)\n","\n","        targets_batch_scaled = torch.stack(targets_list_for_loss_scaled, dim=1)\n","        loss = mse_loss_masked(predictions_batch_scaled, targets_batch_scaled, mask_for_loss)\n","        num_sequences_in_this_super_batch = list_of_batched_timesteps[0].num_graphs # num_graphs from first element (e.g. obs step)\n","\n","        if not torch.isnan(loss) and not torch.isinf(loss) and loss.item() > 0 :\n","            loss.backward()\n","            optimizer.step()\n","            total_loss_scaled += loss.item() * num_sequences_in_this_super_batch\n","        num_sequences_processed += num_sequences_in_this_super_batch\n","\n","    epoch_duration = time.time() - epoch_start_time\n","    avg_loss = total_loss_scaled / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","    return avg_loss, epoch_duration\n","\n","def evaluate_epoch(model, loader, device, timeline_time_features,\n","                   node_feat_mean, node_feat_std, target_mean, target_std, epoch_type=\"Eval\"):\n","    model.eval()\n","    all_batch_predictions_scaled = []\n","    all_batch_targets_scaled = []\n","    all_batch_masks_for_metrics = []\n","    total_loss_scaled = 0\n","    num_sequences_processed = 0\n","    model.node_feat_mean = node_feat_mean.to(device)\n","    model.node_feat_std = node_feat_std.to(device)\n","\n","    target_mean_dev = target_mean.to(device)\n","    target_std_dev = target_std.to(device)\n","\n","    eval_start_time = time.time()\n","    with torch.no_grad():\n","        for list_of_batched_timesteps in loader:\n","            predictions_batch_scaled = model(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","            targets_list_original = []\n","\n","            first_prediction_graph_step_in_batch = list_of_batched_timesteps[model.num_encoder_obs_steps].to(device)\n","            mask_for_metrics = ~first_prediction_graph_step_in_batch.building_mask\n","\n","            for t_pred_idx in range(model.T_pred_horizon):\n","                current_target_timestep_batch = list_of_batched_timesteps[model.num_encoder_obs_steps + t_pred_idx].to(device)\n","                targets_t_nodes_original = current_target_timestep_batch.y.squeeze()\n","                targets_list_original.append(targets_t_nodes_original)\n","\n","            targets_batch_original = torch.stack(targets_list_original, dim=1)\n","            targets_batch_scaled_for_loss = (targets_batch_original - target_mean_dev) / (target_std_dev + 1e-8)\n","            loss = mse_loss_masked(predictions_batch_scaled, targets_batch_scaled_for_loss, mask_for_metrics)\n","            num_sequences_in_this_super_batch = list_of_batched_timesteps[0].num_graphs\n","\n","            if not torch.isnan(loss) and not torch.isinf(loss):\n","                total_loss_scaled += loss.item() * num_sequences_in_this_super_batch\n","            num_sequences_processed += num_sequences_in_this_super_batch\n","\n","            all_batch_predictions_scaled.append(predictions_batch_scaled.cpu())\n","            all_batch_targets_scaled.append(targets_batch_scaled_for_loss.cpu())\n","            all_batch_masks_for_metrics.append(mask_for_metrics.cpu())\n","\n","    eval_duration = time.time() - eval_start_time\n","    avg_loss_scaled = total_loss_scaled / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","\n","    if not all_batch_predictions_scaled:\n","        empty_metrics = {t: {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0} for t in range(model.T_pred_horizon)}\n","        return avg_loss_scaled, empty_metrics, eval_duration\n","\n","    final_predictions_scaled = torch.cat(all_batch_predictions_scaled, dim=0)\n","    final_targets_scaled = torch.cat(all_batch_targets_scaled, dim=0)\n","    final_masks_for_metrics = torch.cat(all_batch_masks_for_metrics, dim=0)\n","\n","    hourly_metrics_original_scale = calculate_hourly_metrics(final_predictions_scaled, final_targets_scaled,\n","                                                             final_masks_for_metrics, target_mean.cpu(), target_std.cpu())\n","    return avg_loss_scaled, hourly_metrics_original_scale, eval_duration\n","\n","# ===========================================================\n","# 5. 主训练流程 (RGCN-Transformer)\n","# ===========================================================\n","\n","def calculate_aggregated_metrics_report(hourly_metrics_dict, T_pred_horizon):\n","    # ... (identical to previous version) ...\n","    metrics_to_aggregate = ['r2', 'mse', 'mae', 'rmse']\n","    aggregated_report = {}\n","    for metric_name in metrics_to_aggregate:\n","        values = [hourly_metrics_dict[t][metric_name] for t in range(T_pred_horizon) if t in hourly_metrics_dict and not np.isnan(hourly_metrics_dict[t][metric_name])]\n","        if values:\n","            aggregated_report[f'avg_{metric_name}'] = np.mean(values)\n","            aggregated_report[f'std_{metric_name}'] = np.std(values)\n","        else:\n","            aggregated_report[f'avg_{metric_name}'] = np.nan\n","            aggregated_report[f'std_{metric_name}'] = np.nan\n","    return aggregated_report\n","\n","def main_training_rgcn_transformer_hourly_heads( # Renamed\n","    all_sequences_data: list,\n","    config: dict,\n","    time_features_for_dataset: torch.Tensor # This is for decoder input steps\n","):\n","    train_start_time = time.time()\n","    report_data = {'config': config}\n","\n","    seed = config.get('seed', 42)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"使用设备: {device}\")\n","    report_data['device'] = str(device)\n","\n","    T_PRED_HORIZON = config.get('T_pred_horizon', 12)\n","    NUM_ENCODER_OBS_STEPS = config.get('num_encoder_obs_steps', 1)\n","    expected_input_len = NUM_ENCODER_OBS_STEPS + T_PRED_HORIZON\n","\n","    valid_sequences_data = [seq for seq in all_sequences_data if isinstance(seq, list) and len(seq) == expected_input_len]\n","    if len(valid_sequences_data) != len(all_sequences_data):\n","        print(f\"警告: 从 {len(all_sequences_data)} 个序列中筛选出 {len(valid_sequences_data)} 个长度为 {expected_input_len} 的有效序列。\")\n","    if not valid_sequences_data:\n","        raise ValueError(f\"没有找到长度为 {expected_input_len} 的有效序列数据。\")\n","    all_sequences_data = valid_sequences_data\n","\n","    # Dataset split\n","    num_total_sequences = len(all_sequences_data)\n","    indices = np.random.permutation(num_total_sequences)\n","    train_split_ratio = config.get('train_split_ratio', 0.7)\n","    val_split_ratio = config.get('val_split_ratio', 0.2)\n","    train_size = int(train_split_ratio * num_total_sequences)\n","    val_size = int(val_split_ratio * num_total_sequences)\n","    train_indices = indices[:train_size]\n","    val_indices = indices[train_size : train_size + val_size]\n","    test_indices = indices[train_size + val_size :]\n","    train_dataset = [all_sequences_data[i] for i in train_indices]\n","    val_dataset   = [all_sequences_data[i] for i in val_indices]\n","    test_dataset  = [all_sequences_data[i] for i in test_indices]\n","    report_data['dataset_split'] = {'total_sequences': num_total_sequences, 'train_size': len(train_dataset), 'val_size': len(val_dataset), 'test_size': len(test_dataset)}\n","\n","    # Scaler calculation\n","    all_train_node_features_list = []\n","    all_train_target_values_list_for_scaling = []\n","    for seq in train_dataset:\n","        for i_step, graph_data in enumerate(seq):\n","            if hasattr(graph_data, 'x') and graph_data.x is not None:\n","                all_train_node_features_list.append(graph_data.x)\n","            if i_step >= NUM_ENCODER_OBS_STEPS and hasattr(graph_data, 'y') and graph_data.y is not None:\n","                y_original = graph_data.y.squeeze(); current_mask_for_loss = ~graph_data.building_mask\n","                valid_target_indices = current_mask_for_loss & ~torch.isnan(y_original)\n","                if valid_target_indices.sum() > 0:\n","                    all_train_target_values_list_for_scaling.append(y_original[valid_target_indices])\n","\n","    if not all_train_node_features_list: raise ValueError(\"训练数据中未找到节点特征 'x'，无法计算scaler！\")\n","    all_train_node_features_tensor = torch.cat(all_train_node_features_list, dim=0)\n","    node_feat_mean = torch.mean(all_train_node_features_tensor, dim=0); node_feat_std = torch.std(all_train_node_features_tensor, dim=0)\n","    node_feat_std[node_feat_std < 1e-8] = 1.0\n","    scaler_path_x = Path(config['results_dir']) / \"node_feature_scaler_rgcn_transformer.pth\"\n","    torch.save({'mean': node_feat_mean, 'std': node_feat_std}, scaler_path_x); print(f\"节点特征x scaler已保存到: {scaler_path_x}\")\n","\n","    if not all_train_target_values_list_for_scaling:\n","        target_mean = torch.tensor(0.0); target_std = torch.tensor(1.0)\n","    else:\n","        all_train_target_values_tensor = torch.cat(all_train_target_values_list_for_scaling, dim=0)\n","        target_mean = torch.mean(all_train_target_values_tensor.float()); target_std = torch.std(all_train_target_values_tensor.float())\n","        if target_std < 1e-8: target_std = torch.tensor(1.0)\n","    target_scaler_path = Path(config['results_dir']) / \"target_scaler_rgcn_transformer.pth\"\n","    torch.save({'mean': target_mean, 'std': target_std}, target_scaler_path); print(f\"目标值y scaler已保存到: {target_scaler_path}\")\n","\n","    # DataLoaders\n","    batch_size = config.get('batch_size', 8); num_workers = config.get('num_workers', 0)\n","    pin_memory_flag = config.get('pin_memory', False) and device.type == 'cuda'\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=num_workers, pin_memory=pin_memory_flag)\n","    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n","    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n","\n","    # Model Initialization\n","    sample_graph_encoder_input_for_dims = all_sequences_data[0][0]\n","    static_node_in_dim = sample_graph_encoder_input_for_dims.x.shape[1]\n","    global_env_in_dim = sample_graph_encoder_input_for_dims.graph_global_env_features.shape[0] if sample_graph_encoder_input_for_dims.graph_global_env_features.ndim == 1 else sample_graph_encoder_input_for_dims.graph_global_env_features.shape[1]\n","    time_in_dim = time_features_for_dataset.shape[1]\n","    num_relations = config.get('num_relations', 5)\n","\n","    d_model = config.get('transformer_d_model', config.get('gru_hidden_dim', 128))\n","    n_head = config.get('transformer_nhead', 4)\n","    num_enc_layers = config.get('transformer_num_encoder_layers', 2)\n","    num_dec_layers = config.get('transformer_num_decoder_layers', 2)\n","    dim_ff = config.get('transformer_dim_feedforward', d_model * 4)\n","    transformer_dropout = config.get('transformer_dropout_rate', config.get('dropout_rate_gru', 0.2))\n","    fusion_mlp_out_dim = config.get('fusion_mlp_output_dim', d_model)\n","    fusion_mlp_hid_dim = config.get('fusion_mlp_hidden_dim', d_model // 2 if d_model else 64)\n","\n","\n","    model = RGCNTransformerModelWithHourlyHeads(\n","        static_node_in_dim=static_node_in_dim, global_env_in_dim=global_env_in_dim, time_in_dim=time_in_dim,\n","        global_env_emb_dim=config.get('global_env_emb_dim', 16), time_emb_dim=config.get('time_emb_dim', 8),\n","        rgcn_hidden_dim=config.get('gcn_hidden_dim', 128), rgcn_output_dim=config.get('gcn_output_dim', 128),\n","        num_relations=num_relations,\n","        transformer_d_model=d_model,\n","        transformer_nhead=n_head,\n","        transformer_num_encoder_layers=num_enc_layers,\n","        transformer_num_decoder_layers=num_dec_layers,\n","        transformer_dim_feedforward=dim_ff,\n","        transformer_dropout_rate=transformer_dropout,\n","        fusion_mlp_output_dim=fusion_mlp_out_dim,\n","        fusion_mlp_hidden_dim=fusion_mlp_hid_dim,\n","        dropout_rate_fusion_mlp=config.get('dropout_rate_fusion_mlp', 0.2),\n","        T_pred_horizon=T_PRED_HORIZON,\n","        dropout_rate_encoders=config.get('dropout_rate_encoders', 0.1),\n","        dropout_rate_rgcn=config.get('dropout_rate_gcn', 0.3),\n","        mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim', 64),\n","        dropout_rate_pred_head=config.get('dropout_rate_pred_head', 0.2),\n","        num_encoder_obs_steps=NUM_ENCODER_OBS_STEPS\n","    ).to(device)\n","\n","    model.node_feat_mean = node_feat_mean.to(device); model.node_feat_std = node_feat_std.to(device)\n","    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"模型总参数量 (RGCN-Transformer): {total_params:,}\"); report_data['model_total_parameters'] = total_params\n","\n","    # ===== Component FLOPS Calculation =====\n","    print(\"\\nCalculating MACs for model components (approximate FLOPS):\")\n","    report_data['component_gmacs'] = {}\n","    dummy_nodes_component = 2500\n","    dummy_edges_component = 60000\n","    dummy_batch_global_comp = 1\n","    model.eval()\n","\n","    # 1. RGCNModule\n","    try:\n","        rgcn_module_to_profile = model.rgcn_module_for_encoder_inputs\n","        dummy_x_rgcn = torch.randn(dummy_nodes_component, rgcn_module_to_profile.rgcn_input_dim, device=device)\n","        dummy_ei_rgcn = torch.randint(0, dummy_nodes_component, (2, dummy_edges_component), device=device)\n","        dummy_ea_rgcn = torch.randn(dummy_edges_component, 5, device=device)\n","        dummy_ea_rgcn[:, 4] = torch.randint(0, model.num_relations, (dummy_edges_component,), device=device).float()\n","        macs_rgcn = torchprofile.profile_macs(rgcn_module_to_profile, args=(dummy_x_rgcn, dummy_ei_rgcn, dummy_ea_rgcn))\n","        report_data['component_gmacs']['rgcn_module'] = macs_rgcn / 1e9\n","        print(f\"  RGCNModule GMACs: {macs_rgcn / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling RGCNModule: {e}\"); report_data['component_gmacs']['rgcn_module'] = \"Error\"\n","\n","    # 2. Transformer Encoder Layer (Manual MAC Calculation for one layer)\n","    print(f\"  Manually Calculating MACs for one Transformer Encoder Layer:\")\n","    try:\n","        N_nodes_trans = dummy_nodes_component\n","        L_enc_seq_trans = NUM_ENCODER_OBS_STEPS\n","\n","        macs_self_attn_enc = N_nodes_trans * L_enc_seq_trans * (4 * d_model * d_model)\n","        macs_ffn_enc = N_nodes_trans * L_enc_seq_trans * ( (d_model * dim_ff) + (dim_ff * d_model) )\n","        macs_one_encoder_layer_manual = macs_self_attn_enc + macs_ffn_enc\n","\n","        gmacs_one_encoder_layer_manual = macs_one_encoder_layer_manual / 1e9\n","        report_data['component_gmacs']['transformer_encoder_layer_one'] = gmacs_one_encoder_layer_manual\n","        report_data['component_gmacs']['transformer_encoder_layer_profiling_notes'] = \"Manually calculated (SelfAttn(4d^2NL) + FFN(2d*d_ff*NL)).\"\n","        print(f\"  Transformer Encoder Layer (One): d_model={d_model}, n_head={n_head}, d_ff={dim_ff}\")\n","        print(f\"  Used for calculation: N_nodes={N_nodes_trans}, L_enc_seq={L_enc_seq_trans}\")\n","        print(f\"  One Encoder Layer GMACs (Manual): {gmacs_one_encoder_layer_manual:.4f}\")\n","        total_encoder_gmacs = num_enc_layers * gmacs_one_encoder_layer_manual\n","        report_data['component_gmacs']['transformer_encoder_total'] = total_encoder_gmacs\n","        print(f\"  Total Transformer Encoder ({num_enc_layers} layers) GMACs (Manual): {total_encoder_gmacs:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error manually calculating Transformer Encoder Layer MACs: {e}\")\n","        report_data['component_gmacs']['transformer_encoder_layer_one'] = \"Error\"; report_data['component_gmacs']['transformer_encoder_total'] = \"Error\"\n","        report_data['component_gmacs']['transformer_encoder_layer_profiling_notes'] = f\"Error: {str(e)}\"\n","\n","    # 3. Transformer Decoder Layer (Manual MAC Calculation for one layer)\n","    print(f\"  Manually Calculating MACs for one Transformer Decoder Layer:\")\n","    try:\n","        N_nodes_trans = dummy_nodes_component\n","        L_dec_seq_trans = T_PRED_HORIZON\n","        # L_enc_seq_mem_trans = NUM_ENCODER_OBS_STEPS # Length of memory from encoder\n","\n","        macs_self_attn_dec = N_nodes_trans * L_dec_seq_trans * (4 * d_model * d_model)\n","        macs_cross_attn_dec = N_nodes_trans * L_dec_seq_trans * (4 * d_model * d_model)\n","        macs_ffn_dec = N_nodes_trans * L_dec_seq_trans * ( (d_model * dim_ff) + (dim_ff * d_model) )\n","        macs_one_decoder_layer_manual = macs_self_attn_dec + macs_cross_attn_dec + macs_ffn_dec\n","\n","        gmacs_one_decoder_layer_manual = macs_one_decoder_layer_manual / 1e9\n","        report_data['component_gmacs']['transformer_decoder_layer_one'] = gmacs_one_decoder_layer_manual\n","        report_data['component_gmacs']['transformer_decoder_layer_profiling_notes'] = \"Manually calculated (SelfAttn(4d^2NL) + CrossAttn(4d^2NL) + FFN(2d*d_ff*NL)).\"\n","        print(f\"  Transformer Decoder Layer (One): d_model={d_model}, n_head={n_head}, d_ff={dim_ff}\")\n","        print(f\"  Used for calculation: N_nodes={N_nodes_trans}, L_dec_seq={L_dec_seq_trans}\")\n","        print(f\"  One Decoder Layer GMACs (Manual): {gmacs_one_decoder_layer_manual:.4f}\")\n","        total_decoder_gmacs = num_dec_layers * gmacs_one_decoder_layer_manual\n","        report_data['component_gmacs']['transformer_decoder_total'] = total_decoder_gmacs\n","        print(f\"  Total Transformer Decoder ({num_dec_layers} layers) GMACs (Manual): {total_decoder_gmacs:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error manually calculating Transformer Decoder Layer MACs: {e}\")\n","        report_data['component_gmacs']['transformer_decoder_layer_one'] = \"Error\"; report_data['component_gmacs']['transformer_decoder_total'] = \"Error\"\n","        report_data['component_gmacs']['transformer_decoder_layer_profiling_notes'] = f\"Error: {str(e)}\"\n","\n","    # 4. Fusion MLP\n","    try:\n","        fusion_mlp_to_profile = model.fusion_mlp\n","        dummy_input_fusion_mlp = torch.randn(dummy_nodes_component, model.fusion_mlp_input_dim, device=device)\n","        macs_fusion_mlp = torchprofile.profile_macs(fusion_mlp_to_profile, args=(dummy_input_fusion_mlp,))\n","        report_data['component_gmacs']['fusion_mlp'] = macs_fusion_mlp / 1e9\n","        print(f\"  Fusion MLP GMACs: {macs_fusion_mlp / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Fusion MLP: {e}\"); report_data['component_gmacs']['fusion_mlp'] = \"Error\"\n","\n","    # 5. Prediction Head\n","    try:\n","        pred_head_to_profile = model.hourly_prediction_heads[0]\n","        dummy_input_pred_head = torch.randn(dummy_nodes_component, model.d_model, device=device)\n","        macs_pred_head = torchprofile.profile_macs(pred_head_to_profile, args=(dummy_input_pred_head,))\n","        report_data['component_gmacs']['prediction_head_mlp'] = macs_pred_head / 1e9\n","        print(f\"  Prediction Head MLP (single hour) GMACs: {macs_pred_head / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Prediction Head: {e}\"); report_data['component_gmacs']['prediction_head_mlp'] = \"Error\"\n","\n","    # 6. Global Environment Encoder\n","    try:\n","        encoder_to_profile = model.global_env_encoder\n","        dummy_input_encoder = torch.randn(dummy_batch_global_comp, model.global_env_in_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['global_env_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  Global Env Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Global Env Encoder: {e}\"); report_data['component_gmacs']['global_env_encoder_mlp'] = \"Error\"\n","\n","    # 7. Time Encoder\n","    try:\n","        encoder_to_profile = model.time_encoder\n","        dummy_input_encoder = torch.randn(dummy_batch_global_comp, model.time_in_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['time_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  Time Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Time Encoder: {e}\"); report_data['component_gmacs']['time_encoder_mlp'] = \"Error\"\n","\n","    model.train()\n","    # ===== End Component FLOPS Calculation =====\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config.get('lr', 0.001), weight_decay=config.get('weight_decay', 1e-5))\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=config.get('scheduler_patience', 20), verbose=True)\n","\n","    best_val_loss_scaled = float('inf'); best_val_hourly_metrics_original = None; best_epoch = 0\n","    patience_counter = 0; max_epochs = config.get('max_epochs', 300); early_stopping_patience = config.get('early_stopping_patience', 45)\n","    model_save_path = Path(config['results_dir']) / f\"best_rgcntransformer_hourly_heads_model_seed{seed}.pth\"\n","    timeline_time_features_on_device = time_features_for_dataset.to(device) # This is for decoder steps\n","    target_mean_cpu = target_mean.cpu(); target_std_cpu = target_std.cpu()\n","    target_mean_on_device = target_mean.to(device); target_std_on_device = target_std.to(device)\n","    epoch_times = []\n","\n","    for epoch in range(1, max_epochs + 1):\n","        train_loss_scaled, epoch_duration = train_epoch(model, train_loader, optimizer, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_on_device, target_std_on_device)\n","        epoch_times.append(epoch_duration)\n","        val_loss_scaled, val_hourly_metrics_original, _ = evaluate_epoch(model, val_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Validation\")\n","        scheduler.step(val_loss_scaled)\n","        print(f\"Epoch {epoch:03d} | Train Scaled MSE: {train_loss_scaled:.4f} | Val Scaled MSE: {val_loss_scaled:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f} | Epoch Time: {epoch_duration:.2f}s\")\n","        _print_hourly_metrics_summary(\"Val\", val_hourly_metrics_original, T_PRED_HORIZON, indent=\"                     \")\n","        if val_loss_scaled < best_val_loss_scaled:\n","            best_val_loss_scaled = val_loss_scaled; best_val_hourly_metrics_original = val_hourly_metrics_original; best_epoch = epoch\n","            patience_counter = 0; torch.save(model.state_dict(), model_save_path)\n","            print(f\"                     ---> Best model saved (Epoch: {epoch}, Val Scaled MSE: {best_val_loss_scaled:.4f})\")\n","        else:\n","            patience_counter += 1\n","        if patience_counter >= early_stopping_patience:\n","            print(f\"Early stopping at epoch {epoch} due to no improvement.\"); break\n","\n","    total_training_duration = time.time() - train_start_time\n","    report_data['total_training_time_seconds'] = total_training_duration\n","    report_data['average_epoch_time_seconds'] = np.mean(epoch_times) if epoch_times else np.nan\n","    report_data['num_epochs_trained'] = epoch; report_data['best_validation_epoch'] = best_epoch\n","    report_data['best_validation_scaled_mse'] = best_val_loss_scaled\n","\n","    model_for_eval = RGCNTransformerModelWithHourlyHeads(\n","        static_node_in_dim=static_node_in_dim, global_env_in_dim=global_env_in_dim, time_in_dim=time_in_dim,\n","        global_env_emb_dim=config.get('global_env_emb_dim', 16), time_emb_dim=config.get('time_emb_dim', 8),\n","        rgcn_hidden_dim=config.get('gcn_hidden_dim', 128), rgcn_output_dim=config.get('gcn_output_dim', 128),\n","        num_relations=num_relations,\n","        transformer_d_model=d_model, transformer_nhead=n_head,\n","        transformer_num_encoder_layers=num_enc_layers, transformer_num_decoder_layers=num_dec_layers,\n","        transformer_dim_feedforward=dim_ff, transformer_dropout_rate=transformer_dropout,\n","        fusion_mlp_output_dim=fusion_mlp_out_dim,\n","        fusion_mlp_hidden_dim=fusion_mlp_hid_dim,\n","        dropout_rate_fusion_mlp=config.get('dropout_rate_fusion_mlp', 0.2),\n","        T_pred_horizon=T_PRED_HORIZON,\n","        dropout_rate_encoders=config.get('dropout_rate_encoders', 0.1),\n","        dropout_rate_rgcn=config.get('dropout_rate_gcn', 0.3),\n","        mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim', 64),\n","        dropout_rate_pred_head=config.get('dropout_rate_pred_head', 0.2),\n","        num_encoder_obs_steps=NUM_ENCODER_OBS_STEPS\n","    ).to(device)\n","    try: model_for_eval.load_state_dict(torch.load(model_save_path, map_location=device))\n","    except Exception as e: print(f\"无法加载最佳模型 ({e})，将使用训练循环结束时的模型。\"); model_for_eval = model\n","\n","    print(\"\\n评估最佳模型在训练集上...\"); best_model_train_loss_scaled, best_model_train_hourly_metrics, train_eval_duration = evaluate_epoch(model_for_eval, train_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Best Model on Train\")\n","    report_data['best_model_train_set_metrics_hourly'] = best_model_train_hourly_metrics; report_data['best_model_train_set_metrics_aggregated'] = calculate_aggregated_metrics_report(best_model_train_hourly_metrics, T_PRED_HORIZON)\n","    report_data['best_model_train_set_eval_time_seconds'] = train_eval_duration; _print_hourly_metrics_summary(\"最佳模型训练集\", best_model_train_hourly_metrics, T_PRED_HORIZON)\n","\n","    report_data['best_model_validation_set_metrics_hourly'] = best_val_hourly_metrics_original\n","    if best_val_hourly_metrics_original: report_data['best_model_validation_set_metrics_aggregated'] = calculate_aggregated_metrics_report(best_val_hourly_metrics_original, T_PRED_HORIZON)\n","    else:\n","        _, reeval_val_metrics, val_eval_duration = evaluate_epoch(model_for_eval, val_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Best Model on Val (Re-eval)\")\n","        report_data['best_model_validation_set_metrics_hourly'] = reeval_val_metrics\n","        report_data['best_model_validation_set_metrics_aggregated'] = calculate_aggregated_metrics_report(reeval_val_metrics, T_PRED_HORIZON)\n","        report_data['best_model_validation_set_eval_time_seconds'] = val_eval_duration\n","    _print_hourly_metrics_summary(\"最佳模型验证集\", report_data['best_model_validation_set_metrics_hourly'], T_PRED_HORIZON)\n","\n","\n","    print(\"\\n评估最佳模型在测试集上...\"); test_loss_scaled, test_hourly_metrics_original, test_inference_duration = evaluate_epoch(model_for_eval, test_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Test\")\n","    report_data['test_set_inference_time_seconds'] = test_inference_duration; report_data['best_model_test_set_metrics_hourly'] = test_hourly_metrics_original; report_data['best_model_test_set_metrics_aggregated'] = calculate_aggregated_metrics_report(test_hourly_metrics_original, T_PRED_HORIZON)\n","    print(\"\\n\" + \"=\"*20 + \" 最终测试集评估结果 (RGCN-Transformer) \" + \"=\"*20); print(f\"平均测试 Scaled MSE: {test_loss_scaled:.4f}\")\n","    _print_hourly_metrics_summary(\"测试集\", test_hourly_metrics_original, T_PRED_HORIZON)\n","\n","    agg_test = report_data['best_model_test_set_metrics_aggregated']\n","    print(f\"平均测试 MSE (Orig) : {agg_test.get('avg_mse', np.nan):.4f} (Std: {agg_test.get('std_mse', np.nan):.4f})\")\n","    print(f\"平均测试 R2 (Orig)  : {agg_test.get('avg_r2', np.nan):.4f} (Std: {agg_test.get('std_r2', np.nan):.4f})\")\n","    print(f\"平均测试 MAE (Orig) : {agg_test.get('avg_mae', np.nan):.4f} (Std: {agg_test.get('std_mae', np.nan):.4f})\")\n","    print(f\"平均测试 RMSE (Orig): {agg_test.get('avg_rmse', np.nan):.4f} (Std: {agg_test.get('std_rmse', np.nan):.4f})\")\n","    print(\"=\"*70)\n","\n","    report_file_path = Path(config['results_dir']) / f\"training_report_transformer_seed{seed}.json\"\n","    try:\n","        class NpEncoder(json.JSONEncoder):\n","            def default(self, obj):\n","                if isinstance(obj, np.integer): return int(obj)\n","                if isinstance(obj, np.floating): return float(obj)\n","                if isinstance(obj, np.ndarray): return obj.tolist()\n","                if isinstance(obj, torch.Tensor): return obj.tolist()\n","                if isinstance(obj, Path): return str(obj)\n","                return super(NpEncoder, self).default(obj)\n","        with open(report_file_path, 'w') as f: json.dump(report_data, f, indent=4, cls=NpEncoder)\n","        print(f\"训练报告已保存到: {report_file_path}\")\n","    except Exception as e: print(f\"保存训练报告失败: {e}\")\n","\n","    return model_for_eval, node_feat_mean, node_feat_std, target_mean, target_std\n","\n","def _print_hourly_metrics_summary(set_name, hourly_metrics, T_pred_horizon, indent=\"  \"):\n","    # ... (identical to previous version) ...\n","    if hourly_metrics is None:\n","        print(f\"{indent}{set_name} metrics not available.\")\n","        return\n","    print(f\"\\n{indent}每小时 {set_name} 指标 (Original Scale):\")\n","    header_printed = False\n","    for hour_idx in range(T_pred_horizon):\n","        metrics = hourly_metrics.get(hour_idx, {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0})\n","        if not header_printed:\n","            print(f\"{indent}  Hour | {'R2':>13s} | {'MSE':>14s} | {'MAE':>14s} | {'RMSE':>15s} | {'Count':>7s}\")\n","            header_printed = True\n","        print(f\"{indent}  {hour_idx:02d}   | {metrics.get('r2', np.nan):13.4f} | {metrics.get('mse', np.nan):14.4f} | {metrics.get('mae', np.nan):14.4f} | {metrics.get('rmse', np.nan):15.4f} | {metrics.get('count', 0):7d}\")\n","    aggregated = calculate_aggregated_metrics_report(hourly_metrics, T_pred_horizon)\n","    print(f\"{indent}  Aggregated Avg R2   : {aggregated.get('avg_r2', np.nan):.4f} (Std: {aggregated.get('std_r2', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MSE  : {aggregated.get('avg_mse', np.nan):.4f} (Std: {aggregated.get('std_mse', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MAE  : {aggregated.get('avg_mae', np.nan):.4f} (Std: {aggregated.get('std_mae', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg RMSE : {aggregated.get('avg_rmse', np.nan):.4f} (Std: {aggregated.get('std_rmse', np.nan):.4f})\")\n","\n","# ===========================================================\n","# 6. 主执行块\n","# ===========================================================\n","if __name__ == \"__main__\":\n","    gc.collect()\n","    if torch.cuda.is_available(): torch.cuda.empty_cache()\n","\n","    DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","    if not DRIVE_BASE_PATH.exists(): DRIVE_BASE_PATH.mkdir(parents=True, exist_ok=True)\n","\n","    DATA_SUBDIR = Path(\"Result/Sequential_12Hour_Data\")\n","    DATA_FILENAME = \"graph_seq_20230503_SeqH7to19_NpyH8fill0.0.pkl\"\n","    RESULTS_SUBDIR = Path(\"Result/Final_RGCNTransformer1\")\n","    RESULTS_SAVE_DIR = DRIVE_BASE_PATH / RESULTS_SUBDIR\n","    os.makedirs(RESULTS_SAVE_DIR, exist_ok=True)\n","    DATA_PATH = DRIVE_BASE_PATH / DATA_SUBDIR / DATA_FILENAME\n","\n","\n","    DATA_YEAR = 2023; DATA_MONTH = 5; DATA_DAY = 3\n","    START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES = 8; PREDICTION_HORIZON = 12\n","\n","    training_config = {\n","        'seed': 42, 'batch_size': 8, 'lr': 0.001, 'weight_decay': 1e-5,\n","        'max_epochs': 1000, 'scheduler_patience': 20, 'early_stopping_patience': 45,\n","        'T_pred_horizon': PREDICTION_HORIZON, 'results_dir': str(RESULTS_SAVE_DIR),\n","        'global_env_emb_dim': 16, 'time_emb_dim': 8,\n","        'gcn_hidden_dim': 128, 'gcn_output_dim': 128,\n","        'num_relations': 5,\n","        # GRU/LSTM keys will be used for Transformer d_model and dropout\n","        'gru_hidden_dim': 128, # Used for transformer_d_model\n","        'dropout_rate_gru': 0.2,  # Used for transformer_dropout_rate\n","        # New Transformer specific keys (can add more if needed, or use defaults in model)\n","        'transformer_nhead': 4,\n","        'transformer_num_encoder_layers': 2,\n","        'transformer_num_decoder_layers': 2,\n","        'transformer_dim_feedforward': 512, # e.g., 2 * d_model for this test\n","        'num_encoder_obs_steps': 1, # Define how many historical steps encoder sees\n","\n","        'mlp_prediction_hidden_dim': 64,\n","        'fusion_mlp_output_dim': 128, # Should match transformer_d_model\n","        'fusion_mlp_hidden_dim': 64,\n","        'dropout_rate_fusion_mlp': 0.2, 'dropout_rate_encoders': 0.1,\n","        'dropout_rate_gcn': 0.3,\n","        'dropout_rate_pred_head': 0.2,\n","        'use_amp': False, 'enable_profiler': False, 'num_workers': 0,\n","        'pin_memory': False, 'train_split_ratio': 0.7, 'val_split_ratio': 0.2,\n","        'h0_from_first_step': True # This config is less relevant for Transformer\n","    }\n","\n","    all_graph_sequences_loaded = None\n","    try:\n","        if not DATA_PATH.exists(): raise FileNotFoundError(f\"数据文件在指定路径未找到: {DATA_PATH}\")\n","        with open(DATA_PATH, \"rb\") as f: all_graph_sequences_loaded = pickle.load(f)\n","        if not all_graph_sequences_loaded or not isinstance(all_graph_sequences_loaded, list) or not all_graph_sequences_loaded[0] or not isinstance(all_graph_sequences_loaded[0], list):\n","            raise ValueError(\"加载的数据格式不正确。\")\n","\n","        num_encoder_obs_steps_from_config = training_config.get('num_encoder_obs_steps', 1)\n","        expected_len_per_sequence = num_encoder_obs_steps_from_config + training_config['T_pred_horizon']\n","\n","        processed_sequences = []\n","        for i, seq in enumerate(all_graph_sequences_loaded):\n","            if not isinstance(seq, list) or len(seq) != expected_len_per_sequence: continue\n","            valid_seq = True\n","            for step_idx, graph_step_data in enumerate(seq):\n","                if not isinstance(graph_step_data, Data) or not hasattr(graph_step_data, 'x') or graph_step_data.x is None or \\\n","                   not hasattr(graph_step_data, 'edge_index') or graph_step_data.edge_index is None or \\\n","                   not hasattr(graph_step_data, 'edge_attr') or graph_step_data.edge_attr is None or \\\n","                   graph_step_data.edge_attr.shape[1] < 5 or \\\n","                   not hasattr(graph_step_data, 'graph_global_env_features') or \\\n","                   (step_idx >= num_encoder_obs_steps_from_config and (not hasattr(graph_step_data, 'y') or graph_step_data.y is None)):\n","                    valid_seq = False; break\n","                if step_idx >= num_encoder_obs_steps_from_config and isinstance(graph_step_data.y, torch.Tensor) and graph_step_data.y.ndim == 1:\n","                    graph_step_data.y = graph_step_data.y.unsqueeze(1)\n","            if valid_seq: processed_sequences.append(seq)\n","        if not processed_sequences: raise ValueError(f\"数据处理后没有长度为 {expected_len_per_sequence} 的有效序列。\")\n","        all_graph_sequences = processed_sequences\n","        print(f\"成功加载并处理 {len(all_graph_sequences)} 个空间窗口的序列数据。\")\n","    except Exception as e: print(f\"加载或验证数据时发生错误: {e}\"); all_graph_sequences = None\n","\n","    if all_graph_sequences:\n","        base_datetime_for_timeline = dt_datetime(DATA_YEAR, DATA_MONTH, DATA_DAY, START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES)\n","        time_features_for_decoder_timeline = generate_time_features_for_sequence(\n","            base_datetime_for_timeline,\n","            training_config['T_pred_horizon']\n","        )\n","\n","        trained_model, final_node_mean, final_node_std, final_target_mean, final_target_std = main_training_rgcn_transformer_hourly_heads(\n","            all_graph_sequences, training_config, time_features_for_decoder_timeline\n","        )\n","        print(\"RGCN-Transformer 模型训练和评估完成!\")\n","    else:\n","        print(\"由于数据加载失败或数据为空，训练流程未启动。\")"]},{"cell_type":"markdown","metadata":{"id":"vmZK4T08LpKh"},"source":["##GCN+LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ESDb7Y6DRpaP","collapsed":true},"outputs":[],"source":["# ===========================================================\n","# 0. 环境 & 依赖\n","# ===========================================================\n","import os\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.loader import DataLoader\n","import pickle\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.metrics import r2_score\n","import gc\n","import math\n","from datetime import datetime as dt_datetime, timedelta\n","from pathlib import Path\n","import time\n","import json\n","import torchprofile\n","\n","torch.cuda.empty_cache()\n","torch.cuda.ipc_collect()\n","gc.collect()\n","\n","# ===========================================================\n","# 1. 特征生成 & 辅助模块 (No changes)\n","# ===========================================================\n","\n","def generate_time_features_for_sequence(base_dt_obj, num_steps):\n","    time_features_list = []\n","    for i in range(num_steps):\n","        current_dt = base_dt_obj + timedelta(hours=i)\n","        hour_norm = current_dt.hour / 23.0\n","        day_of_year_norm = current_dt.timetuple().tm_yday / (366.0 if current_dt.year % 4 == 0 and (current_dt.year % 100 != 0 or current_dt.year % 400 == 0) else 365.0)\n","        hour_sin = math.sin(2 * math.pi * hour_norm)\n","        hour_cos = math.cos(2 * math.pi * hour_norm)\n","        doy_sin = math.sin(2 * math.pi * day_of_year_norm)\n","        doy_cos = math.cos(2 * math.pi * day_of_year_norm)\n","        time_features_list.append(torch.tensor([hour_sin, hour_cos, doy_sin, doy_cos], dtype=torch.float32))\n","    return torch.stack(time_features_list)\n","\n","\n","class MLPEncoder(nn.Module):\n","    def __init__(self, in_dim, out_dim, hid_dim=None, dropout_rate=0.1):\n","        super().__init__()\n","        if hid_dim is None:\n","            hid_dim = max(min(in_dim, out_dim), (in_dim + out_dim) // 2)\n","            if hid_dim == 0 and out_dim > 0 : hid_dim = out_dim\n","            if hid_dim == 0 and in_dim > 0 : hid_dim = in_dim\n","            if hid_dim == 0 : hid_dim = 1\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(in_dim, hid_dim),\n","            nn.ReLU(),\n","            nn.LayerNorm(hid_dim),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hid_dim, out_dim)\n","        )\n","    def forward(self, x):\n","        return self.mlp(x)\n","\n","class GCNModule(nn.Module):\n","    def __init__(self, gcn_input_dim, gcn_hidden_dim, gcn_output_dim, dropout_rate=0.5):\n","        super().__init__()\n","        self.gcn_input_dim = gcn_input_dim\n","        self.gcn_hidden_dim = gcn_hidden_dim\n","        self.gcn_output_dim = gcn_output_dim\n","\n","        self.conv1 = GCNConv(gcn_input_dim, gcn_hidden_dim)\n","        self.bn1 = nn.BatchNorm1d(gcn_hidden_dim)\n","        self.prelu1 = nn.PReLU(gcn_hidden_dim)\n","\n","        self.conv2 = GCNConv(gcn_hidden_dim, gcn_hidden_dim)\n","        self.bn2 = nn.BatchNorm1d(gcn_hidden_dim)\n","        self.prelu2 = nn.PReLU(gcn_hidden_dim)\n","\n","        self.conv3 = GCNConv(gcn_hidden_dim, gcn_output_dim)\n","        self.bn3 = nn.BatchNorm1d(gcn_output_dim)\n","        self.prelu3 = nn.PReLU(gcn_output_dim)\n","\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x, edge_index, edge_weight=None):\n","        if x.size(0) == 0: return x\n","\n","        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n","        if x.size(0) > 1: x = self.bn1(x)\n","        x = self.prelu1(x)\n","        x = self.dropout(x)\n","\n","        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n","        if x.size(0) > 1: x = self.bn2(x)\n","        x = self.prelu2(x)\n","        x = self.dropout(x)\n","\n","        x = self.conv3(x, edge_index, edge_weight=edge_weight)\n","        if x.size(0) > 1: x = self.bn3(x)\n","        x = self.prelu3(x)\n","        return x\n","\n","# ===========================================================\n","# 2. GCN-LSTM 模型定义\n","# ===========================================================\n","class GCNLSTMModelWithHourlyHeads(nn.Module): # Renamed\n","    def __init__(self,\n","                 static_node_in_dim,\n","                 global_env_in_dim,\n","                 time_in_dim,\n","                 global_env_emb_dim,\n","                 time_emb_dim,\n","                 gcn_hidden_dim,\n","                 gcn_output_dim,\n","                 lstm_hidden_dim, # Changed from gru_hidden_dim\n","                 fusion_mlp_output_dim=None,\n","                 fusion_mlp_hidden_dim=None,\n","                 dropout_rate_fusion_mlp=0.1,\n","                 num_lstm_layers=1, # Changed from num_gru_layers\n","                 T_pred_horizon=12,\n","                 dropout_rate_encoders=0.1,\n","                 dropout_rate_gcn=0.3,\n","                 dropout_rate_lstm=0.2, # Changed from dropout_rate_gru\n","                 mlp_prediction_hidden_dim=64,\n","                 dropout_rate_pred_head=0.2\n","                ):\n","        super().__init__()\n","        self.T_pred_horizon = T_pred_horizon\n","        self.static_node_in_dim = static_node_in_dim\n","        self.global_env_in_dim = global_env_in_dim\n","        self.time_in_dim = time_in_dim\n","        self.gcn_output_dim = gcn_output_dim\n","        self.lstm_hidden_dim = lstm_hidden_dim # Store for consistency\n","\n","\n","        self.global_env_encoder = MLPEncoder(global_env_in_dim, global_env_emb_dim, dropout_rate=dropout_rate_encoders)\n","        self.time_encoder = MLPEncoder(time_in_dim, time_emb_dim, dropout_rate=dropout_rate_encoders)\n","        # This MLP processes GCN output to match LSTM hidden dim for h0 (and c0 will be zeros)\n","        self.h0_c0_from_gcn_encoder = MLPEncoder(gcn_output_dim, lstm_hidden_dim, dropout_rate=dropout_rate_encoders)\n","\n","        self.gcn_module_for_h0 = GCNModule(static_node_in_dim, gcn_hidden_dim, gcn_output_dim, dropout_rate_gcn)\n","        self.gcn_module_for_sequence = GCNModule(static_node_in_dim, gcn_hidden_dim, gcn_output_dim, dropout_rate_gcn)\n","\n","        concatenated_feature_dim = gcn_output_dim + global_env_emb_dim + time_emb_dim\n","        actual_fusion_mlp_output_dim = fusion_mlp_output_dim if fusion_mlp_output_dim is not None else concatenated_feature_dim\n","        self.fusion_mlp_input_dim = concatenated_feature_dim\n","\n","        self.fusion_mlp = MLPEncoder(\n","            in_dim=concatenated_feature_dim,\n","            out_dim=actual_fusion_mlp_output_dim,\n","            hid_dim=fusion_mlp_hidden_dim,\n","            dropout_rate=dropout_rate_fusion_mlp\n","        )\n","\n","        lstm_input_size_actual = actual_fusion_mlp_output_dim\n","        self.lstm_input_dim = lstm_input_size_actual # For profiling\n","\n","        self.lstm = nn.LSTM( # Changed from nn.GRU\n","            input_size=lstm_input_size_actual,\n","            hidden_size=lstm_hidden_dim,\n","            num_layers=num_lstm_layers,\n","            batch_first=True,\n","            dropout=dropout_rate_lstm if num_lstm_layers > 1 else 0.0\n","        )\n","\n","        self.hourly_prediction_heads = nn.ModuleList()\n","        for _ in range(T_pred_horizon):\n","            self.hourly_prediction_heads.append(\n","                nn.Sequential(\n","                    nn.Linear(lstm_hidden_dim, mlp_prediction_hidden_dim), # From LSTM output\n","                    nn.ReLU(),\n","                    nn.Dropout(dropout_rate_pred_head),\n","                    nn.Linear(mlp_prediction_hidden_dim, 1)\n","                )\n","            )\n","\n","        self.register_buffer('node_feat_mean', torch.zeros(static_node_in_dim))\n","        self.register_buffer('node_feat_std', torch.ones(static_node_in_dim))\n","\n","\n","    def forward(self, list_of_batched_timesteps: list, timeline_time_features: torch.Tensor, device: torch.device):\n","        pyg_batch_7am = list_of_batched_timesteps[0].to(device)\n","        normalized_x_7am = (pyg_batch_7am.x - self.node_feat_mean) / (self.node_feat_std + 1e-8)\n","\n","        edge_weight_7am = getattr(pyg_batch_7am, 'edge_weight', None)\n","        if edge_weight_7am is not None:\n","            edge_weight_7am = edge_weight_7am.float()\n","\n","        gcn_output_7am = self.gcn_module_for_h0(\n","            normalized_x_7am,\n","            pyg_batch_7am.edge_index,\n","            edge_weight=edge_weight_7am\n","        )\n","        h0_features_for_lstm_nodes = self.h0_c0_from_gcn_encoder(gcn_output_7am)\n","\n","        # LSTM expects a tuple (h_0, c_0)\n","        h0_for_lstm = h0_features_for_lstm_nodes.unsqueeze(0) # (1, N_nodes, lstm_hidden_dim)\n","        c0_for_lstm = torch.zeros_like(h0_for_lstm)          # (1, N_nodes, lstm_hidden_dim)\n","\n","        if self.lstm.num_layers > 1:\n","            h0_for_lstm = h0_for_lstm.repeat(self.lstm.num_layers, 1, 1)\n","            c0_for_lstm = c0_for_lstm.repeat(self.lstm.num_layers, 1, 1)\n","\n","        initial_hidden_state = (h0_for_lstm, c0_for_lstm)\n","\n","\n","        all_lstm_input_features_over_time = [] # Renamed\n","        for t_pred_idx in range(self.T_pred_horizon):\n","            pyg_batch_this_timestep = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","            normalized_x = (pyg_batch_this_timestep.x - self.node_feat_mean) / (self.node_feat_std + 1e-8)\n","\n","            edge_weight_this_timestep = getattr(pyg_batch_this_timestep, 'edge_weight', None)\n","            if edge_weight_this_timestep is not None:\n","                edge_weight_this_timestep = edge_weight_this_timestep.float()\n","\n","            gcn_output_nodes_t = self.gcn_module_for_sequence(\n","                normalized_x,\n","                pyg_batch_this_timestep.edge_index,\n","                edge_weight=edge_weight_this_timestep\n","            )\n","\n","            global_env_feat_t_unencoded = pyg_batch_this_timestep.graph_global_env_features\n","            expected_num_graphs_in_batch_t = pyg_batch_this_timestep.num_graphs\n","            expected_global_features_dim = self.global_env_encoder.mlp[0].in_features\n","            if not (global_env_feat_t_unencoded.shape == (expected_num_graphs_in_batch_t, expected_global_features_dim)):\n","                if global_env_feat_t_unencoded.ndim == 1 and \\\n","                   global_env_feat_t_unencoded.shape[0] == expected_num_graphs_in_batch_t * expected_global_features_dim:\n","                    global_env_feat_t_unencoded = global_env_feat_t_unencoded.view(expected_num_graphs_in_batch_t, expected_global_features_dim)\n","                elif global_env_feat_t_unencoded.numel() == expected_num_graphs_in_batch_t * expected_global_features_dim:\n","                    global_env_feat_t_unencoded = global_env_feat_t_unencoded.view(expected_num_graphs_in_batch_t, expected_global_features_dim)\n","                else:\n","                    print(f\"Warning: LSTM Input Time {t_pred_idx}: Correcting global_env_feat shape from {global_env_feat_t_unencoded.shape} to ({expected_num_graphs_in_batch_t}, {expected_global_features_dim}) with zeros due to mismatch.\")\n","                    global_env_feat_t_unencoded = torch.zeros(expected_num_graphs_in_batch_t, expected_global_features_dim, device=device)\n","            global_env_emb_t = self.global_env_encoder(global_env_feat_t_unencoded)\n","            global_env_emb_t_expanded = global_env_emb_t[pyg_batch_this_timestep.batch]\n","\n","            current_raw_time_feat_for_timestep_t = timeline_time_features[t_pred_idx, :].to(device)\n","            current_emb_time_feat_for_timestep_t = self.time_encoder(current_raw_time_feat_for_timestep_t)\n","            num_nodes_in_pyg_batch = pyg_batch_this_timestep.num_nodes\n","            time_emb_t_expanded_to_nodes = current_emb_time_feat_for_timestep_t.unsqueeze(0).expand(num_nodes_in_pyg_batch, -1)\n","\n","            concatenated_features_for_timestep_t = torch.cat([gcn_output_nodes_t, global_env_emb_t_expanded, time_emb_t_expanded_to_nodes], dim=-1)\n","            fused_features_for_timestep_t = self.fusion_mlp(concatenated_features_for_timestep_t)\n","            all_lstm_input_features_over_time.append(fused_features_for_timestep_t)\n","\n","        stacked_lstm_input_features = torch.stack(all_lstm_input_features_over_time, dim=1) # Renamed\n","\n","        if initial_hidden_state[0].shape[1] != stacked_lstm_input_features.shape[0]: # Check h0's N_nodes\n","            print(f\"CRITICAL WARNING: Node count mismatch for LSTM h0/c0 ({initial_hidden_state[0].shape[1]}) and LSTM input sequence ({stacked_lstm_input_features.shape[0]}).\")\n","            if initial_hidden_state[0].shape[1] > stacked_lstm_input_features.shape[0]:\n","                h0_adj = initial_hidden_state[0][:, :stacked_lstm_input_features.shape[0], :]\n","                c0_adj = initial_hidden_state[1][:, :stacked_lstm_input_features.shape[0], :]\n","                initial_hidden_state = (h0_adj, c0_adj)\n","\n","        lstm_out, _ = self.lstm(stacked_lstm_input_features, initial_hidden_state) # Changed from self.gru\n","\n","        all_hourly_final_predictions_scaled = []\n","        for t in range(self.T_pred_horizon):\n","            lstm_out_t = lstm_out[:, t, :] # Use lstm_out\n","            prediction_t_scaled = self.hourly_prediction_heads[t](lstm_out_t)\n","            all_hourly_final_predictions_scaled.append(prediction_t_scaled.squeeze(-1))\n","\n","        predictions_scaled = torch.stack(all_hourly_final_predictions_scaled, dim=1)\n","        return predictions_scaled\n","\n","# ===========================================================\n","# 3. 评估指标函数 (在原始尺度上计算指标) - NO CHANGES\n","# ===========================================================\n","# ... (mse_loss_masked, calculate_hourly_metrics are identical) ...\n","def mse_loss_masked(predictions_scaled, targets_scaled, mask):\n","    expanded_mask = mask.unsqueeze(1).expand_as(targets_scaled)\n","    valid_targets_mask = ~torch.isnan(targets_scaled)\n","    final_mask = expanded_mask & valid_targets_mask\n","    if final_mask.sum() == 0:\n","        return torch.tensor(0.0, device=predictions_scaled.device, requires_grad=True)\n","    loss = F.mse_loss(predictions_scaled[final_mask], targets_scaled[final_mask])\n","    return loss\n","\n","def calculate_hourly_metrics(predictions_scaled, targets_scaled, node_masks, target_mean, target_std):\n","    target_mean_cpu = target_mean.cpu()\n","    target_std_cpu = target_std.cpu()\n","    preds_unscaled = predictions_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","    targets_unscaled = targets_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","\n","    num_nodes_total, T_horizon = preds_unscaled.shape\n","    hourly_metrics_dict = {}\n","\n","    preds_np = preds_unscaled.numpy()\n","    targets_np = targets_unscaled.numpy()\n","    mask_np = node_masks.cpu().numpy()\n","\n","    for t in range(T_horizon):\n","        preds_t_all_nodes = preds_np[:, t]\n","        targets_t_all_nodes = targets_np[:, t]\n","        preds_t_on_loss_nodes = preds_t_all_nodes[mask_np]\n","        targets_t_on_loss_nodes = targets_t_all_nodes[mask_np]\n","        valid_target_data_mask_t = ~np.isnan(targets_t_on_loss_nodes)\n","        preds_t_final_valid = preds_t_on_loss_nodes[valid_target_data_mask_t]\n","        targets_t_final_valid = targets_t_on_loss_nodes[valid_target_data_mask_t]\n","\n","        if preds_t_final_valid.shape[0] < 2:\n","            hourly_metrics_dict[t] = {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count': 0}\n","            continue\n","        mse = np.mean((preds_t_final_valid - targets_t_final_valid)**2)\n","        mae = np.mean(np.abs(preds_t_final_valid - targets_t_final_valid))\n","        rmse = np.sqrt(mse)\n","        try:\n","            r2 = r2_score(targets_t_final_valid, preds_t_final_valid)\n","        except ValueError:\n","            r2 = np.nan\n","        hourly_metrics_dict[t] = {'mse': mse, 'mae': mae, 'rmse': rmse, 'r2': r2, 'count': preds_t_final_valid.shape[0]}\n","    return hourly_metrics_dict\n","\n","# ===========================================================\n","# 4. 训练与评估循环 (适配y归一化) - NO CHANGES\n","# ===========================================================\n","# ... (train_epoch, evaluate_epoch are identical) ...\n","def train_epoch(model, loader, optimizer, device, timeline_time_features,\n","                node_feat_mean, node_feat_std, target_mean, target_std):\n","    model.train()\n","    total_loss_scaled = 0\n","    num_sequences_processed = 0\n","    model.node_feat_mean = node_feat_mean.to(device)\n","    model.node_feat_std = node_feat_std.to(device)\n","\n","    target_mean_dev = target_mean.to(device)\n","    target_std_dev = target_std.to(device)\n","\n","    epoch_start_time = time.time()\n","    for list_of_batched_timesteps in loader:\n","        optimizer.zero_grad()\n","        predictions_batch_scaled = model(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","\n","        targets_list_for_loss_scaled = []\n","        first_predicted_timestep_batch = list_of_batched_timesteps[1].to(device)\n","        mask_for_loss = ~first_predicted_timestep_batch.building_mask\n","\n","        for t_pred_idx in range(model.T_pred_horizon):\n","            current_target_timestep_batch = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","            targets_t_nodes_original = current_target_timestep_batch.y.squeeze()\n","            targets_t_nodes_scaled = (targets_t_nodes_original - target_mean_dev) / (target_std_dev + 1e-8)\n","            targets_list_for_loss_scaled.append(targets_t_nodes_scaled)\n","\n","        targets_batch_scaled = torch.stack(targets_list_for_loss_scaled, dim=1)\n","        loss = mse_loss_masked(predictions_batch_scaled, targets_batch_scaled, mask_for_loss)\n","        num_sequences_in_this_super_batch = list_of_batched_timesteps[0].num_graphs\n","\n","        if not torch.isnan(loss) and not torch.isinf(loss) and loss.item() > 0 :\n","            loss.backward()\n","            optimizer.step()\n","            total_loss_scaled += loss.item() * num_sequences_in_this_super_batch\n","        num_sequences_processed += num_sequences_in_this_super_batch\n","\n","    epoch_duration = time.time() - epoch_start_time\n","    avg_loss = total_loss_scaled / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","    return avg_loss, epoch_duration\n","\n","def evaluate_epoch(model, loader, device, timeline_time_features,\n","                   node_feat_mean, node_feat_std, target_mean, target_std, epoch_type=\"Eval\"):\n","    model.eval()\n","    all_batch_predictions_scaled = []\n","    all_batch_targets_scaled = []\n","    all_batch_masks_for_metrics = []\n","    total_loss_scaled = 0\n","    num_sequences_processed = 0\n","    model.node_feat_mean = node_feat_mean.to(device)\n","    model.node_feat_std = node_feat_std.to(device)\n","\n","    target_mean_dev = target_mean.to(device)\n","    target_std_dev = target_std.to(device)\n","\n","    eval_start_time = time.time()\n","    with torch.no_grad():\n","        for list_of_batched_timesteps in loader:\n","            predictions_batch_scaled = model(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","            targets_list_original = []\n","            first_predicted_timestep_batch = list_of_batched_timesteps[1].to(device)\n","            mask_for_metrics = ~first_predicted_timestep_batch.building_mask\n","\n","            for t_pred_idx in range(model.T_pred_horizon):\n","                current_target_timestep_batch = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","                targets_t_nodes_original = current_target_timestep_batch.y.squeeze()\n","                targets_list_original.append(targets_t_nodes_original)\n","\n","            targets_batch_original = torch.stack(targets_list_original, dim=1)\n","            targets_batch_scaled_for_loss = (targets_batch_original - target_mean_dev) / (target_std_dev + 1e-8)\n","            loss = mse_loss_masked(predictions_batch_scaled, targets_batch_scaled_for_loss, mask_for_metrics)\n","            num_sequences_in_this_super_batch = list_of_batched_timesteps[0].num_graphs\n","\n","            if not torch.isnan(loss) and not torch.isinf(loss):\n","                total_loss_scaled += loss.item() * num_sequences_in_this_super_batch\n","            num_sequences_processed += num_sequences_in_this_super_batch\n","\n","            all_batch_predictions_scaled.append(predictions_batch_scaled.cpu())\n","            all_batch_targets_scaled.append(targets_batch_scaled_for_loss.cpu())\n","            all_batch_masks_for_metrics.append(mask_for_metrics.cpu())\n","\n","    eval_duration = time.time() - eval_start_time\n","    avg_loss_scaled = total_loss_scaled / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","\n","    if not all_batch_predictions_scaled:\n","        empty_metrics = {t: {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0} for t in range(model.T_pred_horizon)}\n","        return avg_loss_scaled, empty_metrics, eval_duration\n","\n","    final_predictions_scaled = torch.cat(all_batch_predictions_scaled, dim=0)\n","    final_targets_scaled = torch.cat(all_batch_targets_scaled, dim=0)\n","    final_masks_for_metrics = torch.cat(all_batch_masks_for_metrics, dim=0)\n","\n","    hourly_metrics_original_scale = calculate_hourly_metrics(final_predictions_scaled, final_targets_scaled,\n","                                                             final_masks_for_metrics, target_mean.cpu(), target_std.cpu())\n","    return avg_loss_scaled, hourly_metrics_original_scale, eval_duration\n","\n","# ===========================================================\n","# 5. 主训练流程 (GCN-LSTM)\n","# ===========================================================\n","\n","def calculate_aggregated_metrics_report(hourly_metrics_dict, T_pred_horizon):\n","    # ... (identical to previous version) ...\n","    metrics_to_aggregate = ['r2', 'mse', 'mae', 'rmse']\n","    aggregated_report = {}\n","    for metric_name in metrics_to_aggregate:\n","        values = [hourly_metrics_dict[t][metric_name] for t in range(T_pred_horizon) if t in hourly_metrics_dict and not np.isnan(hourly_metrics_dict[t][metric_name])]\n","        if values:\n","            aggregated_report[f'avg_{metric_name}'] = np.mean(values)\n","            aggregated_report[f'std_{metric_name}'] = np.std(values)\n","        else:\n","            aggregated_report[f'avg_{metric_name}'] = np.nan\n","            aggregated_report[f'std_{metric_name}'] = np.nan\n","    return aggregated_report\n","\n","def main_training_gcn_lstm_hourly_heads( # Renamed\n","    all_sequences_data: list,\n","    config: dict,\n","    time_features_for_dataset: torch.Tensor\n","):\n","    train_start_time = time.time()\n","    report_data = {'config': config}\n","\n","    seed = config.get('seed', 42)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"使用设备: {device}\")\n","    report_data['device'] = str(device)\n","\n","    T_PRED_HORIZON = config.get('T_pred_horizon', 12)\n","    expected_input_len = T_PRED_HORIZON + 1\n","    valid_sequences_data = [seq for seq in all_sequences_data if isinstance(seq, list) and len(seq) == expected_input_len]\n","    if len(valid_sequences_data) != len(all_sequences_data):\n","        print(f\"警告: 从 {len(all_sequences_data)} 个序列中筛选出 {len(valid_sequences_data)} 个长度为 {expected_input_len} 的有效序列。\")\n","    if not valid_sequences_data:\n","        raise ValueError(f\"没有找到长度为 {expected_input_len} 的有效序列数据。\")\n","    all_sequences_data = valid_sequences_data\n","\n","    # Dataset split\n","    num_total_sequences = len(all_sequences_data)\n","    indices = np.random.permutation(num_total_sequences)\n","    train_split_ratio = config.get('train_split_ratio', 0.7)\n","    val_split_ratio = config.get('val_split_ratio', 0.2)\n","    train_size = int(train_split_ratio * num_total_sequences)\n","    val_size = int(val_split_ratio * num_total_sequences)\n","    train_indices = indices[:train_size]\n","    val_indices = indices[train_size : train_size + val_size]\n","    test_indices = indices[train_size + val_size :]\n","    train_dataset = [all_sequences_data[i] for i in train_indices]\n","    val_dataset   = [all_sequences_data[i] for i in val_indices]\n","    test_dataset  = [all_sequences_data[i] for i in test_indices]\n","    report_data['dataset_split'] = {'total_sequences': num_total_sequences, 'train_size': len(train_dataset), 'val_size': len(val_dataset), 'test_size': len(test_dataset)}\n","\n","    # Scaler calculation\n","    all_train_node_features_list = []\n","    all_train_target_values_list_for_scaling = []\n","    for seq in train_dataset:\n","        for i_step, graph_data in enumerate(seq):\n","            if hasattr(graph_data, 'x') and graph_data.x is not None:\n","                all_train_node_features_list.append(graph_data.x)\n","            if i_step > 0 and hasattr(graph_data, 'y') and graph_data.y is not None:\n","                y_original = graph_data.y.squeeze(); current_mask_for_loss = ~graph_data.building_mask\n","                valid_target_indices = current_mask_for_loss & ~torch.isnan(y_original)\n","                if valid_target_indices.sum() > 0:\n","                    all_train_target_values_list_for_scaling.append(y_original[valid_target_indices])\n","\n","    if not all_train_node_features_list: raise ValueError(\"训练数据中未找到节点特征 'x'，无法计算scaler！\")\n","    all_train_node_features_tensor = torch.cat(all_train_node_features_list, dim=0)\n","    node_feat_mean = torch.mean(all_train_node_features_tensor, dim=0); node_feat_std = torch.std(all_train_node_features_tensor, dim=0)\n","    node_feat_std[node_feat_std < 1e-8] = 1.0\n","    scaler_path_x = Path(config['results_dir']) / \"node_feature_scaler_gcn_lstm.pth\" # Renamed\n","    torch.save({'mean': node_feat_mean, 'std': node_feat_std}, scaler_path_x); print(f\"节点特征x scaler已保存到: {scaler_path_x}\")\n","\n","    if not all_train_target_values_list_for_scaling:\n","        target_mean = torch.tensor(0.0); target_std = torch.tensor(1.0)\n","    else:\n","        all_train_target_values_tensor = torch.cat(all_train_target_values_list_for_scaling, dim=0)\n","        target_mean = torch.mean(all_train_target_values_tensor.float()); target_std = torch.std(all_train_target_values_tensor.float())\n","        if target_std < 1e-8: target_std = torch.tensor(1.0)\n","    target_scaler_path = Path(config['results_dir']) / \"target_scaler_gcn_lstm.pth\" # Renamed\n","    torch.save({'mean': target_mean, 'std': target_std}, target_scaler_path); print(f\"目标值y scaler已保存到: {target_scaler_path}\")\n","\n","    # DataLoaders\n","    batch_size = config.get('batch_size', 8); num_workers = config.get('num_workers', 0)\n","    pin_memory_flag = config.get('pin_memory', False) and device.type == 'cuda'\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=num_workers, pin_memory=pin_memory_flag)\n","    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n","    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n","\n","    # Model Initialization\n","    sample_graph_7am_for_dims = all_sequences_data[0][0]\n","    static_node_in_dim = sample_graph_7am_for_dims.x.shape[1]\n","    global_env_in_dim = sample_graph_7am_for_dims.graph_global_env_features.shape[0] if sample_graph_7am_for_dims.graph_global_env_features.ndim == 1 else sample_graph_7am_for_dims.graph_global_env_features.shape[1]\n","    time_in_dim = time_features_for_dataset.shape[1]\n","\n","    model = GCNLSTMModelWithHourlyHeads( # Use GCNLSTMModel\n","        static_node_in_dim=static_node_in_dim, global_env_in_dim=global_env_in_dim, time_in_dim=time_in_dim,\n","        global_env_emb_dim=config.get('global_env_emb_dim', 16), time_emb_dim=config.get('time_emb_dim', 8),\n","        gcn_hidden_dim=config.get('gcn_hidden_dim', 128),\n","        gcn_output_dim=config.get('gcn_output_dim', 128),\n","        lstm_hidden_dim=config.get('gru_hidden_dim', 128), # Using gru_hidden_dim key for LSTM hidden\n","        fusion_mlp_output_dim=config.get('fusion_mlp_output_dim', 128),\n","        fusion_mlp_hidden_dim=config.get('fusion_mlp_hidden_dim', 64),\n","        dropout_rate_fusion_mlp=config.get('dropout_rate_fusion_mlp', 0.2),\n","        num_lstm_layers=config.get('num_gru_layers', 1), # Using num_gru_layers key for LSTM layers\n","        T_pred_horizon=T_PRED_HORIZON,\n","        dropout_rate_encoders=config.get('dropout_rate_encoders', 0.1),\n","        dropout_rate_gcn=config.get('dropout_rate_gcn', 0.3),\n","        dropout_rate_lstm=config.get('dropout_rate_gru', 0.2), # Using dropout_rate_gru key for LSTM dropout\n","        mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim', 64),\n","        dropout_rate_pred_head=config.get('dropout_rate_pred_head', 0.2)\n","    ).to(device)\n","\n","    model.node_feat_mean = node_feat_mean.to(device); model.node_feat_std = node_feat_std.to(device)\n","    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"模型总参数量 (GCN-LSTM): {total_params:,}\"); report_data['model_total_parameters'] = total_params\n","\n","    # ===== Component FLOPS Calculation =====\n","    print(\"\\nCalculating MACs for model components (approximate FLOPS):\")\n","    report_data['component_gmacs'] = {}\n","    dummy_nodes_component = 2500\n","    dummy_edges_component = 60000\n","    dummy_batch_global_comp = 1\n","    model.eval()\n","\n","    # 1. GCNModule\n","    try:\n","        gcn_module_to_profile = model.gcn_module_for_h0\n","        dummy_x_gcn = torch.randn(dummy_nodes_component, gcn_module_to_profile.gcn_input_dim, device=device)\n","        dummy_ei_gcn = torch.randint(0, dummy_nodes_component, (2, dummy_edges_component), device=device)\n","        dummy_ew_gcn = torch.rand(dummy_edges_component, device=device)\n","        macs_gcn = torchprofile.profile_macs(gcn_module_to_profile, args=(dummy_x_gcn, dummy_ei_gcn, dummy_ew_gcn))\n","        report_data['component_gmacs']['gcn_module'] = macs_gcn / 1e9\n","        print(f\"  GCNModule GMACs: {macs_gcn / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling GCNModule: {e}\"); report_data['component_gmacs']['gcn_module'] = \"Error\"\n","\n","    # 2. LSTM Layer (Manual MAC Calculation)\n","    print(f\"  Manually Calculating MACs for LSTM Layer:\")\n","    try:\n","        lstm_layer = model.lstm # Changed from model.gru\n","        N_nodes = dummy_nodes_component\n","        L_seq = T_PRED_HORIZON\n","        H_in = lstm_layer.input_size\n","        H_hidden = lstm_layer.hidden_size\n","        num_layers = lstm_layer.num_layers\n","\n","        macs_lstm_manual = 0\n","        # For a single layer LSTM: MACs ≈ N * L * 4 * (H_in * H_hidden + H_hidden^2)\n","        macs_lstm_manual = N_nodes * L_seq * 4 * (H_in * H_hidden + H_hidden * H_hidden) # For the first layer\n","        if num_layers > 1:\n","            # Subsequent (num_layers - 1) layers: N * L * 4 * (H_hidden * H_hidden + H_hidden^2)\n","            macs_lstm_manual += N_nodes * L_seq * (num_layers - 1) * 4 * (H_hidden * H_hidden + H_hidden * H_hidden)\n","\n","        gmacs_lstm_manual = macs_lstm_manual / 1e9\n","        report_data['component_gmacs']['lstm_layer'] = gmacs_lstm_manual # Renamed key\n","        report_data['component_gmacs']['lstm_layer_profiling_notes'] = \"Manually calculated based on formula.\"\n","        print(f\"  LSTM Parameters: input_size={H_in}, hidden_size={H_hidden}, num_layers={num_layers}\")\n","        print(f\"  Used for calculation: N_nodes={N_nodes}, L_seq={L_seq}\")\n","        print(f\"  LSTM Layer GMACs (Manual): {gmacs_lstm_manual:.4f} (for sequence length {L_seq})\")\n","\n","    except Exception as e:\n","        print(f\"  Error manually calculating LSTM Layer MACs: {e}\")\n","        report_data['component_gmacs']['lstm_layer'] = \"Error\" # Renamed key\n","        report_data['component_gmacs']['lstm_layer_profiling_notes'] = f\"Error during manual calculation: {str(e)}\"\n","\n","    # 3. Fusion MLP (MLPEncoder)\n","    try:\n","        fusion_mlp_to_profile = model.fusion_mlp\n","        dummy_input_fusion_mlp = torch.randn(dummy_nodes_component, model.fusion_mlp_input_dim, device=device)\n","        macs_fusion_mlp = torchprofile.profile_macs(fusion_mlp_to_profile, args=(dummy_input_fusion_mlp,))\n","        report_data['component_gmacs']['fusion_mlp'] = macs_fusion_mlp / 1e9\n","        print(f\"  Fusion MLP GMACs: {macs_fusion_mlp / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Fusion MLP: {e}\"); report_data['component_gmacs']['fusion_mlp'] = \"Error\"\n","\n","    # 4. Prediction Head (one MLP from ModuleList)\n","    try:\n","        pred_head_to_profile = model.hourly_prediction_heads[0]\n","        dummy_input_pred_head = torch.randn(dummy_nodes_component, model.lstm_hidden_dim, device=device) # Input is lstm_hidden_dim\n","        macs_pred_head = torchprofile.profile_macs(pred_head_to_profile, args=(dummy_input_pred_head,))\n","        report_data['component_gmacs']['prediction_head_mlp'] = macs_pred_head / 1e9\n","        print(f\"  Prediction Head MLP (single hour) GMACs: {macs_pred_head / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Prediction Head: {e}\"); report_data['component_gmacs']['prediction_head_mlp'] = \"Error\"\n","\n","    # 5. Global Environment Encoder (MLPEncoder)\n","    try:\n","        encoder_to_profile = model.global_env_encoder\n","        dummy_input_encoder = torch.randn(dummy_batch_global_comp, model.global_env_in_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['global_env_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  Global Env Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Global Env Encoder: {e}\"); report_data['component_gmacs']['global_env_encoder_mlp'] = \"Error\"\n","\n","    # 6. Time Encoder (MLPEncoder)\n","    try:\n","        encoder_to_profile = model.time_encoder\n","        dummy_input_encoder = torch.randn(dummy_batch_global_comp, model.time_in_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['time_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  Time Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Time Encoder: {e}\"); report_data['component_gmacs']['time_encoder_mlp'] = \"Error\"\n","\n","    # 7. H0/C0 from GCN Encoder (MLPEncoder)\n","    try:\n","        encoder_to_profile = model.h0_c0_from_gcn_encoder # Renamed\n","        dummy_input_encoder = torch.randn(dummy_nodes_component, model.gcn_output_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['h0_c0_from_gcn_encoder_mlp'] = macs_encoder / 1e9 # Renamed key\n","        print(f\"  H0/C0 from GCN Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\") # Note: This MLP now maps to lstm_hidden_dim\n","    except Exception as e:\n","        print(f\"  Error profiling H0/C0 from GCN Encoder: {e}\"); report_data['component_gmacs']['h0_c0_from_gcn_encoder_mlp'] = \"Error\"\n","\n","    model.train()\n","    # ===== End Component FLOPS Calculation =====\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config.get('lr', 0.001), weight_decay=config.get('weight_decay', 1e-5))\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=config.get('scheduler_patience', 20), verbose=True)\n","\n","    best_val_loss_scaled = float('inf'); best_val_hourly_metrics_original = None; best_epoch = 0\n","    patience_counter = 0; max_epochs = config.get('max_epochs', 300); early_stopping_patience = config.get('early_stopping_patience', 45)\n","    model_save_path = Path(config['results_dir']) / f\"best_gcnlstm_hourly_heads_model_seed{seed}.pth\" # Renamed\n","    timeline_time_features_on_device = time_features_for_dataset.to(device)\n","    target_mean_cpu = target_mean.cpu(); target_std_cpu = target_std.cpu()\n","    target_mean_on_device = target_mean.to(device); target_std_on_device = target_std.to(device)\n","    epoch_times = []\n","\n","    for epoch in range(1, max_epochs + 1):\n","        train_loss_scaled, epoch_duration = train_epoch(model, train_loader, optimizer, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_on_device, target_std_on_device)\n","        epoch_times.append(epoch_duration)\n","        val_loss_scaled, val_hourly_metrics_original, _ = evaluate_epoch(model, val_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Validation\")\n","        scheduler.step(val_loss_scaled)\n","        print(f\"Epoch {epoch:03d} | Train Scaled MSE: {train_loss_scaled:.4f} | Val Scaled MSE: {val_loss_scaled:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f} | Epoch Time: {epoch_duration:.2f}s\")\n","        _print_hourly_metrics_summary(\"Val\", val_hourly_metrics_original, T_PRED_HORIZON, indent=\"                     \")\n","        if val_loss_scaled < best_val_loss_scaled:\n","            best_val_loss_scaled = val_loss_scaled; best_val_hourly_metrics_original = val_hourly_metrics_original; best_epoch = epoch\n","            patience_counter = 0; torch.save(model.state_dict(), model_save_path)\n","            print(f\"                     ---> Best model saved (Epoch: {epoch}, Val Scaled MSE: {best_val_loss_scaled:.4f})\")\n","        else:\n","            patience_counter += 1\n","        if patience_counter >= early_stopping_patience:\n","            print(f\"Early stopping at epoch {epoch} due to no improvement.\"); break\n","\n","    total_training_duration = time.time() - train_start_time\n","    report_data['total_training_time_seconds'] = total_training_duration\n","    report_data['average_epoch_time_seconds'] = np.mean(epoch_times) if epoch_times else np.nan\n","    report_data['num_epochs_trained'] = epoch; report_data['best_validation_epoch'] = best_epoch\n","    report_data['best_validation_scaled_mse'] = best_val_loss_scaled\n","\n","    model_for_eval = GCNLSTMModelWithHourlyHeads( # Use GCNLSTMModel\n","        static_node_in_dim=static_node_in_dim, global_env_in_dim=global_env_in_dim, time_in_dim=time_in_dim,\n","        global_env_emb_dim=config.get('global_env_emb_dim', 16), time_emb_dim=config.get('time_emb_dim', 8),\n","        gcn_hidden_dim=config.get('gcn_hidden_dim', 128),\n","        gcn_output_dim=config.get('gcn_output_dim', 128),\n","        lstm_hidden_dim=config.get('gru_hidden_dim', 128),\n","        fusion_mlp_output_dim=config.get('fusion_mlp_output_dim', 128),\n","        fusion_mlp_hidden_dim=config.get('fusion_mlp_hidden_dim', 64),\n","        dropout_rate_fusion_mlp=config.get('dropout_rate_fusion_mlp', 0.2),\n","        num_lstm_layers=config.get('num_gru_layers', 1), T_pred_horizon=T_PRED_HORIZON,\n","        dropout_rate_encoders=config.get('dropout_rate_encoders', 0.1),\n","        dropout_rate_gcn=config.get('dropout_rate_gcn', 0.3),\n","        dropout_rate_lstm=config.get('dropout_rate_gru', 0.2),\n","        mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim', 64),\n","        dropout_rate_pred_head=config.get('dropout_rate_pred_head', 0.2)\n","    ).to(device)\n","    try: model_for_eval.load_state_dict(torch.load(model_save_path, map_location=device))\n","    except Exception as e: print(f\"无法加载最佳模型 ({e})，将使用训练循环结束时的模型。\"); model_for_eval = model\n","\n","    print(\"\\n评估最佳模型在训练集上...\"); best_model_train_loss_scaled, best_model_train_hourly_metrics, train_eval_duration = evaluate_epoch(model_for_eval, train_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Best Model on Train\")\n","    report_data['best_model_train_set_metrics_hourly'] = best_model_train_hourly_metrics; report_data['best_model_train_set_metrics_aggregated'] = calculate_aggregated_metrics_report(best_model_train_hourly_metrics, T_PRED_HORIZON)\n","    report_data['best_model_train_set_eval_time_seconds'] = train_eval_duration; _print_hourly_metrics_summary(\"最佳模型训练集\", best_model_train_hourly_metrics, T_PRED_HORIZON)\n","\n","    report_data['best_model_validation_set_metrics_hourly'] = best_val_hourly_metrics_original\n","    if best_val_hourly_metrics_original: report_data['best_model_validation_set_metrics_aggregated'] = calculate_aggregated_metrics_report(best_val_hourly_metrics_original, T_PRED_HORIZON)\n","    else:\n","        _, reeval_val_metrics, val_eval_duration = evaluate_epoch(model_for_eval, val_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Best Model on Val (Re-eval)\")\n","        report_data['best_model_validation_set_metrics_hourly'] = reeval_val_metrics\n","        report_data['best_model_validation_set_metrics_aggregated'] = calculate_aggregated_metrics_report(reeval_val_metrics, T_PRED_HORIZON)\n","        report_data['best_model_validation_set_eval_time_seconds'] = val_eval_duration\n","    _print_hourly_metrics_summary(\"最佳模型验证集\", report_data['best_model_validation_set_metrics_hourly'], T_PRED_HORIZON)\n","\n","\n","    print(\"\\n评估最佳模型在测试集上...\"); test_loss_scaled, test_hourly_metrics_original, test_inference_duration = evaluate_epoch(model_for_eval, test_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Test\")\n","    report_data['test_set_inference_time_seconds'] = test_inference_duration; report_data['best_model_test_set_metrics_hourly'] = test_hourly_metrics_original; report_data['best_model_test_set_metrics_aggregated'] = calculate_aggregated_metrics_report(test_hourly_metrics_original, T_PRED_HORIZON)\n","    print(\"\\n\" + \"=\"*20 + \" 最终测试集评估结果 (GCN-LSTM) \" + \"=\"*20); print(f\"平均测试 Scaled MSE: {test_loss_scaled:.4f}\") # Renamed\n","    _print_hourly_metrics_summary(\"测试集\", test_hourly_metrics_original, T_PRED_HORIZON)\n","\n","    agg_test = report_data['best_model_test_set_metrics_aggregated']\n","    print(f\"平均测试 MSE (Orig) : {agg_test.get('avg_mse', np.nan):.4f} (Std: {agg_test.get('std_mse', np.nan):.4f})\")\n","    print(f\"平均测试 R2 (Orig)  : {agg_test.get('avg_r2', np.nan):.4f} (Std: {agg_test.get('std_r2', np.nan):.4f})\")\n","    print(f\"平均测试 MAE (Orig) : {agg_test.get('avg_mae', np.nan):.4f} (Std: {agg_test.get('std_mae', np.nan):.4f})\")\n","    print(f\"平均测试 RMSE (Orig): {agg_test.get('avg_rmse', np.nan):.4f} (Std: {agg_test.get('std_rmse', np.nan):.4f})\")\n","    print(\"=\"*70)\n","\n","    report_file_path = Path(config['results_dir']) / f\"training_report_gcn_lstm_seed{seed}.json\" # Renamed\n","    try:\n","        class NpEncoder(json.JSONEncoder):\n","            def default(self, obj):\n","                if isinstance(obj, np.integer): return int(obj)\n","                if isinstance(obj, np.floating): return float(obj)\n","                if isinstance(obj, np.ndarray): return obj.tolist()\n","                if isinstance(obj, torch.Tensor): return obj.tolist()\n","                if isinstance(obj, Path): return str(obj)\n","                return super(NpEncoder, self).default(obj)\n","        with open(report_file_path, 'w') as f: json.dump(report_data, f, indent=4, cls=NpEncoder)\n","        print(f\"训练报告已保存到: {report_file_path}\")\n","    except Exception as e: print(f\"保存训练报告失败: {e}\")\n","\n","    return model_for_eval, node_feat_mean, node_feat_std, target_mean, target_std\n","\n","def _print_hourly_metrics_summary(set_name, hourly_metrics, T_pred_horizon, indent=\"  \"):\n","    # ... (identical to previous version) ...\n","    if hourly_metrics is None:\n","        print(f\"{indent}{set_name} metrics not available.\")\n","        return\n","    print(f\"\\n{indent}每小时 {set_name} 指标 (Original Scale):\")\n","    header_printed = False\n","    for hour_idx in range(T_pred_horizon):\n","        metrics = hourly_metrics.get(hour_idx, {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0})\n","        if not header_printed:\n","            print(f\"{indent}  Hour | {'R2':>13s} | {'MSE':>14s} | {'MAE':>14s} | {'RMSE':>15s} | {'Count':>7s}\")\n","            header_printed = True\n","        print(f\"{indent}  {hour_idx:02d}   | {metrics.get('r2', np.nan):13.4f} | {metrics.get('mse', np.nan):14.4f} | {metrics.get('mae', np.nan):14.4f} | {metrics.get('rmse', np.nan):15.4f} | {metrics.get('count', 0):7d}\")\n","    aggregated = calculate_aggregated_metrics_report(hourly_metrics, T_pred_horizon)\n","    print(f\"{indent}  Aggregated Avg R2   : {aggregated.get('avg_r2', np.nan):.4f} (Std: {aggregated.get('std_r2', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MSE  : {aggregated.get('avg_mse', np.nan):.4f} (Std: {aggregated.get('std_mse', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MAE  : {aggregated.get('avg_mae', np.nan):.4f} (Std: {aggregated.get('std_mae', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg RMSE : {aggregated.get('avg_rmse', np.nan):.4f} (Std: {aggregated.get('std_rmse', np.nan):.4f})\")\n","\n","# ===========================================================\n","# 6. 主执行块\n","# ===========================================================\n","if __name__ == \"__main__\":\n","    gc.collect()\n","    if torch.cuda.is_available(): torch.cuda.empty_cache()\n","\n","    DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","    if not DRIVE_BASE_PATH.exists(): DRIVE_BASE_PATH.mkdir(parents=True, exist_ok=True)\n","\n","    DATA_SUBDIR = Path(\"Result/Sequential_12Hour_Data\")\n","    DATA_FILENAME = \"graph_seq_20230503_SeqH7to19_NpyH8fill0.0.pkl\"\n","    RESULTS_SUBDIR = Path(\"Result/Final_GCNLSTM1\") # Renamed output subdir\n","    RESULTS_SAVE_DIR = DRIVE_BASE_PATH / RESULTS_SUBDIR\n","    os.makedirs(RESULTS_SAVE_DIR, exist_ok=True)\n","    DATA_PATH = DRIVE_BASE_PATH / DATA_SUBDIR / DATA_FILENAME\n","\n","\n","    DATA_YEAR = 2023; DATA_MONTH = 5; DATA_DAY = 3\n","    START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES = 8; PREDICTION_HORIZON = 12\n","\n","    training_config = {\n","        'seed': 42, 'batch_size': 8, 'lr': 0.001, 'weight_decay': 1e-5,\n","        'max_epochs': 1000, 'scheduler_patience': 20, 'early_stopping_patience': 45,\n","        'T_pred_horizon': PREDICTION_HORIZON, 'results_dir': str(RESULTS_SAVE_DIR),\n","        'global_env_emb_dim': 16, 'time_emb_dim': 8,\n","        'gcn_hidden_dim': 128,\n","        'gcn_output_dim': 128,\n","        # 'num_relations' and 'gine_edge_dim' are not needed for GCN\n","        'gru_hidden_dim': 128, # This will be lstm_hidden_dim\n","        'num_gru_layers': 1,  # This will be num_lstm_layers\n","        'mlp_prediction_hidden_dim': 64,\n","        'fusion_mlp_output_dim': 128,\n","        'fusion_mlp_hidden_dim': 64,\n","        'dropout_rate_fusion_mlp': 0.2, 'dropout_rate_encoders': 0.1,\n","        'dropout_rate_gcn': 0.3,\n","        'dropout_rate_gru': 0.2,  # This will be dropout_rate_lstm\n","        'dropout_rate_pred_head': 0.2,\n","        'use_amp': False, 'enable_profiler': False, 'num_workers': 0,\n","        'pin_memory': False, 'train_split_ratio': 0.7, 'val_split_ratio': 0.2,\n","        'h0_from_first_step': True\n","    }\n","\n","    all_graph_sequences_loaded = None\n","    try:\n","        if not DATA_PATH.exists(): raise FileNotFoundError(f\"数据文件在指定路径未找到: {DATA_PATH}\")\n","        with open(DATA_PATH, \"rb\") as f: all_graph_sequences_loaded = pickle.load(f)\n","        if not all_graph_sequences_loaded or not isinstance(all_graph_sequences_loaded, list) or not all_graph_sequences_loaded[0] or not isinstance(all_graph_sequences_loaded[0], list):\n","            raise ValueError(\"加载的数据格式不正确。\")\n","\n","        expected_len_per_sequence = training_config['T_pred_horizon'] + 1\n","\n","        processed_sequences = []\n","        for i, seq in enumerate(all_graph_sequences_loaded):\n","            if not isinstance(seq, list) or len(seq) != expected_len_per_sequence: continue\n","            valid_seq = True\n","            for step_idx, graph_step_data in enumerate(seq):\n","                if not isinstance(graph_step_data, Data) or not hasattr(graph_step_data, 'x') or graph_step_data.x is None or \\\n","                   not hasattr(graph_step_data, 'edge_index') or graph_step_data.edge_index is None or \\\n","                   not hasattr(graph_step_data, 'graph_global_env_features') or \\\n","                   (step_idx > 0 and (not hasattr(graph_step_data, 'y') or graph_step_data.y is None)):\n","                    valid_seq = False; break\n","                if step_idx > 0 and isinstance(graph_step_data.y, torch.Tensor) and graph_step_data.y.ndim == 1:\n","                    graph_step_data.y = graph_step_data.y.unsqueeze(1)\n","            if valid_seq: processed_sequences.append(seq)\n","        if not processed_sequences: raise ValueError(f\"数据处理后没有长度为 {expected_len_per_sequence} 的有效序列。\")\n","        all_graph_sequences = processed_sequences\n","        print(f\"成功加载并处理 {len(all_graph_sequences)} 个空间窗口的序列数据。\")\n","    except Exception as e: print(f\"加载或验证数据时发生错误: {e}\"); all_graph_sequences = None\n","\n","    if all_graph_sequences:\n","        base_datetime_for_timeline = dt_datetime(DATA_YEAR, DATA_MONTH, DATA_DAY, START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES)\n","        time_features_for_dataset_timeline = generate_time_features_for_sequence(\n","            base_datetime_for_timeline,\n","            training_config['T_pred_horizon']\n","        )\n","\n","        trained_model, final_node_mean, final_node_std, final_target_mean, final_target_std = main_training_gcn_lstm_hourly_heads( # Renamed call\n","            all_graph_sequences, training_config, time_features_for_dataset_timeline\n","        )\n","        print(\"GCN-LSTM 模型训练和评估完成!\")\n","    else:\n","        print(\"由于数据加载失败或数据为空，训练流程未启动。\")"]},{"cell_type":"markdown","metadata":{"id":"lafh8hfCBpkG"},"source":["##GINE+LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"89Wccy_0Tr9n"},"outputs":[],"source":["# ===========================================================\n","# 0. 环境 & 依赖\n","# ===========================================================\n","import os\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch_geometric.nn import GINEConv # Using GINEConv\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.loader import DataLoader\n","import pickle\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.metrics import r2_score\n","import gc\n","import math\n","from datetime import datetime as dt_datetime, timedelta\n","from pathlib import Path\n","import time\n","import json\n","import torchprofile\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","torch.cuda.ipc_collect()\n","\n","\n","# ===========================================================\n","# 1. 特征生成 & 辅助模块 (No changes)\n","# ===========================================================\n","\n","def generate_time_features_for_sequence(base_dt_obj, num_steps):\n","    time_features_list = []\n","    for i in range(num_steps):\n","        current_dt = base_dt_obj + timedelta(hours=i)\n","        hour_norm = current_dt.hour / 23.0\n","        day_of_year_norm = current_dt.timetuple().tm_yday / (366.0 if current_dt.year % 4 == 0 and (current_dt.year % 100 != 0 or current_dt.year % 400 == 0) else 365.0)\n","        hour_sin = math.sin(2 * math.pi * hour_norm)\n","        hour_cos = math.cos(2 * math.pi * hour_norm)\n","        doy_sin = math.sin(2 * math.pi * day_of_year_norm)\n","        doy_cos = math.cos(2 * math.pi * day_of_year_norm)\n","        time_features_list.append(torch.tensor([hour_sin, hour_cos, doy_sin, doy_cos], dtype=torch.float32))\n","    return torch.stack(time_features_list)\n","\n","\n","class MLPEncoder(nn.Module):\n","    def __init__(self, in_dim, out_dim, hid_dim=None, dropout_rate=0.1):\n","        super().__init__()\n","        if hid_dim is None:\n","            hid_dim = max(min(in_dim, out_dim), (in_dim + out_dim) // 2)\n","            if hid_dim == 0 and out_dim > 0 : hid_dim = out_dim\n","            if hid_dim == 0 and in_dim > 0 : hid_dim = in_dim\n","            if hid_dim == 0 : hid_dim = 1\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(in_dim, hid_dim),\n","            nn.ReLU(),\n","            nn.LayerNorm(hid_dim),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hid_dim, out_dim)\n","        )\n","    def forward(self, x):\n","        return self.mlp(x)\n","\n","class GINEModule(nn.Module):\n","    def __init__(self, gine_input_dim, gine_hidden_dim, gine_output_dim, edge_feature_dim, dropout_rate=0.5):\n","        super().__init__()\n","        self.gine_input_dim = gine_input_dim\n","        self.gine_hidden_dim = gine_hidden_dim\n","        self.gine_output_dim = gine_output_dim\n","        self.edge_feature_dim = edge_feature_dim\n","\n","        nn1 = nn.Sequential(\n","            nn.Linear(gine_input_dim, gine_hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(gine_hidden_dim, gine_hidden_dim)\n","        )\n","        self.conv1 = GINEConv(nn1, edge_dim=self.edge_feature_dim)\n","        self.bn1 = nn.BatchNorm1d(gine_hidden_dim)\n","        self.prelu1 = nn.PReLU(gine_hidden_dim)\n","\n","        nn2 = nn.Sequential(\n","            nn.Linear(gine_hidden_dim, gine_hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(gine_hidden_dim, gine_hidden_dim)\n","        )\n","        self.conv2 = GINEConv(nn2, edge_dim=self.edge_feature_dim)\n","        self.bn2 = nn.BatchNorm1d(gine_hidden_dim)\n","        self.prelu2 = nn.PReLU(gine_hidden_dim)\n","\n","        nn3 = nn.Sequential(\n","            nn.Linear(gine_hidden_dim, gine_hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(gine_hidden_dim, gine_hidden_dim)\n","        )\n","        self.conv3 = GINEConv(nn3, edge_dim=self.edge_feature_dim)\n","        self.bn3 = nn.BatchNorm1d(gine_hidden_dim)\n","        self.prelu3 = nn.PReLU(gine_hidden_dim)\n","\n","        self.lin_out = nn.Linear(gine_hidden_dim, gine_output_dim)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x, edge_index, edge_attr):\n","        if x.size(0) == 0: return x\n","\n","        x = self.conv1(x, edge_index, edge_attr=edge_attr)\n","        if x.size(0) > 1: x = self.bn1(x)\n","        x = self.prelu1(x)\n","        x = self.dropout(x)\n","\n","        x = self.conv2(x, edge_index, edge_attr=edge_attr)\n","        if x.size(0) > 1: x = self.bn2(x)\n","        x = self.prelu2(x)\n","        x = self.dropout(x)\n","\n","        x = self.conv3(x, edge_index, edge_attr=edge_attr)\n","        if x.size(0) > 1: x = self.bn3(x)\n","        x = self.prelu3(x)\n","\n","        x = self.lin_out(x)\n","        return x\n","\n","# ===========================================================\n","# 2. GINE-LSTM 模型定义\n","# ===========================================================\n","class GINELSTMModelWithHourlyHeads(nn.Module): # Renamed\n","    def __init__(self,\n","                 static_node_in_dim,\n","                 global_env_in_dim,\n","                 time_in_dim,\n","                 global_env_emb_dim,\n","                 time_emb_dim,\n","                 gine_hidden_dim,\n","                 gine_output_dim,\n","                 gine_edge_dim,\n","                 lstm_hidden_dim, # Changed from gru_hidden_dim\n","                 fusion_mlp_output_dim=None,\n","                 fusion_mlp_hidden_dim=None,\n","                 dropout_rate_fusion_mlp=0.1,\n","                 num_lstm_layers=1, # Changed from num_gru_layers\n","                 T_pred_horizon=12,\n","                 dropout_rate_encoders=0.1,\n","                 dropout_rate_gine=0.3,\n","                 dropout_rate_lstm=0.2, # Changed from dropout_rate_gru\n","                 mlp_prediction_hidden_dim=64,\n","                 dropout_rate_pred_head=0.2\n","                ):\n","        super().__init__()\n","        self.T_pred_horizon = T_pred_horizon\n","        self.static_node_in_dim = static_node_in_dim\n","        self.global_env_in_dim = global_env_in_dim\n","        self.time_in_dim = time_in_dim\n","        self.gine_output_dim = gine_output_dim\n","        self.lstm_hidden_dim = lstm_hidden_dim\n","        self.gine_edge_dim = gine_edge_dim\n","\n","        self.global_env_encoder = MLPEncoder(global_env_in_dim, global_env_emb_dim, dropout_rate=dropout_rate_encoders)\n","        self.time_encoder = MLPEncoder(time_in_dim, time_emb_dim, dropout_rate=dropout_rate_encoders)\n","        self.h0_c0_from_gine_encoder = MLPEncoder(gine_output_dim, lstm_hidden_dim, dropout_rate=dropout_rate_encoders) # For h0 of LSTM\n","\n","        self.gine_module_for_h0 = GINEModule(static_node_in_dim, gine_hidden_dim, gine_output_dim,\n","                                             edge_feature_dim=gine_edge_dim, dropout_rate=dropout_rate_gine)\n","        self.gine_module_for_sequence = GINEModule(static_node_in_dim, gine_hidden_dim, gine_output_dim,\n","                                                 edge_feature_dim=gine_edge_dim, dropout_rate=dropout_rate_gine)\n","\n","        concatenated_feature_dim = gine_output_dim + global_env_emb_dim + time_emb_dim\n","        actual_fusion_mlp_output_dim = fusion_mlp_output_dim if fusion_mlp_output_dim is not None else concatenated_feature_dim\n","        self.fusion_mlp_input_dim = concatenated_feature_dim\n","\n","        self.fusion_mlp = MLPEncoder(\n","            in_dim=concatenated_feature_dim,\n","            out_dim=actual_fusion_mlp_output_dim,\n","            hid_dim=fusion_mlp_hidden_dim,\n","            dropout_rate=dropout_rate_fusion_mlp\n","        )\n","\n","        lstm_input_size_actual = actual_fusion_mlp_output_dim\n","        self.lstm_input_dim = lstm_input_size_actual # For profiling\n","\n","        self.lstm = nn.LSTM( # Changed from nn.GRU\n","            input_size=lstm_input_size_actual,\n","            hidden_size=lstm_hidden_dim,\n","            num_layers=num_lstm_layers,\n","            batch_first=True,\n","            dropout=dropout_rate_lstm if num_lstm_layers > 1 else 0.0\n","        )\n","\n","        self.hourly_prediction_heads = nn.ModuleList()\n","        for _ in range(T_pred_horizon):\n","            self.hourly_prediction_heads.append(\n","                nn.Sequential(\n","                    nn.Linear(lstm_hidden_dim, mlp_prediction_hidden_dim), # Input from LSTM\n","                    nn.ReLU(),\n","                    nn.Dropout(dropout_rate_pred_head),\n","                    nn.Linear(mlp_prediction_hidden_dim, 1)\n","                )\n","            )\n","\n","        self.register_buffer('node_feat_mean', torch.zeros(static_node_in_dim))\n","        self.register_buffer('node_feat_std', torch.ones(static_node_in_dim))\n","\n","    def forward(self, list_of_batched_timesteps: list, timeline_time_features: torch.Tensor, device: torch.device):\n","        pyg_batch_7am = list_of_batched_timesteps[0].to(device)\n","        normalized_x_7am = (pyg_batch_7am.x - self.node_feat_mean) / (self.node_feat_std + 1e-8)\n","\n","        edge_attr_7am = pyg_batch_7am.edge_attr\n","        if edge_attr_7am is not None:\n","            edge_attr_7am = edge_attr_7am.float()\n","        else:\n","            num_edges_7am = pyg_batch_7am.edge_index.size(1)\n","            edge_attr_7am = torch.zeros(num_edges_7am, self.gine_edge_dim, device=device)\n","\n","        gine_output_7am = self.gine_module_for_h0(\n","            normalized_x_7am,\n","            pyg_batch_7am.edge_index,\n","            edge_attr=edge_attr_7am\n","        )\n","        h0_features_for_lstm_nodes = self.h0_c0_from_gine_encoder(gine_output_7am)\n","\n","        h0_for_lstm = h0_features_for_lstm_nodes.unsqueeze(0)\n","        c0_for_lstm = torch.zeros_like(h0_for_lstm)\n","\n","        if self.lstm.num_layers > 1: # Changed from self.gru\n","            h0_for_lstm = h0_for_lstm.repeat(self.lstm.num_layers, 1, 1)\n","            c0_for_lstm = c0_for_lstm.repeat(self.lstm.num_layers, 1, 1)\n","\n","        initial_hidden_state = (h0_for_lstm, c0_for_lstm)\n","\n","        all_lstm_input_features_over_time = [] # Renamed\n","        for t_pred_idx in range(self.T_pred_horizon):\n","            pyg_batch_this_timestep = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","            normalized_x = (pyg_batch_this_timestep.x - self.node_feat_mean) / (self.node_feat_std + 1e-8)\n","\n","            edge_attr_this_timestep = pyg_batch_this_timestep.edge_attr\n","            if edge_attr_this_timestep is not None:\n","                edge_attr_this_timestep = edge_attr_this_timestep.float()\n","            else:\n","                num_edges_current = pyg_batch_this_timestep.edge_index.size(1)\n","                edge_attr_this_timestep = torch.zeros(num_edges_current, self.gine_edge_dim, device=device)\n","\n","            gine_output_nodes_t = self.gine_module_for_sequence(\n","                normalized_x,\n","                pyg_batch_this_timestep.edge_index,\n","                edge_attr=edge_attr_this_timestep\n","            )\n","\n","            global_env_feat_t_unencoded = pyg_batch_this_timestep.graph_global_env_features\n","            expected_num_graphs_in_batch_t = pyg_batch_this_timestep.num_graphs\n","            expected_global_features_dim = self.global_env_encoder.mlp[0].in_features\n","            if not (global_env_feat_t_unencoded.shape == (expected_num_graphs_in_batch_t, expected_global_features_dim)):\n","                if global_env_feat_t_unencoded.ndim == 1 and \\\n","                   global_env_feat_t_unencoded.shape[0] == expected_num_graphs_in_batch_t * expected_global_features_dim:\n","                    global_env_feat_t_unencoded = global_env_feat_t_unencoded.view(expected_num_graphs_in_batch_t, expected_global_features_dim)\n","                elif global_env_feat_t_unencoded.numel() == expected_num_graphs_in_batch_t * expected_global_features_dim:\n","                    global_env_feat_t_unencoded = global_env_feat_t_unencoded.view(expected_num_graphs_in_batch_t, expected_global_features_dim)\n","                else:\n","                    print(f\"Warning: LSTM Input Time {t_pred_idx}: Correcting global_env_feat shape from {global_env_feat_t_unencoded.shape} to ({expected_num_graphs_in_batch_t}, {expected_global_features_dim}) with zeros due to mismatch.\")\n","                    global_env_feat_t_unencoded = torch.zeros(expected_num_graphs_in_batch_t, expected_global_features_dim, device=device)\n","            global_env_emb_t = self.global_env_encoder(global_env_feat_t_unencoded)\n","            global_env_emb_t_expanded = global_env_emb_t[pyg_batch_this_timestep.batch]\n","\n","            current_raw_time_feat_for_timestep_t = timeline_time_features[t_pred_idx, :].to(device)\n","            current_emb_time_feat_for_timestep_t = self.time_encoder(current_raw_time_feat_for_timestep_t)\n","            num_nodes_in_pyg_batch = pyg_batch_this_timestep.num_nodes\n","            time_emb_t_expanded_to_nodes = current_emb_time_feat_for_timestep_t.unsqueeze(0).expand(num_nodes_in_pyg_batch, -1)\n","\n","            concatenated_features_for_timestep_t = torch.cat([gine_output_nodes_t, global_env_emb_t_expanded, time_emb_t_expanded_to_nodes], dim=-1)\n","            fused_features_for_timestep_t = self.fusion_mlp(concatenated_features_for_timestep_t)\n","            all_lstm_input_features_over_time.append(fused_features_for_timestep_t)\n","\n","        stacked_lstm_input_features = torch.stack(all_lstm_input_features_over_time, dim=1)\n","\n","        if initial_hidden_state[0].shape[1] != stacked_lstm_input_features.shape[0]:\n","            print(f\"CRITICAL WARNING: Node count mismatch for LSTM h0/c0 ({initial_hidden_state[0].shape[1]}) and LSTM input sequence ({stacked_lstm_input_features.shape[0]}).\")\n","            if initial_hidden_state[0].shape[1] > stacked_lstm_input_features.shape[0]:\n","                h0_adj = initial_hidden_state[0][:, :stacked_lstm_input_features.shape[0], :]\n","                c0_adj = initial_hidden_state[1][:, :stacked_lstm_input_features.shape[0], :]\n","                initial_hidden_state = (h0_adj, c0_adj)\n","\n","        lstm_out, _ = self.lstm(stacked_lstm_input_features, initial_hidden_state)\n","\n","        all_hourly_final_predictions_scaled = []\n","        for t in range(self.T_pred_horizon):\n","            lstm_out_t = lstm_out[:, t, :]\n","            prediction_t_scaled = self.hourly_prediction_heads[t](lstm_out_t)\n","            all_hourly_final_predictions_scaled.append(prediction_t_scaled.squeeze(-1))\n","\n","        predictions_scaled = torch.stack(all_hourly_final_predictions_scaled, dim=1)\n","        return predictions_scaled\n","\n","# ===========================================================\n","# 3. 评估指标函数 (在原始尺度上计算指标) - NO CHANGES\n","# ===========================================================\n","# ... (mse_loss_masked, calculate_hourly_metrics are identical) ...\n","def mse_loss_masked(predictions_scaled, targets_scaled, mask):\n","    expanded_mask = mask.unsqueeze(1).expand_as(targets_scaled)\n","    valid_targets_mask = ~torch.isnan(targets_scaled)\n","    final_mask = expanded_mask & valid_targets_mask\n","    if final_mask.sum() == 0:\n","        return torch.tensor(0.0, device=predictions_scaled.device, requires_grad=True)\n","    loss = F.mse_loss(predictions_scaled[final_mask], targets_scaled[final_mask])\n","    return loss\n","\n","def calculate_hourly_metrics(predictions_scaled, targets_scaled, node_masks, target_mean, target_std):\n","    target_mean_cpu = target_mean.cpu()\n","    target_std_cpu = target_std.cpu()\n","    preds_unscaled = predictions_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","    targets_unscaled = targets_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","\n","    num_nodes_total, T_horizon = preds_unscaled.shape\n","    hourly_metrics_dict = {}\n","\n","    preds_np = preds_unscaled.numpy()\n","    targets_np = targets_unscaled.numpy()\n","    mask_np = node_masks.cpu().numpy()\n","\n","    for t in range(T_horizon):\n","        preds_t_all_nodes = preds_np[:, t]\n","        targets_t_all_nodes = targets_np[:, t]\n","        preds_t_on_loss_nodes = preds_t_all_nodes[mask_np]\n","        targets_t_on_loss_nodes = targets_t_all_nodes[mask_np]\n","        valid_target_data_mask_t = ~np.isnan(targets_t_on_loss_nodes)\n","        preds_t_final_valid = preds_t_on_loss_nodes[valid_target_data_mask_t]\n","        targets_t_final_valid = targets_t_on_loss_nodes[valid_target_data_mask_t]\n","\n","        if preds_t_final_valid.shape[0] < 2:\n","            hourly_metrics_dict[t] = {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count': 0}\n","            continue\n","        mse = np.mean((preds_t_final_valid - targets_t_final_valid)**2)\n","        mae = np.mean(np.abs(preds_t_final_valid - targets_t_final_valid))\n","        rmse = np.sqrt(mse)\n","        try:\n","            r2 = r2_score(targets_t_final_valid, preds_t_final_valid)\n","        except ValueError:\n","            r2 = np.nan\n","        hourly_metrics_dict[t] = {'mse': mse, 'mae': mae, 'rmse': rmse, 'r2': r2, 'count': preds_t_final_valid.shape[0]}\n","    return hourly_metrics_dict\n","\n","# ===========================================================\n","# 4. 训练与评估循环 (适配y归一化) - NO CHANGES\n","# ===========================================================\n","# ... (train_epoch, evaluate_epoch are identical) ...\n","def train_epoch(model, loader, optimizer, device, timeline_time_features,\n","                node_feat_mean, node_feat_std, target_mean, target_std):\n","    model.train()\n","    total_loss_scaled = 0\n","    num_sequences_processed = 0\n","    model.node_feat_mean = node_feat_mean.to(device)\n","    model.node_feat_std = node_feat_std.to(device)\n","\n","    target_mean_dev = target_mean.to(device)\n","    target_std_dev = target_std.to(device)\n","\n","    epoch_start_time = time.time()\n","    for list_of_batched_timesteps in loader:\n","        optimizer.zero_grad()\n","        predictions_batch_scaled = model(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","\n","        targets_list_for_loss_scaled = []\n","        first_predicted_timestep_batch = list_of_batched_timesteps[1].to(device)\n","        mask_for_loss = ~first_predicted_timestep_batch.building_mask\n","\n","        for t_pred_idx in range(model.T_pred_horizon):\n","            current_target_timestep_batch = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","            targets_t_nodes_original = current_target_timestep_batch.y.squeeze()\n","            targets_t_nodes_scaled = (targets_t_nodes_original - target_mean_dev) / (target_std_dev + 1e-8)\n","            targets_list_for_loss_scaled.append(targets_t_nodes_scaled)\n","\n","        targets_batch_scaled = torch.stack(targets_list_for_loss_scaled, dim=1)\n","        loss = mse_loss_masked(predictions_batch_scaled, targets_batch_scaled, mask_for_loss)\n","        num_sequences_in_this_super_batch = list_of_batched_timesteps[0].num_graphs\n","\n","        if not torch.isnan(loss) and not torch.isinf(loss) and loss.item() > 0 :\n","            loss.backward()\n","            optimizer.step()\n","            total_loss_scaled += loss.item() * num_sequences_in_this_super_batch\n","        num_sequences_processed += num_sequences_in_this_super_batch\n","\n","    epoch_duration = time.time() - epoch_start_time\n","    avg_loss = total_loss_scaled / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","    return avg_loss, epoch_duration\n","\n","def evaluate_epoch(model, loader, device, timeline_time_features,\n","                   node_feat_mean, node_feat_std, target_mean, target_std, epoch_type=\"Eval\"):\n","    model.eval()\n","    all_batch_predictions_scaled = []\n","    all_batch_targets_scaled = []\n","    all_batch_masks_for_metrics = []\n","    total_loss_scaled = 0\n","    num_sequences_processed = 0\n","    model.node_feat_mean = node_feat_mean.to(device)\n","    model.node_feat_std = node_feat_std.to(device)\n","\n","    target_mean_dev = target_mean.to(device)\n","    target_std_dev = target_std.to(device)\n","\n","    eval_start_time = time.time()\n","    with torch.no_grad():\n","        for list_of_batched_timesteps in loader:\n","            predictions_batch_scaled = model(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","            targets_list_original = []\n","            first_predicted_timestep_batch = list_of_batched_timesteps[1].to(device)\n","            mask_for_metrics = ~first_predicted_timestep_batch.building_mask\n","\n","            for t_pred_idx in range(model.T_pred_horizon):\n","                current_target_timestep_batch = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","                targets_t_nodes_original = current_target_timestep_batch.y.squeeze()\n","                targets_list_original.append(targets_t_nodes_original)\n","\n","            targets_batch_original = torch.stack(targets_list_original, dim=1)\n","            targets_batch_scaled_for_loss = (targets_batch_original - target_mean_dev) / (target_std_dev + 1e-8)\n","            loss = mse_loss_masked(predictions_batch_scaled, targets_batch_scaled_for_loss, mask_for_metrics)\n","            num_sequences_in_this_super_batch = list_of_batched_timesteps[0].num_graphs\n","\n","            if not torch.isnan(loss) and not torch.isinf(loss):\n","                total_loss_scaled += loss.item() * num_sequences_in_this_super_batch\n","            num_sequences_processed += num_sequences_in_this_super_batch\n","\n","            all_batch_predictions_scaled.append(predictions_batch_scaled.cpu())\n","            all_batch_targets_scaled.append(targets_batch_scaled_for_loss.cpu())\n","            all_batch_masks_for_metrics.append(mask_for_metrics.cpu())\n","\n","    eval_duration = time.time() - eval_start_time\n","    avg_loss_scaled = total_loss_scaled / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","\n","    if not all_batch_predictions_scaled:\n","        empty_metrics = {t: {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0} for t in range(model.T_pred_horizon)}\n","        return avg_loss_scaled, empty_metrics, eval_duration\n","\n","    final_predictions_scaled = torch.cat(all_batch_predictions_scaled, dim=0)\n","    final_targets_scaled = torch.cat(all_batch_targets_scaled, dim=0)\n","    final_masks_for_metrics = torch.cat(all_batch_masks_for_metrics, dim=0)\n","\n","    hourly_metrics_original_scale = calculate_hourly_metrics(final_predictions_scaled, final_targets_scaled,\n","                                                             final_masks_for_metrics, target_mean.cpu(), target_std.cpu())\n","    return avg_loss_scaled, hourly_metrics_original_scale, eval_duration\n","\n","# ===========================================================\n","# 5. 主训练流程 (GINE-LSTM)\n","# ===========================================================\n","\n","def calculate_aggregated_metrics_report(hourly_metrics_dict, T_pred_horizon):\n","    # ... (identical) ...\n","    metrics_to_aggregate = ['r2', 'mse', 'mae', 'rmse']\n","    aggregated_report = {}\n","    for metric_name in metrics_to_aggregate:\n","        values = [hourly_metrics_dict[t][metric_name] for t in range(T_pred_horizon) if t in hourly_metrics_dict and not np.isnan(hourly_metrics_dict[t][metric_name])]\n","        if values:\n","            aggregated_report[f'avg_{metric_name}'] = np.mean(values)\n","            aggregated_report[f'std_{metric_name}'] = np.std(values)\n","        else:\n","            aggregated_report[f'avg_{metric_name}'] = np.nan\n","            aggregated_report[f'std_{metric_name}'] = np.nan\n","    return aggregated_report\n","\n","def main_training_gine_lstm_hourly_heads( # Renamed\n","    all_sequences_data: list,\n","    config: dict,\n","    time_features_for_dataset: torch.Tensor\n","):\n","    train_start_time = time.time()\n","    report_data = {'config': config}\n","\n","    seed = config.get('seed', 42)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"使用设备: {device}\")\n","    report_data['device'] = str(device)\n","\n","    T_PRED_HORIZON = config.get('T_pred_horizon', 12)\n","    expected_input_len = T_PRED_HORIZON + 1\n","    valid_sequences_data = [seq for seq in all_sequences_data if isinstance(seq, list) and len(seq) == expected_input_len]\n","    if len(valid_sequences_data) != len(all_sequences_data):\n","        print(f\"警告: 从 {len(all_sequences_data)} 个序列中筛选出 {len(valid_sequences_data)} 个长度为 {expected_input_len} 的有效序列。\")\n","    if not valid_sequences_data:\n","        raise ValueError(f\"没有找到长度为 {expected_input_len} 的有效序列数据。\")\n","    all_sequences_data = valid_sequences_data\n","\n","    # Dataset split\n","    num_total_sequences = len(all_sequences_data)\n","    indices = np.random.permutation(num_total_sequences)\n","    train_split_ratio = config.get('train_split_ratio', 0.7)\n","    val_split_ratio = config.get('val_split_ratio', 0.2)\n","    train_size = int(train_split_ratio * num_total_sequences)\n","    val_size = int(val_split_ratio * num_total_sequences)\n","    train_indices = indices[:train_size]\n","    val_indices = indices[train_size : train_size + val_size]\n","    test_indices = indices[train_size + val_size :]\n","    train_dataset = [all_sequences_data[i] for i in train_indices]\n","    val_dataset   = [all_sequences_data[i] for i in val_indices]\n","    test_dataset  = [all_sequences_data[i] for i in test_indices]\n","    report_data['dataset_split'] = {'total_sequences': num_total_sequences, 'train_size': len(train_dataset), 'val_size': len(val_dataset), 'test_size': len(test_dataset)}\n","\n","    # Scaler calculation\n","    all_train_node_features_list = []\n","    all_train_target_values_list_for_scaling = []\n","    for seq in train_dataset:\n","        for i_step, graph_data in enumerate(seq):\n","            if hasattr(graph_data, 'x') and graph_data.x is not None:\n","                all_train_node_features_list.append(graph_data.x)\n","            if i_step > 0 and hasattr(graph_data, 'y') and graph_data.y is not None:\n","                y_original = graph_data.y.squeeze(); current_mask_for_loss = ~graph_data.building_mask\n","                valid_target_indices = current_mask_for_loss & ~torch.isnan(y_original)\n","                if valid_target_indices.sum() > 0:\n","                    all_train_target_values_list_for_scaling.append(y_original[valid_target_indices])\n","\n","    if not all_train_node_features_list: raise ValueError(\"训练数据中未找到节点特征 'x'，无法计算scaler！\")\n","    all_train_node_features_tensor = torch.cat(all_train_node_features_list, dim=0)\n","    node_feat_mean = torch.mean(all_train_node_features_tensor, dim=0); node_feat_std = torch.std(all_train_node_features_tensor, dim=0)\n","    node_feat_std[node_feat_std < 1e-8] = 1.0\n","    scaler_path_x = Path(config['results_dir']) / \"node_feature_scaler_gine_lstm.pth\" # Renamed\n","    torch.save({'mean': node_feat_mean, 'std': node_feat_std}, scaler_path_x); print(f\"节点特征x scaler已保存到: {scaler_path_x}\")\n","\n","    if not all_train_target_values_list_for_scaling:\n","        target_mean = torch.tensor(0.0); target_std = torch.tensor(1.0)\n","    else:\n","        all_train_target_values_tensor = torch.cat(all_train_target_values_list_for_scaling, dim=0)\n","        target_mean = torch.mean(all_train_target_values_tensor.float()); target_std = torch.std(all_train_target_values_tensor.float())\n","        if target_std < 1e-8: target_std = torch.tensor(1.0)\n","    target_scaler_path = Path(config['results_dir']) / \"target_scaler_gine_lstm.pth\" # Renamed\n","    torch.save({'mean': target_mean, 'std': target_std}, target_scaler_path); print(f\"目标值y scaler已保存到: {target_scaler_path}\")\n","\n","    # DataLoaders\n","    batch_size = config.get('batch_size', 8); num_workers = config.get('num_workers', 0)\n","    pin_memory_flag = config.get('pin_memory', False) and device.type == 'cuda'\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=num_workers, pin_memory=pin_memory_flag)\n","    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n","    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n","\n","    # Model Initialization\n","    sample_graph_7am_for_dims = all_sequences_data[0][0]\n","    static_node_in_dim = sample_graph_7am_for_dims.x.shape[1]\n","    global_env_in_dim = sample_graph_7am_for_dims.graph_global_env_features.shape[0] if sample_graph_7am_for_dims.graph_global_env_features.ndim == 1 else sample_graph_7am_for_dims.graph_global_env_features.shape[1]\n","    time_in_dim = time_features_for_dataset.shape[1]\n","\n","    if hasattr(sample_graph_7am_for_dims, 'edge_attr') and sample_graph_7am_for_dims.edge_attr is not None:\n","        inferred_gine_edge_dim = sample_graph_7am_for_dims.edge_attr.shape[1]\n","    else:\n","        inferred_gine_edge_dim = config.get('gine_edge_dim', 5)\n","        print(f\"Warning: Could not infer gine_edge_dim from sample data, using config or default: {inferred_gine_edge_dim}\")\n","    gine_edge_dim_config = config.get('gine_edge_dim', inferred_gine_edge_dim)\n","\n","    model = GINELSTMModelWithHourlyHeads( # Use GINELSTMModel\n","        static_node_in_dim=static_node_in_dim, global_env_in_dim=global_env_in_dim, time_in_dim=time_in_dim,\n","        global_env_emb_dim=config.get('global_env_emb_dim', 16), time_emb_dim=config.get('time_emb_dim', 8),\n","        gine_hidden_dim=config.get('gcn_hidden_dim', 128),\n","        gine_output_dim=config.get('gcn_output_dim', 128),\n","        gine_edge_dim=gine_edge_dim_config,\n","        lstm_hidden_dim=config.get('gru_hidden_dim', 128), # Using gru_hidden_dim key for LSTM\n","        fusion_mlp_output_dim=config.get('fusion_mlp_output_dim', 128),\n","        fusion_mlp_hidden_dim=config.get('fusion_mlp_hidden_dim', 64),\n","        dropout_rate_fusion_mlp=config.get('dropout_rate_fusion_mlp', 0.2),\n","        num_lstm_layers=config.get('num_gru_layers', 1), # Using num_gru_layers key for LSTM\n","        T_pred_horizon=T_PRED_HORIZON,\n","        dropout_rate_encoders=config.get('dropout_rate_encoders', 0.1),\n","        dropout_rate_gine=config.get('dropout_rate_gcn', 0.3),\n","        dropout_rate_lstm=config.get('dropout_rate_gru', 0.2), # Using dropout_rate_gru key for LSTM\n","        mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim', 64),\n","        dropout_rate_pred_head=config.get('dropout_rate_pred_head', 0.2)\n","    ).to(device)\n","\n","    model.node_feat_mean = node_feat_mean.to(device); model.node_feat_std = node_feat_std.to(device)\n","    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"模型总参数量 (GINE-LSTM): {total_params:,}\"); report_data['model_total_parameters'] = total_params\n","\n","    # ===== Component FLOPS Calculation =====\n","    print(\"\\nCalculating MACs for model components (approximate FLOPS):\")\n","    report_data['component_gmacs'] = {}\n","    dummy_nodes_component = 2500\n","    dummy_edges_component = 60000\n","    dummy_batch_global_comp = 1\n","    model.eval()\n","\n","    # 1. GINEModule\n","    try:\n","        gine_module_to_profile = model.gine_module_for_h0\n","        dummy_x_gine = torch.randn(dummy_nodes_component, gine_module_to_profile.gine_input_dim, device=device)\n","        dummy_ei_gine = torch.randint(0, dummy_nodes_component, (2, dummy_edges_component), device=device)\n","        dummy_ea_gine = torch.randn(dummy_edges_component, gine_module_to_profile.edge_feature_dim, device=device)\n","\n","        macs_gine = torchprofile.profile_macs(gine_module_to_profile, args=(dummy_x_gine, dummy_ei_gine, dummy_ea_gine))\n","        report_data['component_gmacs']['gine_module'] = macs_gine / 1e9\n","        print(f\"  GINEModule GMACs (edge_dim={gine_module_to_profile.edge_feature_dim}): {macs_gine / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling GINEModule: {e}\"); report_data['component_gmacs']['gine_module'] = \"Error\"\n","\n","    # 2. LSTM Layer (Manual MAC Calculation)\n","    print(f\"  Manually Calculating MACs for LSTM Layer:\")\n","    try:\n","        lstm_layer = model.lstm\n","        N_nodes = dummy_nodes_component\n","        L_seq = T_PRED_HORIZON\n","        H_in = lstm_layer.input_size\n","        H_hidden = lstm_layer.hidden_size\n","        num_layers = lstm_layer.num_layers\n","\n","        macs_lstm_manual = 0\n","        # For a single layer LSTM: MACs ≈ N * L * 4 * (H_in * H_hidden + H_hidden^2)\n","        macs_lstm_manual = N_nodes * L_seq * 4 * (H_in * H_hidden + H_hidden * H_hidden)\n","        if num_layers > 1:\n","            macs_lstm_manual += N_nodes * L_seq * (num_layers - 1) * 4 * (H_hidden * H_hidden + H_hidden * H_hidden)\n","\n","        gmacs_lstm_manual = macs_lstm_manual / 1e9\n","        report_data['component_gmacs']['lstm_layer'] = gmacs_lstm_manual\n","        report_data['component_gmacs']['lstm_layer_profiling_notes'] = \"Manually calculated based on formula.\"\n","        print(f\"  LSTM Parameters: input_size={H_in}, hidden_size={H_hidden}, num_layers={num_layers}\")\n","        print(f\"  Used for calculation: N_nodes={N_nodes}, L_seq={L_seq}\")\n","        print(f\"  LSTM Layer GMACs (Manual): {gmacs_lstm_manual:.4f} (for sequence length {L_seq})\")\n","\n","    except Exception as e:\n","        print(f\"  Error manually calculating LSTM Layer MACs: {e}\")\n","        report_data['component_gmacs']['lstm_layer'] = \"Error\"\n","        report_data['component_gmacs']['lstm_layer_profiling_notes'] = f\"Error during manual calculation: {str(e)}\"\n","\n","    # 3. Fusion MLP, 4. Prediction Head, 5. Global Env Encoder, 6. Time Encoder\n","    try:\n","        fusion_mlp_to_profile = model.fusion_mlp\n","        dummy_input_fusion_mlp = torch.randn(dummy_nodes_component, model.fusion_mlp_input_dim, device=device)\n","        macs_fusion_mlp = torchprofile.profile_macs(fusion_mlp_to_profile, args=(dummy_input_fusion_mlp,))\n","        report_data['component_gmacs']['fusion_mlp'] = macs_fusion_mlp / 1e9\n","        print(f\"  Fusion MLP GMACs: {macs_fusion_mlp / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Fusion MLP: {e}\"); report_data['component_gmacs']['fusion_mlp'] = \"Error\"\n","\n","    try:\n","        pred_head_to_profile = model.hourly_prediction_heads[0]\n","        dummy_input_pred_head = torch.randn(dummy_nodes_component, model.lstm_hidden_dim, device=device) # Input from LSTM\n","        macs_pred_head = torchprofile.profile_macs(pred_head_to_profile, args=(dummy_input_pred_head,))\n","        report_data['component_gmacs']['prediction_head_mlp'] = macs_pred_head / 1e9\n","        print(f\"  Prediction Head MLP (single hour) GMACs: {macs_pred_head / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Prediction Head: {e}\"); report_data['component_gmacs']['prediction_head_mlp'] = \"Error\"\n","\n","    try:\n","        encoder_to_profile = model.global_env_encoder\n","        dummy_input_encoder = torch.randn(dummy_batch_global_comp, model.global_env_in_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['global_env_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  Global Env Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Global Env Encoder: {e}\"); report_data['component_gmacs']['global_env_encoder_mlp'] = \"Error\"\n","\n","    try:\n","        encoder_to_profile = model.time_encoder\n","        dummy_input_encoder = torch.randn(dummy_batch_global_comp, model.time_in_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['time_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  Time Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Time Encoder: {e}\"); report_data['component_gmacs']['time_encoder_mlp'] = \"Error\"\n","\n","    # 7. H0/C0 from GINE Encoder (MLPEncoder)\n","    try:\n","        encoder_to_profile = model.h0_c0_from_gine_encoder # Renamed\n","        dummy_input_encoder = torch.randn(dummy_nodes_component, model.gine_output_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['h0_c0_from_gine_encoder_mlp'] = macs_encoder / 1e9 # Renamed key\n","        print(f\"  H0/C0 from GINE Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling H0/C0 from GINE Encoder: {e}\"); report_data['component_gmacs']['h0_c0_from_gine_encoder_mlp'] = \"Error\"\n","\n","    model.train()\n","    # ===== End Component FLOPS Calculation =====\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config.get('lr', 0.001), weight_decay=config.get('weight_decay', 1e-5))\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=config.get('scheduler_patience', 20), verbose=True)\n","\n","    best_val_loss_scaled = float('inf'); best_val_hourly_metrics_original = None; best_epoch = 0\n","    patience_counter = 0; max_epochs = config.get('max_epochs', 300); early_stopping_patience = config.get('early_stopping_patience', 45)\n","    model_save_path = Path(config['results_dir']) / f\"best_ginelstm_hourly_heads_model_seed{seed}.pth\" # Renamed\n","    timeline_time_features_on_device = time_features_for_dataset.to(device)\n","    target_mean_cpu = target_mean.cpu(); target_std_cpu = target_std.cpu()\n","    target_mean_on_device = target_mean.to(device); target_std_on_device = target_std.to(device)\n","    epoch_times = []\n","\n","    for epoch in range(1, max_epochs + 1):\n","        train_loss_scaled, epoch_duration = train_epoch(model, train_loader, optimizer, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_on_device, target_std_on_device)\n","        epoch_times.append(epoch_duration)\n","        val_loss_scaled, val_hourly_metrics_original, _ = evaluate_epoch(model, val_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Validation\")\n","        scheduler.step(val_loss_scaled)\n","        print(f\"Epoch {epoch:03d} | Train Scaled MSE: {train_loss_scaled:.4f} | Val Scaled MSE: {val_loss_scaled:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f} | Epoch Time: {epoch_duration:.2f}s\")\n","        _print_hourly_metrics_summary(\"Val\", val_hourly_metrics_original, T_PRED_HORIZON, indent=\"                     \")\n","        if val_loss_scaled < best_val_loss_scaled:\n","            best_val_loss_scaled = val_loss_scaled; best_val_hourly_metrics_original = val_hourly_metrics_original; best_epoch = epoch\n","            patience_counter = 0; torch.save(model.state_dict(), model_save_path)\n","            print(f\"                     ---> Best model saved (Epoch: {epoch}, Val Scaled MSE: {best_val_loss_scaled:.4f})\")\n","        else:\n","            patience_counter += 1\n","        if patience_counter >= early_stopping_patience:\n","            print(f\"Early stopping at epoch {epoch} due to no improvement.\"); break\n","\n","    total_training_duration = time.time() - train_start_time\n","    report_data['total_training_time_seconds'] = total_training_duration\n","    report_data['average_epoch_time_seconds'] = np.mean(epoch_times) if epoch_times else np.nan\n","    report_data['num_epochs_trained'] = epoch; report_data['best_validation_epoch'] = best_epoch\n","    report_data['best_validation_scaled_mse'] = best_val_loss_scaled\n","\n","    model_for_eval = GINELSTMModelWithHourlyHeads( # Use GINELSTMModel\n","        static_node_in_dim=static_node_in_dim, global_env_in_dim=global_env_in_dim, time_in_dim=time_in_dim,\n","        global_env_emb_dim=config.get('global_env_emb_dim', 16), time_emb_dim=config.get('time_emb_dim', 8),\n","        gine_hidden_dim=config.get('gcn_hidden_dim', 128),\n","        gine_output_dim=config.get('gcn_output_dim', 128),\n","        gine_edge_dim=gine_edge_dim_config,\n","        lstm_hidden_dim=config.get('gru_hidden_dim', 128),\n","        fusion_mlp_output_dim=config.get('fusion_mlp_output_dim', 128),\n","        fusion_mlp_hidden_dim=config.get('fusion_mlp_hidden_dim', 64),\n","        dropout_rate_fusion_mlp=config.get('dropout_rate_fusion_mlp', 0.2),\n","        num_lstm_layers=config.get('num_gru_layers', 1), T_pred_horizon=T_PRED_HORIZON,\n","        dropout_rate_encoders=config.get('dropout_rate_encoders', 0.1),\n","        dropout_rate_gine=config.get('dropout_rate_gcn', 0.3),\n","        dropout_rate_lstm=config.get('dropout_rate_gru', 0.2),\n","        mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim', 64),\n","        dropout_rate_pred_head=config.get('dropout_rate_pred_head', 0.2)\n","    ).to(device)\n","    try: model_for_eval.load_state_dict(torch.load(model_save_path, map_location=device))\n","    except Exception as e: print(f\"无法加载最佳模型 ({e})，将使用训练循环结束时的模型。\"); model_for_eval = model\n","\n","    print(\"\\n评估最佳模型在训练集上...\"); best_model_train_loss_scaled, best_model_train_hourly_metrics, train_eval_duration = evaluate_epoch(model_for_eval, train_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Best Model on Train\")\n","    report_data['best_model_train_set_metrics_hourly'] = best_model_train_hourly_metrics; report_data['best_model_train_set_metrics_aggregated'] = calculate_aggregated_metrics_report(best_model_train_hourly_metrics, T_PRED_HORIZON)\n","    report_data['best_model_train_set_eval_time_seconds'] = train_eval_duration; _print_hourly_metrics_summary(\"最佳模型训练集\", best_model_train_hourly_metrics, T_PRED_HORIZON)\n","\n","    report_data['best_model_validation_set_metrics_hourly'] = best_val_hourly_metrics_original\n","    if best_val_hourly_metrics_original: report_data['best_model_validation_set_metrics_aggregated'] = calculate_aggregated_metrics_report(best_val_hourly_metrics_original, T_PRED_HORIZON)\n","    else:\n","        _, reeval_val_metrics, val_eval_duration = evaluate_epoch(model_for_eval, val_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Best Model on Val (Re-eval)\")\n","        report_data['best_model_validation_set_metrics_hourly'] = reeval_val_metrics\n","        report_data['best_model_validation_set_metrics_aggregated'] = calculate_aggregated_metrics_report(reeval_val_metrics, T_PRED_HORIZON)\n","        report_data['best_model_validation_set_eval_time_seconds'] = val_eval_duration\n","    _print_hourly_metrics_summary(\"最佳模型验证集\", report_data['best_model_validation_set_metrics_hourly'], T_PRED_HORIZON)\n","\n","\n","    print(\"\\n评估最佳模型在测试集上...\"); test_loss_scaled, test_hourly_metrics_original, test_inference_duration = evaluate_epoch(model_for_eval, test_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Test\")\n","    report_data['test_set_inference_time_seconds'] = test_inference_duration; report_data['best_model_test_set_metrics_hourly'] = test_hourly_metrics_original; report_data['best_model_test_set_metrics_aggregated'] = calculate_aggregated_metrics_report(test_hourly_metrics_original, T_PRED_HORIZON)\n","    print(\"\\n\" + \"=\"*20 + \" 最终测试集评估结果 (GINE-LSTM) \" + \"=\"*20); print(f\"平均测试 Scaled MSE: {test_loss_scaled:.4f}\") # Renamed\n","    _print_hourly_metrics_summary(\"测试集\", test_hourly_metrics_original, T_PRED_HORIZON)\n","\n","    agg_test = report_data['best_model_test_set_metrics_aggregated']\n","    print(f\"平均测试 MSE (Orig) : {agg_test.get('avg_mse', np.nan):.4f} (Std: {agg_test.get('std_mse', np.nan):.4f})\")\n","    print(f\"平均测试 R2 (Orig)  : {agg_test.get('avg_r2', np.nan):.4f} (Std: {agg_test.get('std_r2', np.nan):.4f})\")\n","    print(f\"平均测试 MAE (Orig) : {agg_test.get('avg_mae', np.nan):.4f} (Std: {agg_test.get('std_mae', np.nan):.4f})\")\n","    print(f\"平均测试 RMSE (Orig): {agg_test.get('avg_rmse', np.nan):.4f} (Std: {agg_test.get('std_rmse', np.nan):.4f})\")\n","    print(\"=\"*70)\n","\n","    report_file_path = Path(config['results_dir']) / f\"training_report_gine_lstm_seed{seed}.json\" # Renamed\n","    try:\n","        class NpEncoder(json.JSONEncoder):\n","            def default(self, obj):\n","                if isinstance(obj, np.integer): return int(obj)\n","                if isinstance(obj, np.floating): return float(obj)\n","                if isinstance(obj, np.ndarray): return obj.tolist()\n","                if isinstance(obj, torch.Tensor): return obj.tolist()\n","                if isinstance(obj, Path): return str(obj)\n","                return super(NpEncoder, self).default(obj)\n","        with open(report_file_path, 'w') as f: json.dump(report_data, f, indent=4, cls=NpEncoder)\n","        print(f\"训练报告已保存到: {report_file_path}\")\n","    except Exception as e: print(f\"保存训练报告失败: {e}\")\n","\n","    return model_for_eval, node_feat_mean, node_feat_std, target_mean, target_std\n","\n","def _print_hourly_metrics_summary(set_name, hourly_metrics, T_pred_horizon, indent=\"  \"):\n","    # ... (identical) ...\n","    if hourly_metrics is None:\n","        print(f\"{indent}{set_name} metrics not available.\")\n","        return\n","    print(f\"\\n{indent}每小时 {set_name} 指标 (Original Scale):\")\n","    header_printed = False\n","    for hour_idx in range(T_pred_horizon):\n","        metrics = hourly_metrics.get(hour_idx, {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0})\n","        if not header_printed:\n","            print(f\"{indent}  Hour | {'R2':>13s} | {'MSE':>14s} | {'MAE':>14s} | {'RMSE':>15s} | {'Count':>7s}\")\n","            header_printed = True\n","        print(f\"{indent}  {hour_idx:02d}   | {metrics.get('r2', np.nan):13.4f} | {metrics.get('mse', np.nan):14.4f} | {metrics.get('mae', np.nan):14.4f} | {metrics.get('rmse', np.nan):15.4f} | {metrics.get('count', 0):7d}\")\n","    aggregated = calculate_aggregated_metrics_report(hourly_metrics, T_pred_horizon)\n","    print(f\"{indent}  Aggregated Avg R2   : {aggregated.get('avg_r2', np.nan):.4f} (Std: {aggregated.get('std_r2', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MSE  : {aggregated.get('avg_mse', np.nan):.4f} (Std: {aggregated.get('std_mse', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MAE  : {aggregated.get('avg_mae', np.nan):.4f} (Std: {aggregated.get('std_mae', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg RMSE : {aggregated.get('avg_rmse', np.nan):.4f} (Std: {aggregated.get('std_rmse', np.nan):.4f})\")\n","\n","# ===========================================================\n","# 6. 主执行块\n","# ===========================================================\n","if __name__ == \"__main__\":\n","    gc.collect()\n","    if torch.cuda.is_available(): torch.cuda.empty_cache()\n","\n","    DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","\n","    if not DRIVE_BASE_PATH.exists(): DRIVE_BASE_PATH.mkdir(parents=True, exist_ok=True)\n","\n","    DATA_SUBDIR = Path(\"Result/Sequential_12Hour_Data\")\n","    DATA_FILENAME = \"graph_seq_20230503_SeqH7to19_NpyH8fill0.0.pkl\"\n","    RESULTS_SUBDIR = Path(\"Result/Final_GINELSTM1\")\n","    RESULTS_SAVE_DIR = DRIVE_BASE_PATH / RESULTS_SUBDIR\n","    os.makedirs(RESULTS_SAVE_DIR, exist_ok=True)\n","    DATA_PATH = DRIVE_BASE_PATH / DATA_SUBDIR / DATA_FILENAME\n","\n","    DATA_YEAR = 2023; DATA_MONTH = 5; DATA_DAY = 3\n","    START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES = 8; PREDICTION_HORIZON = 12\n","\n","    training_config = {\n","        'seed': 42, 'batch_size': 8, 'lr': 0.001, 'weight_decay': 1e-5,\n","        'max_epochs': 1000, 'scheduler_patience': 20, 'early_stopping_patience': 45,\n","        'T_pred_horizon': PREDICTION_HORIZON, 'results_dir': str(RESULTS_SAVE_DIR),\n","        'global_env_emb_dim': 16, 'time_emb_dim': 8,\n","        'gcn_hidden_dim': 128, # Used as gine_hidden_dim\n","        'gcn_output_dim': 128, # Used as gine_output_dim\n","        'gine_edge_dim': 6,   # Ensure this matches your edge_attr dimension\n","        'gru_hidden_dim': 128, # This will be lstm_hidden_dim\n","        'num_gru_layers': 1,  # This will be num_lstm_layers\n","        'mlp_prediction_hidden_dim': 64,\n","        'fusion_mlp_output_dim': 128,\n","        'fusion_mlp_hidden_dim': 64,\n","        'dropout_rate_fusion_mlp': 0.2, 'dropout_rate_encoders': 0.1,\n","        'dropout_rate_gcn': 0.3, # Used as dropout_rate_gine\n","        'dropout_rate_gru': 0.2, # Used as dropout_rate_lstm\n","        'dropout_rate_pred_head': 0.2,\n","        'use_amp': False, 'enable_profiler': False, 'num_workers': 0,\n","        'pin_memory': False, 'train_split_ratio': 0.7, 'val_split_ratio': 0.2,\n","        'h0_from_first_step': True\n","    }\n","\n","    all_graph_sequences_loaded = None\n","    try:\n","        if not DATA_PATH.exists(): raise FileNotFoundError(f\"数据文件在指定路径未找到: {DATA_PATH}\")\n","        with open(DATA_PATH, \"rb\") as f: all_graph_sequences_loaded = pickle.load(f)\n","        if not all_graph_sequences_loaded or not isinstance(all_graph_sequences_loaded, list) or not all_graph_sequences_loaded[0] or not isinstance(all_graph_sequences_loaded[0], list):\n","            raise ValueError(\"加载的数据格式不正确。\")\n","\n","        expected_len_per_sequence = training_config['T_pred_horizon'] + 1\n","\n","        processed_sequences = []\n","        for i, seq in enumerate(all_graph_sequences_loaded):\n","            if not isinstance(seq, list) or len(seq) != expected_len_per_sequence: continue\n","            valid_seq = True\n","            for step_idx, graph_step_data in enumerate(seq):\n","                if not isinstance(graph_step_data, Data) or not hasattr(graph_step_data, 'x') or graph_step_data.x is None or \\\n","                   not hasattr(graph_step_data, 'edge_index') or graph_step_data.edge_index is None or \\\n","                   not hasattr(graph_step_data, 'edge_attr') or graph_step_data.edge_attr is None or \\\n","                   graph_step_data.edge_attr.shape[1] != training_config.get('gine_edge_dim', 5) or \\\n","                   not hasattr(graph_step_data, 'graph_global_env_features') or \\\n","                   (step_idx > 0 and (not hasattr(graph_step_data, 'y') or graph_step_data.y is None)):\n","                    if hasattr(graph_step_data, 'edge_attr') and graph_step_data.edge_attr is not None and \\\n","                       graph_step_data.edge_attr.shape[1] != training_config.get('gine_edge_dim', 5):\n","                       print(f\"Warning: Seq {i}, step {step_idx} has edge_attr dim {graph_step_data.edge_attr.shape[1]}, expected {training_config.get('gine_edge_dim', 5)}\")\n","                    valid_seq = False; break\n","                if step_idx > 0 and isinstance(graph_step_data.y, torch.Tensor) and graph_step_data.y.ndim == 1:\n","                    graph_step_data.y = graph_step_data.y.unsqueeze(1)\n","            if valid_seq: processed_sequences.append(seq)\n","        if not processed_sequences: raise ValueError(f\"数据处理后没有长度为 {expected_len_per_sequence} 的有效序列。\")\n","        all_graph_sequences = processed_sequences\n","        print(f\"成功加载并处理 {len(all_graph_sequences)} 个空间窗口的序列数据。\")\n","    except Exception as e: print(f\"加载或验证数据时发生错误: {e}\"); all_graph_sequences = None\n","\n","    if all_graph_sequences:\n","        base_datetime_for_timeline = dt_datetime(DATA_YEAR, DATA_MONTH, DATA_DAY, START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES)\n","        time_features_for_dataset_timeline = generate_time_features_for_sequence(\n","            base_datetime_for_timeline,\n","            training_config['T_pred_horizon']\n","        )\n","\n","        trained_model, final_node_mean, final_node_std, final_target_mean, final_target_std = main_training_gine_lstm_hourly_heads( # Renamed call\n","            all_graph_sequences, training_config, time_features_for_dataset_timeline\n","        )\n","        print(\"GINE-LSTM 模型训练和评估完成!\")\n","    else:\n","        print(\"由于数据加载失败或数据为空，训练流程未启动。\")"]},{"cell_type":"markdown","metadata":{"id":"0KS-6YucB5Lk"},"source":["##GAE+LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"vbcx75pioLOq"},"outputs":[],"source":["# ===========================================================\n","# 0. 环境 & 依赖\n","# ===========================================================\n","import os\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch_geometric.nn import RGCNConv # Using RGCNConv for C-GVAE\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.loader import DataLoader\n","import pickle\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.metrics import r2_score\n","import gc\n","import math\n","from datetime import datetime as dt_datetime, timedelta\n","from pathlib import Path\n","import time\n","import json\n","import torchprofile\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","torch.cuda.ipc_collect()\n","\n","# ===========================================================\n","# 1. 特征生成 & 辅助模块 (No changes)\n","# ===========================================================\n","\n","def generate_time_features_for_sequence(base_dt_obj, num_steps):\n","    time_features_list = []\n","    for i in range(num_steps):\n","        current_dt = base_dt_obj + timedelta(hours=i)\n","        hour_norm = current_dt.hour / 23.0\n","        day_of_year_norm = current_dt.timetuple().tm_yday / (366.0 if current_dt.year % 4 == 0 and (current_dt.year % 100 != 0 or current_dt.year % 400 == 0) else 365.0)\n","        hour_sin = math.sin(2 * math.pi * hour_norm)\n","        hour_cos = math.cos(2 * math.pi * hour_norm)\n","        doy_sin = math.sin(2 * math.pi * day_of_year_norm)\n","        doy_cos = math.cos(2 * math.pi * day_of_year_norm)\n","        time_features_list.append(torch.tensor([hour_sin, hour_cos, doy_sin, doy_cos], dtype=torch.float32))\n","    return torch.stack(time_features_list)\n","\n","\n","class MLPEncoder(nn.Module):\n","    def __init__(self, in_dim, out_dim, hid_dim=None, dropout_rate=0.1):\n","        super().__init__()\n","        if hid_dim is None:\n","            hid_dim = max(min(in_dim, out_dim), (in_dim + out_dim) // 2)\n","            if hid_dim == 0 and out_dim > 0 : hid_dim = out_dim\n","            if hid_dim == 0 and in_dim > 0 : hid_dim = in_dim\n","            if hid_dim == 0 : hid_dim = 1\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(in_dim, hid_dim),\n","            nn.ReLU(),\n","            nn.LayerNorm(hid_dim),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hid_dim, out_dim)\n","        )\n","    def forward(self, x):\n","        return self.mlp(x)\n","\n","# ===========================================================\n","# 2. C-GVAE Components (using RGCNConv)\n","# ===========================================================\n","\n","class RGCNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, num_relations, dropout_rate, use_residual=True):\n","        super().__init__()\n","        self.conv = RGCNConv(in_channels, out_channels, num_relations=num_relations)\n","        self.norm = nn.BatchNorm1d(out_channels)\n","        self.activation = nn.PReLU()\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.use_residual = use_residual\n","\n","        if self.use_residual:\n","            if in_channels == out_channels:\n","                self.residual_projection = nn.Identity()\n","            else:\n","                self.residual_projection = nn.Linear(in_channels, out_channels)\n","\n","    def forward(self, x_input, edge_index, edge_attr):\n","        if x_input.size(0) == 0: return x_input\n","\n","        # Ensure edge_attr provides relation types for RGCNConv\n","        if edge_attr is None or edge_attr.shape[1] < 1 : # Assuming relation type is at least the first column\n","             raise ValueError(\"RGCNBlock requires edge_attr with relation types.\")\n","        edge_type = edge_attr[:,0].long() # Assuming relation type is the first column if generic edge_attr passed\n","        # If your edge_attr has a specific column for relation type (e.g., index 4 from previous RGCNModule), adjust here.\n","        # For simplicity with C-GVAE, let's assume edge_attr *is* the relation type vector or its first column.\n","        # If edge_attr itself is multi-dimensional beyond just relation type, RGCNConv might not use other dimensions.\n","        # The original RGCNModule used edge_attr[:, 4].long().\n","        # Let's stick to that convention if edge_attr has 5+ columns, otherwise use column 0.\n","        if edge_attr.shape[1] >=5: # Match original RGCNModule behavior\n","            edge_type = edge_attr[:, 4].long()\n","        else: # Fallback or if edge_attr is just relation types\n","            edge_type = edge_attr[:,0].long()\n","\n","\n","        h = self.conv(x_input, edge_index, edge_type=edge_type)\n","        if h.shape[0] > 1:\n","            h = self.norm(h)\n","        h = self.activation(h)\n","        h = self.dropout(h)\n","\n","        if self.use_residual:\n","            projected_x_input = self.residual_projection(x_input)\n","            h = h + projected_x_input\n","        return h\n","\n","class Encoder_CGVAE(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers, num_relations, dropout_rate):\n","        super().__init__()\n","        layers = []\n","        current_dim = input_dim\n","        for i in range(num_layers):\n","            block_use_residual = (i > 0)\n","            layers.append(RGCNBlock(current_dim, hidden_dim, num_relations, dropout_rate,\n","                                   use_residual=block_use_residual if current_dim == hidden_dim else False))\n","            current_dim = hidden_dim\n","        self.rgcn_layers = nn.ModuleList(layers)\n","        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n","        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n","\n","    def forward(self, x, edge_index, edge_attr):\n","        for layer in self.rgcn_layers:\n","            x = layer(x, edge_index, edge_attr)\n","        mu = self.fc_mu(x)\n","        logvar = self.fc_logvar(x)\n","        return mu, logvar\n","\n","class Decoder_CGVAE(nn.Module):\n","    def __init__(self, latent_dim, hidden_dim, output_dim, original_node_feature_dim,\n","                 num_layers, num_relations, dropout_rate):\n","        super().__init__()\n","        self.initial_input_dim = latent_dim + original_node_feature_dim # z and original x\n","\n","        layers = []\n","        current_dim = self.initial_input_dim\n","        for i in range(num_layers):\n","            block_use_residual = (i > 0)\n","            layers.append(RGCNBlock(current_dim, hidden_dim, num_relations, dropout_rate,\n","                                   use_residual=block_use_residual if current_dim == hidden_dim else False))\n","            current_dim = hidden_dim\n","        self.rgcn_layers = nn.ModuleList(layers)\n","        self.fc_out = nn.Linear(hidden_dim, output_dim) # Output transformed features\n","\n","    def forward(self, z, x_original, edge_index, edge_attr):\n","        decoder_input = torch.cat([z, x_original], dim=1)\n","        x = decoder_input\n","        for layer in self.rgcn_layers:\n","            x = layer(x, edge_index, edge_attr)\n","        output_features = self.fc_out(x)\n","        return output_features\n","\n","class RGCN_CGVAE_FeatureExtractor(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, latent_dim, output_feature_dim,\n","                 num_encoder_layers, num_decoder_layers, num_relations, dropout_rate):\n","        super().__init__()\n","        self.encoder = Encoder_CGVAE(input_dim, hidden_dim, latent_dim,\n","                                     num_encoder_layers, num_relations, dropout_rate)\n","        self.decoder = Decoder_CGVAE(latent_dim, hidden_dim, output_feature_dim,\n","                                     input_dim, num_decoder_layers, num_relations, dropout_rate)\n","        self.latent_dim = latent_dim # For profiling\n","\n","    def reparameterize(self, mu, logvar):\n","        # For feature extraction in a downstream task, often mu is used directly during eval.\n","        # During VAE training, sampling is done. Here, we might always use mu if not training VAE loss.\n","        if self.training: # If you were to add KLD loss, you'd sample here\n","            std = torch.exp(0.5 * logvar)\n","            eps = torch.randn_like(std)\n","            return mu + eps * std\n","        return mu # Deterministic for feature extraction\n","\n","    def forward(self, x, edge_index, edge_attr):\n","        mu, logvar = self.encoder(x, edge_index, edge_attr)\n","\n","        # Using mu directly as 'z' for feature extraction for the downstream task.\n","        # If VAE component was trained with KLD, self.reparameterize(mu, logvar) would be used during VAE training.\n","        z = mu\n","\n","        output_features = self.decoder(z, x, edge_index, edge_attr) # Pass original x to decoder\n","        return output_features # , mu, logvar # Optionally return mu, logvar if KLD loss is used elsewhere\n","\n","# ===========================================================\n","# 2.b CGVAE-LSTM 模型定义 (Main Model)\n","# ===========================================================\n","class CGVAELSTMModelWithHourlyHeads(nn.Module): # Renamed\n","    def __init__(self,\n","                 static_node_in_dim,      # Input to CGVAE encoder\n","                 global_env_in_dim,\n","                 time_in_dim,\n","                 global_env_emb_dim,\n","                 time_emb_dim,\n","                 # CGVAE specific params (using repurposed gcn keys from config)\n","                 cgvae_hidden_dim,        # Hidden dim inside CGVAE's RGCNBlocks\n","                 cgvae_latent_dim,        # Latent dim of CGVAE\n","                 cgvae_output_dim,        # Output dim of CGVAE's decoder (input to LSTM pipeline)\n","                 cgvae_num_encoder_layers,\n","                 cgvae_num_decoder_layers,\n","                 cgvae_dropout_rate,\n","                 num_relations,           # For RGCNConv inside CGVAE\n","                 # LSTM specific params (using repurposed gru keys from config)\n","                 lstm_hidden_dim,\n","                 fusion_mlp_output_dim=None,\n","                 fusion_mlp_hidden_dim=None,\n","                 dropout_rate_fusion_mlp=0.1,\n","                 num_lstm_layers=1,\n","                 T_pred_horizon=12,\n","                 dropout_rate_encoders=0.1,\n","                 # dropout_rate_gcn/gine is now cgvae_dropout_rate\n","                 dropout_rate_lstm=0.2,\n","                 mlp_prediction_hidden_dim=64,\n","                 dropout_rate_pred_head=0.2\n","                ):\n","        super().__init__()\n","        self.T_pred_horizon = T_pred_horizon\n","        self.static_node_in_dim = static_node_in_dim\n","        self.global_env_in_dim = global_env_in_dim\n","        self.time_in_dim = time_in_dim\n","        self.cgvae_output_dim = cgvae_output_dim # Features from CGVAE\n","        self.lstm_hidden_dim = lstm_hidden_dim\n","        self.num_relations = num_relations # For profiling CGVAE component\n","\n","        # Standard Encoders\n","        self.global_env_encoder = MLPEncoder(global_env_in_dim, global_env_emb_dim, dropout_rate=dropout_rate_encoders)\n","        self.time_encoder = MLPEncoder(time_in_dim, time_emb_dim, dropout_rate=dropout_rate_encoders)\n","\n","        # MLP to process output of CGVAE's decoder before LSTM's h0/c0\n","        self.h0_c0_from_cgvae_encoder = MLPEncoder(cgvae_output_dim, lstm_hidden_dim, dropout_rate=dropout_rate_encoders)\n","\n","        # CGVAE Feature Extractors\n","        self.cgvae_module_for_h0 = RGCN_CGVAE_FeatureExtractor(\n","            input_dim=static_node_in_dim, hidden_dim=cgvae_hidden_dim, latent_dim=cgvae_latent_dim,\n","            output_feature_dim=cgvae_output_dim, num_encoder_layers=cgvae_num_encoder_layers,\n","            num_decoder_layers=cgvae_num_decoder_layers, num_relations=num_relations,\n","            dropout_rate=cgvae_dropout_rate\n","        )\n","        self.cgvae_module_for_sequence = RGCN_CGVAE_FeatureExtractor(\n","            input_dim=static_node_in_dim, hidden_dim=cgvae_hidden_dim, latent_dim=cgvae_latent_dim,\n","            output_feature_dim=cgvae_output_dim, num_encoder_layers=cgvae_num_encoder_layers,\n","            num_decoder_layers=cgvae_num_decoder_layers, num_relations=num_relations,\n","            dropout_rate=cgvae_dropout_rate\n","        )\n","\n","        concatenated_feature_dim = cgvae_output_dim + global_env_emb_dim + time_emb_dim\n","        actual_fusion_mlp_output_dim = fusion_mlp_output_dim if fusion_mlp_output_dim is not None else concatenated_feature_dim\n","        self.fusion_mlp_input_dim = concatenated_feature_dim\n","\n","        self.fusion_mlp = MLPEncoder(\n","            in_dim=concatenated_feature_dim,\n","            out_dim=actual_fusion_mlp_output_dim,\n","            hid_dim=fusion_mlp_hidden_dim,\n","            dropout_rate=dropout_rate_fusion_mlp\n","        )\n","\n","        lstm_input_size_actual = actual_fusion_mlp_output_dim\n","        self.lstm_input_dim = lstm_input_size_actual\n","\n","        self.lstm = nn.LSTM(\n","            input_size=lstm_input_size_actual,\n","            hidden_size=lstm_hidden_dim,\n","            num_layers=num_lstm_layers,\n","            batch_first=True,\n","            dropout=dropout_rate_lstm if num_lstm_layers > 1 else 0.0\n","        )\n","\n","        self.hourly_prediction_heads = nn.ModuleList()\n","        for _ in range(T_pred_horizon):\n","            self.hourly_prediction_heads.append(\n","                nn.Sequential(\n","                    nn.Linear(lstm_hidden_dim, mlp_prediction_hidden_dim),\n","                    nn.ReLU(),\n","                    nn.Dropout(dropout_rate_pred_head),\n","                    nn.Linear(mlp_prediction_hidden_dim, 1)\n","                )\n","            )\n","\n","        self.register_buffer('node_feat_mean', torch.zeros(static_node_in_dim))\n","        self.register_buffer('node_feat_std', torch.ones(static_node_in_dim))\n","\n","    def forward(self, list_of_batched_timesteps: list, timeline_time_features: torch.Tensor, device: torch.device):\n","        pyg_batch_7am = list_of_batched_timesteps[0].to(device)\n","        normalized_x_7am = (pyg_batch_7am.x - self.node_feat_mean) / (self.node_feat_std + 1e-8)\n","\n","        edge_attr_7am = pyg_batch_7am.edge_attr\n","        if edge_attr_7am is not None:\n","            edge_attr_7am = edge_attr_7am.float()\n","        else:\n","            num_edges_7am = pyg_batch_7am.edge_index.size(1)\n","            # Infer edge_attr dim for placeholder if needed (assuming relation type is the key part for RGCNBlock)\n","            # This part might need adjustment based on how edge_attr is structured for RGCNBlock's num_relations\n","            # For now, assuming a simple edge_attr of size 1 (relation type) if not present.\n","            # Or, if your RGCNBlock expects a specific dimensionality (e.g., 5 for original RGCNModule), use that.\n","            # Let's assume edge_attr for RGCNBlock needs at least 1 dim for relation type\n","            # The RGCNBlock was adapted to take edge_attr[:,4] or edge_attr[:,0]\n","            # If edge_attr is truly None, then a placeholder is needed.\n","            # For now, let's assume `edge_attr` will be present from data loading.\n","            # If it can be None, the GNN module needs to handle it (e.g. error or default behavior).\n","            # The RGCNBlock inside CGVAE will raise error if edge_attr is None and it expects types.\n","            # We will assume data loader provides edge_attr.\n","            # For safety, if it's None and num_relations>0, it's problematic.\n","            # Let's ensure the dummy data and actual data provide edge_attr if num_relations > 0.\n","            # The RGCNBlock expects edge_attr to provide relation types.\n","            if edge_attr_7am is None:\n","                raise ValueError(\"edge_attr cannot be None for RGCN_CGVAE_FeatureExtractor.\")\n","\n","\n","        # Pass original x, edge_index, edge_attr to CGVAE\n","        cgvae_output_7am = self.cgvae_module_for_h0(\n","            normalized_x_7am, # Original node features (normalized)\n","            pyg_batch_7am.edge_index,\n","            edge_attr_7am\n","        ) # This output is the transformed features from CGVAE's decoder\n","\n","        h0_features_for_lstm_nodes = self.h0_c0_from_cgvae_encoder(cgvae_output_7am)\n","\n","        h0_for_lstm = h0_features_for_lstm_nodes.unsqueeze(0)\n","        c0_for_lstm = torch.zeros_like(h0_for_lstm)\n","\n","        if self.lstm.num_layers > 1:\n","            h0_for_lstm = h0_for_lstm.repeat(self.lstm.num_layers, 1, 1)\n","            c0_for_lstm = c0_for_lstm.repeat(self.lstm.num_layers, 1, 1)\n","\n","        initial_hidden_state = (h0_for_lstm, c0_for_lstm)\n","\n","        all_lstm_input_features_over_time = []\n","        for t_pred_idx in range(self.T_pred_horizon):\n","            pyg_batch_this_timestep = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","            normalized_x = (pyg_batch_this_timestep.x - self.node_feat_mean) / (self.node_feat_std + 1e-8)\n","\n","            edge_attr_this_timestep = pyg_batch_this_timestep.edge_attr\n","            if edge_attr_this_timestep is not None:\n","                edge_attr_this_timestep = edge_attr_this_timestep.float()\n","            else:\n","                raise ValueError(\"edge_attr cannot be None for RGCN_CGVAE_FeatureExtractor in sequence.\")\n","\n","\n","            cgvae_output_nodes_t = self.cgvae_module_for_sequence(\n","                normalized_x, # Original node features (normalized) for this timestep\n","                pyg_batch_this_timestep.edge_index,\n","                edge_attr_this_timestep\n","            )\n","\n","            global_env_feat_t_unencoded = pyg_batch_this_timestep.graph_global_env_features\n","            # ... (rest of global_env and time feature processing remains the same)\n","            expected_num_graphs_in_batch_t = pyg_batch_this_timestep.num_graphs\n","            expected_global_features_dim = self.global_env_encoder.mlp[0].in_features\n","            if not (global_env_feat_t_unencoded.shape == (expected_num_graphs_in_batch_t, expected_global_features_dim)):\n","                if global_env_feat_t_unencoded.ndim == 1 and \\\n","                   global_env_feat_t_unencoded.shape[0] == expected_num_graphs_in_batch_t * expected_global_features_dim:\n","                    global_env_feat_t_unencoded = global_env_feat_t_unencoded.view(expected_num_graphs_in_batch_t, expected_global_features_dim)\n","                elif global_env_feat_t_unencoded.numel() == expected_num_graphs_in_batch_t * expected_global_features_dim:\n","                    global_env_feat_t_unencoded = global_env_feat_t_unencoded.view(expected_num_graphs_in_batch_t, expected_global_features_dim)\n","                else:\n","                    print(f\"Warning: LSTM Input Time {t_pred_idx}: Correcting global_env_feat shape from {global_env_feat_t_unencoded.shape} to ({expected_num_graphs_in_batch_t}, {expected_global_features_dim}) with zeros due to mismatch.\")\n","                    global_env_feat_t_unencoded = torch.zeros(expected_num_graphs_in_batch_t, expected_global_features_dim, device=device)\n","            global_env_emb_t = self.global_env_encoder(global_env_feat_t_unencoded)\n","            global_env_emb_t_expanded = global_env_emb_t[pyg_batch_this_timestep.batch]\n","\n","            current_raw_time_feat_for_timestep_t = timeline_time_features[t_pred_idx, :].to(device)\n","            current_emb_time_feat_for_timestep_t = self.time_encoder(current_raw_time_feat_for_timestep_t)\n","            num_nodes_in_pyg_batch = pyg_batch_this_timestep.num_nodes\n","            time_emb_t_expanded_to_nodes = current_emb_time_feat_for_timestep_t.unsqueeze(0).expand(num_nodes_in_pyg_batch, -1)\n","\n","            concatenated_features_for_timestep_t = torch.cat([cgvae_output_nodes_t, global_env_emb_t_expanded, time_emb_t_expanded_to_nodes], dim=-1)\n","            fused_features_for_timestep_t = self.fusion_mlp(concatenated_features_for_timestep_t)\n","            all_lstm_input_features_over_time.append(fused_features_for_timestep_t)\n","\n","        stacked_lstm_input_features = torch.stack(all_lstm_input_features_over_time, dim=1)\n","\n","        if initial_hidden_state[0].shape[1] != stacked_lstm_input_features.shape[0]:\n","            print(f\"CRITICAL WARNING: Node count mismatch for LSTM h0/c0 ({initial_hidden_state[0].shape[1]}) and LSTM input sequence ({stacked_lstm_input_features.shape[0]}).\")\n","            if initial_hidden_state[0].shape[1] > stacked_lstm_input_features.shape[0]:\n","                h0_adj = initial_hidden_state[0][:, :stacked_lstm_input_features.shape[0], :]\n","                c0_adj = initial_hidden_state[1][:, :stacked_lstm_input_features.shape[0], :]\n","                initial_hidden_state = (h0_adj, c0_adj)\n","\n","        lstm_out, _ = self.lstm(stacked_lstm_input_features, initial_hidden_state)\n","\n","        all_hourly_final_predictions_scaled = []\n","        for t in range(self.T_pred_horizon):\n","            lstm_out_t = lstm_out[:, t, :]\n","            prediction_t_scaled = self.hourly_prediction_heads[t](lstm_out_t)\n","            all_hourly_final_predictions_scaled.append(prediction_t_scaled.squeeze(-1))\n","\n","        predictions_scaled = torch.stack(all_hourly_final_predictions_scaled, dim=1)\n","        return predictions_scaled\n","\n","# ===========================================================\n","# 3. 评估指标函数 (在原始尺度上计算指标) - NO CHANGES\n","# ===========================================================\n","# ... (mse_loss_masked, calculate_hourly_metrics are identical) ...\n","def mse_loss_masked(predictions_scaled, targets_scaled, mask):\n","    expanded_mask = mask.unsqueeze(1).expand_as(targets_scaled)\n","    valid_targets_mask = ~torch.isnan(targets_scaled)\n","    final_mask = expanded_mask & valid_targets_mask\n","    if final_mask.sum() == 0:\n","        return torch.tensor(0.0, device=predictions_scaled.device, requires_grad=True)\n","    loss = F.mse_loss(predictions_scaled[final_mask], targets_scaled[final_mask])\n","    return loss\n","\n","def calculate_hourly_metrics(predictions_scaled, targets_scaled, node_masks, target_mean, target_std):\n","    target_mean_cpu = target_mean.cpu()\n","    target_std_cpu = target_std.cpu()\n","    preds_unscaled = predictions_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","    targets_unscaled = targets_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","\n","    num_nodes_total, T_horizon = preds_unscaled.shape\n","    hourly_metrics_dict = {}\n","\n","    preds_np = preds_unscaled.numpy()\n","    targets_np = targets_unscaled.numpy()\n","    mask_np = node_masks.cpu().numpy()\n","\n","    for t in range(T_horizon):\n","        preds_t_all_nodes = preds_np[:, t]\n","        targets_t_all_nodes = targets_np[:, t]\n","        preds_t_on_loss_nodes = preds_t_all_nodes[mask_np]\n","        targets_t_on_loss_nodes = targets_t_all_nodes[mask_np]\n","        valid_target_data_mask_t = ~np.isnan(targets_t_on_loss_nodes)\n","        preds_t_final_valid = preds_t_on_loss_nodes[valid_target_data_mask_t]\n","        targets_t_final_valid = targets_t_on_loss_nodes[valid_target_data_mask_t]\n","\n","        if preds_t_final_valid.shape[0] < 2:\n","            hourly_metrics_dict[t] = {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count': 0}\n","            continue\n","        mse = np.mean((preds_t_final_valid - targets_t_final_valid)**2)\n","        mae = np.mean(np.abs(preds_t_final_valid - targets_t_final_valid))\n","        rmse = np.sqrt(mse)\n","        try:\n","            r2 = r2_score(targets_t_final_valid, preds_t_final_valid)\n","        except ValueError:\n","            r2 = np.nan\n","        hourly_metrics_dict[t] = {'mse': mse, 'mae': mae, 'rmse': rmse, 'r2': r2, 'count': preds_t_final_valid.shape[0]}\n","    return hourly_metrics_dict\n","\n","# ===========================================================\n","# 4. 训练与评估循环 (适配y归一化) - NO CHANGES\n","# ===========================================================\n","# ... (train_epoch, evaluate_epoch are identical) ...\n","def train_epoch(model, loader, optimizer, device, timeline_time_features,\n","                node_feat_mean, node_feat_std, target_mean, target_std):\n","    model.train()\n","    total_loss_scaled = 0\n","    num_sequences_processed = 0\n","    model.node_feat_mean = node_feat_mean.to(device)\n","    model.node_feat_std = node_feat_std.to(device)\n","\n","    target_mean_dev = target_mean.to(device)\n","    target_std_dev = target_std.to(device)\n","\n","    epoch_start_time = time.time()\n","    for list_of_batched_timesteps in loader:\n","        optimizer.zero_grad()\n","        predictions_batch_scaled = model(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","\n","        targets_list_for_loss_scaled = []\n","        first_predicted_timestep_batch = list_of_batched_timesteps[1].to(device)\n","        mask_for_loss = ~first_predicted_timestep_batch.building_mask\n","\n","        for t_pred_idx in range(model.T_pred_horizon):\n","            current_target_timestep_batch = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","            targets_t_nodes_original = current_target_timestep_batch.y.squeeze()\n","            targets_t_nodes_scaled = (targets_t_nodes_original - target_mean_dev) / (target_std_dev + 1e-8)\n","            targets_list_for_loss_scaled.append(targets_t_nodes_scaled)\n","\n","        targets_batch_scaled = torch.stack(targets_list_for_loss_scaled, dim=1)\n","        loss = mse_loss_masked(predictions_batch_scaled, targets_batch_scaled, mask_for_loss)\n","        num_sequences_in_this_super_batch = list_of_batched_timesteps[0].num_graphs\n","\n","        if not torch.isnan(loss) and not torch.isinf(loss) and loss.item() > 0 :\n","            loss.backward()\n","            optimizer.step()\n","            total_loss_scaled += loss.item() * num_sequences_in_this_super_batch\n","        num_sequences_processed += num_sequences_in_this_super_batch\n","\n","    epoch_duration = time.time() - epoch_start_time\n","    avg_loss = total_loss_scaled / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","    return avg_loss, epoch_duration\n","\n","def evaluate_epoch(model, loader, device, timeline_time_features,\n","                   node_feat_mean, node_feat_std, target_mean, target_std, epoch_type=\"Eval\"):\n","    model.eval()\n","    all_batch_predictions_scaled = []\n","    all_batch_targets_scaled = []\n","    all_batch_masks_for_metrics = []\n","    total_loss_scaled = 0\n","    num_sequences_processed = 0\n","    model.node_feat_mean = node_feat_mean.to(device)\n","    model.node_feat_std = node_feat_std.to(device)\n","\n","    target_mean_dev = target_mean.to(device)\n","    target_std_dev = target_std.to(device)\n","\n","    eval_start_time = time.time()\n","    with torch.no_grad():\n","        for list_of_batched_timesteps in loader:\n","            predictions_batch_scaled = model(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","            targets_list_original = []\n","            first_predicted_timestep_batch = list_of_batched_timesteps[1].to(device)\n","            mask_for_metrics = ~first_predicted_timestep_batch.building_mask\n","\n","            for t_pred_idx in range(model.T_pred_horizon):\n","                current_target_timestep_batch = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","                targets_t_nodes_original = current_target_timestep_batch.y.squeeze()\n","                targets_list_original.append(targets_t_nodes_original)\n","\n","            targets_batch_original = torch.stack(targets_list_original, dim=1)\n","            targets_batch_scaled_for_loss = (targets_batch_original - target_mean_dev) / (target_std_dev + 1e-8)\n","            loss = mse_loss_masked(predictions_batch_scaled, targets_batch_scaled_for_loss, mask_for_metrics)\n","            num_sequences_in_this_super_batch = list_of_batched_timesteps[0].num_graphs\n","\n","            if not torch.isnan(loss) and not torch.isinf(loss):\n","                total_loss_scaled += loss.item() * num_sequences_in_this_super_batch\n","            num_sequences_processed += num_sequences_in_this_super_batch\n","\n","            all_batch_predictions_scaled.append(predictions_batch_scaled.cpu())\n","            all_batch_targets_scaled.append(targets_batch_scaled_for_loss.cpu())\n","            all_batch_masks_for_metrics.append(mask_for_metrics.cpu())\n","\n","    eval_duration = time.time() - eval_start_time\n","    avg_loss_scaled = total_loss_scaled / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","\n","    if not all_batch_predictions_scaled:\n","        empty_metrics = {t: {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0} for t in range(model.T_pred_horizon)}\n","        return avg_loss_scaled, empty_metrics, eval_duration\n","\n","    final_predictions_scaled = torch.cat(all_batch_predictions_scaled, dim=0)\n","    final_targets_scaled = torch.cat(all_batch_targets_scaled, dim=0)\n","    final_masks_for_metrics = torch.cat(all_batch_masks_for_metrics, dim=0)\n","\n","    hourly_metrics_original_scale = calculate_hourly_metrics(final_predictions_scaled, final_targets_scaled,\n","                                                             final_masks_for_metrics, target_mean.cpu(), target_std.cpu())\n","    return avg_loss_scaled, hourly_metrics_original_scale, eval_duration\n","\n","# ===========================================================\n","# 5. 主训练流程 (CGVAE-LSTM)\n","# ===========================================================\n","\n","def calculate_aggregated_metrics_report(hourly_metrics_dict, T_pred_horizon):\n","    # ... (identical) ...\n","    metrics_to_aggregate = ['r2', 'mse', 'mae', 'rmse']\n","    aggregated_report = {}\n","    for metric_name in metrics_to_aggregate:\n","        values = [hourly_metrics_dict[t][metric_name] for t in range(T_pred_horizon) if t in hourly_metrics_dict and not np.isnan(hourly_metrics_dict[t][metric_name])]\n","        if values:\n","            aggregated_report[f'avg_{metric_name}'] = np.mean(values)\n","            aggregated_report[f'std_{metric_name}'] = np.std(values)\n","        else:\n","            aggregated_report[f'avg_{metric_name}'] = np.nan\n","            aggregated_report[f'std_{metric_name}'] = np.nan\n","    return aggregated_report\n","\n","def main_training_cgvae_lstm_hourly_heads( # Renamed\n","    all_sequences_data: list,\n","    config: dict,\n","    time_features_for_dataset: torch.Tensor\n","):\n","    train_start_time = time.time()\n","    report_data = {'config': config}\n","\n","    seed = config.get('seed', 42)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"使用设备: {device}\")\n","    report_data['device'] = str(device)\n","\n","    T_PRED_HORIZON = config.get('T_pred_horizon', 12)\n","    expected_input_len = T_PRED_HORIZON + 1\n","    valid_sequences_data = [seq for seq in all_sequences_data if isinstance(seq, list) and len(seq) == expected_input_len]\n","    if len(valid_sequences_data) != len(all_sequences_data):\n","        print(f\"警告: 从 {len(all_sequences_data)} 个序列中筛选出 {len(valid_sequences_data)} 个长度为 {expected_input_len} 的有效序列。\")\n","    if not valid_sequences_data:\n","        raise ValueError(f\"没有找到长度为 {expected_input_len} 的有效序列数据。\")\n","    all_sequences_data = valid_sequences_data\n","\n","    # Dataset split\n","    num_total_sequences = len(all_sequences_data)\n","    indices = np.random.permutation(num_total_sequences)\n","    train_split_ratio = config.get('train_split_ratio', 0.7)\n","    val_split_ratio = config.get('val_split_ratio', 0.2)\n","    train_size = int(train_split_ratio * num_total_sequences)\n","    val_size = int(val_split_ratio * num_total_sequences)\n","    train_indices = indices[:train_size]\n","    val_indices = indices[train_size : train_size + val_size]\n","    test_indices = indices[train_size + val_size :]\n","    train_dataset = [all_sequences_data[i] for i in train_indices]\n","    val_dataset   = [all_sequences_data[i] for i in val_indices]\n","    test_dataset  = [all_sequences_data[i] for i in test_indices]\n","    report_data['dataset_split'] = {'total_sequences': num_total_sequences, 'train_size': len(train_dataset), 'val_size': len(val_dataset), 'test_size': len(test_dataset)}\n","\n","    # Scaler calculation\n","    all_train_node_features_list = []\n","    all_train_target_values_list_for_scaling = []\n","    for seq in train_dataset:\n","        for i_step, graph_data in enumerate(seq):\n","            if hasattr(graph_data, 'x') and graph_data.x is not None:\n","                all_train_node_features_list.append(graph_data.x)\n","            if i_step > 0 and hasattr(graph_data, 'y') and graph_data.y is not None:\n","                y_original = graph_data.y.squeeze(); current_mask_for_loss = ~graph_data.building_mask\n","                valid_target_indices = current_mask_for_loss & ~torch.isnan(y_original)\n","                if valid_target_indices.sum() > 0:\n","                    all_train_target_values_list_for_scaling.append(y_original[valid_target_indices])\n","\n","    if not all_train_node_features_list: raise ValueError(\"训练数据中未找到节点特征 'x'，无法计算scaler！\")\n","    all_train_node_features_tensor = torch.cat(all_train_node_features_list, dim=0)\n","    node_feat_mean = torch.mean(all_train_node_features_tensor, dim=0); node_feat_std = torch.std(all_train_node_features_tensor, dim=0)\n","    node_feat_std[node_feat_std < 1e-8] = 1.0\n","    scaler_path_x = Path(config['results_dir']) / \"node_feature_scaler_cgvae_lstm.pth\" # Renamed\n","    torch.save({'mean': node_feat_mean, 'std': node_feat_std}, scaler_path_x); print(f\"节点特征x scaler已保存到: {scaler_path_x}\")\n","\n","    if not all_train_target_values_list_for_scaling:\n","        target_mean = torch.tensor(0.0); target_std = torch.tensor(1.0)\n","    else:\n","        all_train_target_values_tensor = torch.cat(all_train_target_values_list_for_scaling, dim=0)\n","        target_mean = torch.mean(all_train_target_values_tensor.float()); target_std = torch.std(all_train_target_values_tensor.float())\n","        if target_std < 1e-8: target_std = torch.tensor(1.0)\n","    target_scaler_path = Path(config['results_dir']) / \"target_scaler_cgvae_lstm.pth\" # Renamed\n","    torch.save({'mean': target_mean, 'std': target_std}, target_scaler_path); print(f\"目标值y scaler已保存到: {target_scaler_path}\")\n","\n","    # DataLoaders\n","    batch_size = config.get('batch_size', 8); num_workers = config.get('num_workers', 0)\n","    pin_memory_flag = config.get('pin_memory', False) and device.type == 'cuda'\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=num_workers, pin_memory=pin_memory_flag)\n","    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n","    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n","\n","    # Model Initialization\n","    sample_graph_7am_for_dims = all_sequences_data[0][0]\n","    static_node_in_dim = sample_graph_7am_for_dims.x.shape[1]\n","    global_env_in_dim = sample_graph_7am_for_dims.graph_global_env_features.shape[0] if sample_graph_7am_for_dims.graph_global_env_features.ndim == 1 else sample_graph_7am_for_dims.graph_global_env_features.shape[1]\n","    time_in_dim = time_features_for_dataset.shape[1]\n","    num_relations = config.get('num_relations', 5) # Needed for RGCNBlocks in CGVAE\n","\n","    model = CGVAELSTMModelWithHourlyHeads(\n","        static_node_in_dim=static_node_in_dim, global_env_in_dim=global_env_in_dim, time_in_dim=time_in_dim,\n","        global_env_emb_dim=config.get('global_env_emb_dim', 16), time_emb_dim=config.get('time_emb_dim', 8),\n","\n","        cgvae_hidden_dim=config.get('gcn_hidden_dim', 128),       # Repurposed from 'gcn_hidden_dim'\n","        cgvae_latent_dim=config.get('cgvae_latent_dim', 64),     # New config, e.g., 64\n","        cgvae_output_dim=config.get('gcn_output_dim', 128),     # Repurposed, output of CGVAE decoder\n","        cgvae_num_encoder_layers=config.get('cgvae_num_encoder_layers', 2), # New\n","        cgvae_num_decoder_layers=config.get('cgvae_num_decoder_layers', 2), # New\n","        cgvae_dropout_rate=config.get('dropout_rate_gcn', 0.3), # Repurposed\n","        num_relations=num_relations,\n","\n","        lstm_hidden_dim=config.get('gru_hidden_dim', 128),\n","        fusion_mlp_output_dim=config.get('fusion_mlp_output_dim', 128),\n","        fusion_mlp_hidden_dim=config.get('fusion_mlp_hidden_dim', 64),\n","        dropout_rate_fusion_mlp=config.get('dropout_rate_fusion_mlp', 0.2),\n","        num_lstm_layers=config.get('num_gru_layers', 1),\n","        T_pred_horizon=T_PRED_HORIZON,\n","        dropout_rate_encoders=config.get('dropout_rate_encoders', 0.1),\n","        dropout_rate_lstm=config.get('dropout_rate_gru', 0.2),\n","        mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim', 64),\n","        dropout_rate_pred_head=config.get('dropout_rate_pred_head', 0.2)\n","    ).to(device)\n","\n","    model.node_feat_mean = node_feat_mean.to(device); model.node_feat_std = node_feat_std.to(device)\n","    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"模型总参数量 (CGVAE-LSTM): {total_params:,}\"); report_data['model_total_parameters'] = total_params\n","\n","    # ===== Component FLOPS Calculation =====\n","    print(\"\\nCalculating MACs for model components (approximate FLOPS):\")\n","    report_data['component_gmacs'] = {}\n","    dummy_nodes_component = 2500\n","    dummy_edges_component = 60000\n","    dummy_batch_global_comp = 1\n","    model.eval()\n","\n","    # 1. RGCN_CGVAE_FeatureExtractor (Profile one instance)\n","    try:\n","        cgvae_to_profile = model.cgvae_module_for_h0\n","        dummy_x_cgvae = torch.randn(dummy_nodes_component, model.static_node_in_dim, device=device)\n","        dummy_ei_cgvae = torch.randint(0, dummy_nodes_component, (2, dummy_edges_component), device=device)\n","        # Edge_attr for RGCNConv within CGVAE needs relation types.\n","        # Assuming edge_attr from data has at least 5 columns, and col 4 is relation type.\n","        # If not, adjust dummy_ea_cgvae and RGCNBlock's edge_type extraction.\n","        dummy_ea_cgvae = torch.randn(dummy_edges_component, 5, device=device) # Min 5 cols for previous RGCNBlock\n","        dummy_ea_cgvae[:, 4] = torch.randint(0, model.num_relations, (dummy_edges_component,), device=device).float()\n","\n","        # It's hard to profile the whole CGVAE if reparameterize uses training flag.\n","        # Let's profile encoder and decoder parts containing RGCNBlocks.\n","        # For simplicity, we'll try to profile the whole cgvae_to_profile.\n","        # If it fails, we'll need to sum parts.\n","        # To make it deterministic for profiling, temporarily override reparameterize or ensure model.training=False\n","\n","        # Store original training state\n","        original_training_state = cgvae_to_profile.training\n","        cgvae_to_profile.eval() # Ensure deterministic path (use mu)\n","\n","        macs_cgvae = torchprofile.profile_macs(cgvae_to_profile, args=(dummy_x_cgvae, dummy_ei_cgvae, dummy_ea_cgvae))\n","\n","        cgvae_to_profile.train(original_training_state) # Restore original state\n","\n","        report_data['component_gmacs']['rgcn_cgvae_feature_extractor'] = macs_cgvae / 1e9\n","        print(f\"  RGCN_CGVAE_FeatureExtractor GMACs: {macs_cgvae / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling RGCN_CGVAE_FeatureExtractor: {e}\")\n","        report_data['component_gmacs']['rgcn_cgvae_feature_extractor'] = \"Error\"\n","        if hasattr(cgvae_to_profile, 'train'): # Ensure it's a nn.Module\n","            cgvae_to_profile.train(original_training_state) # Restore original state on error too\n","\n","\n","    # 2. LSTM Layer (Manual MAC Calculation)\n","    # ... (LSTM MAC calculation code identical to GINE-LSTM version) ...\n","    print(f\"  Manually Calculating MACs for LSTM Layer:\")\n","    try:\n","        lstm_layer = model.lstm\n","        N_nodes = dummy_nodes_component\n","        L_seq = T_PRED_HORIZON\n","        H_in = lstm_layer.input_size\n","        H_hidden = lstm_layer.hidden_size\n","        num_layers = lstm_layer.num_layers\n","        macs_lstm_manual = 0\n","        macs_lstm_manual = N_nodes * L_seq * 4 * (H_in * H_hidden + H_hidden * H_hidden)\n","        if num_layers > 1:\n","            macs_lstm_manual += N_nodes * L_seq * (num_layers - 1) * 4 * (H_hidden * H_hidden + H_hidden * H_hidden)\n","        gmacs_lstm_manual = macs_lstm_manual / 1e9\n","        report_data['component_gmacs']['lstm_layer'] = gmacs_lstm_manual\n","        report_data['component_gmacs']['lstm_layer_profiling_notes'] = \"Manually calculated based on formula.\"\n","        print(f\"  LSTM Parameters: input_size={H_in}, hidden_size={H_hidden}, num_layers={num_layers}\")\n","        print(f\"  Used for calculation: N_nodes={N_nodes}, L_seq={L_seq}\")\n","        print(f\"  LSTM Layer GMACs (Manual): {gmacs_lstm_manual:.4f} (for sequence length {L_seq})\")\n","    except Exception as e:\n","        print(f\"  Error manually calculating LSTM Layer MACs: {e}\")\n","        report_data['component_gmacs']['lstm_layer'] = \"Error\"\n","        report_data['component_gmacs']['lstm_layer_profiling_notes'] = f\"Error during manual calculation: {str(e)}\"\n","\n","\n","    # 3. Fusion MLP, 4. Prediction Head, 5. Global Env Encoder, 6. Time Encoder\n","    try:\n","        fusion_mlp_to_profile = model.fusion_mlp\n","        dummy_input_fusion_mlp = torch.randn(dummy_nodes_component, model.fusion_mlp_input_dim, device=device)\n","        macs_fusion_mlp = torchprofile.profile_macs(fusion_mlp_to_profile, args=(dummy_input_fusion_mlp,))\n","        report_data['component_gmacs']['fusion_mlp'] = macs_fusion_mlp / 1e9\n","        print(f\"  Fusion MLP GMACs: {macs_fusion_mlp / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Fusion MLP: {e}\"); report_data['component_gmacs']['fusion_mlp'] = \"Error\"\n","\n","    try:\n","        pred_head_to_profile = model.hourly_prediction_heads[0]\n","        dummy_input_pred_head = torch.randn(dummy_nodes_component, model.lstm_hidden_dim, device=device)\n","        macs_pred_head = torchprofile.profile_macs(pred_head_to_profile, args=(dummy_input_pred_head,))\n","        report_data['component_gmacs']['prediction_head_mlp'] = macs_pred_head / 1e9\n","        print(f\"  Prediction Head MLP (single hour) GMACs: {macs_pred_head / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Prediction Head: {e}\"); report_data['component_gmacs']['prediction_head_mlp'] = \"Error\"\n","\n","    try:\n","        encoder_to_profile = model.global_env_encoder\n","        dummy_input_encoder = torch.randn(dummy_batch_global_comp, model.global_env_in_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['global_env_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  Global Env Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Global Env Encoder: {e}\"); report_data['component_gmacs']['global_env_encoder_mlp'] = \"Error\"\n","\n","    try:\n","        encoder_to_profile = model.time_encoder\n","        dummy_input_encoder = torch.randn(dummy_batch_global_comp, model.time_in_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['time_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  Time Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling Time Encoder: {e}\"); report_data['component_gmacs']['time_encoder_mlp'] = \"Error\"\n","\n","    # 7. H0/C0 from CGVAE Encoder (MLPEncoder)\n","    try:\n","        encoder_to_profile = model.h0_c0_from_cgvae_encoder\n","        dummy_input_encoder = torch.randn(dummy_nodes_component, model.cgvae_output_dim, device=device) # Input is cgvae_output_dim\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['h0_c0_from_cgvae_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  H0/C0 from CGVAE Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling H0/C0 from CGVAE Encoder: {e}\"); report_data['component_gmacs']['h0_c0_from_cgvae_encoder_mlp'] = \"Error\"\n","\n","    model.train()\n","    # ===== End Component FLOPS Calculation =====\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config.get('lr', 0.001), weight_decay=config.get('weight_decay', 1e-5))\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=config.get('scheduler_patience', 20), verbose=True)\n","\n","    best_val_loss_scaled = float('inf'); best_val_hourly_metrics_original = None; best_epoch = 0\n","    patience_counter = 0; max_epochs = config.get('max_epochs', 300); early_stopping_patience = config.get('early_stopping_patience', 45)\n","    model_save_path = Path(config['results_dir']) / f\"best_cgvaelstm_hourly_heads_model_seed{seed}.pth\" # Renamed\n","    timeline_time_features_on_device = time_features_for_dataset.to(device)\n","    target_mean_cpu = target_mean.cpu(); target_std_cpu = target_std.cpu()\n","    target_mean_on_device = target_mean.to(device); target_std_on_device = target_std.to(device)\n","    epoch_times = []\n","\n","    for epoch in range(1, max_epochs + 1):\n","        train_loss_scaled, epoch_duration = train_epoch(model, train_loader, optimizer, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_on_device, target_std_on_device)\n","        epoch_times.append(epoch_duration)\n","        val_loss_scaled, val_hourly_metrics_original, _ = evaluate_epoch(model, val_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Validation\")\n","        scheduler.step(val_loss_scaled)\n","        print(f\"Epoch {epoch:03d} | Train Scaled MSE: {train_loss_scaled:.4f} | Val Scaled MSE: {val_loss_scaled:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f} | Epoch Time: {epoch_duration:.2f}s\")\n","        _print_hourly_metrics_summary(\"Val\", val_hourly_metrics_original, T_PRED_HORIZON, indent=\"                     \")\n","        if val_loss_scaled < best_val_loss_scaled:\n","            best_val_loss_scaled = val_loss_scaled; best_val_hourly_metrics_original = val_hourly_metrics_original; best_epoch = epoch\n","            patience_counter = 0; torch.save(model.state_dict(), model_save_path)\n","            print(f\"                     ---> Best model saved (Epoch: {epoch}, Val Scaled MSE: {best_val_loss_scaled:.4f})\")\n","        else:\n","            patience_counter += 1\n","        if patience_counter >= early_stopping_patience:\n","            print(f\"Early stopping at epoch {epoch} due to no improvement.\"); break\n","\n","    total_training_duration = time.time() - train_start_time\n","    report_data['total_training_time_seconds'] = total_training_duration\n","    report_data['average_epoch_time_seconds'] = np.mean(epoch_times) if epoch_times else np.nan\n","    report_data['num_epochs_trained'] = epoch; report_data['best_validation_epoch'] = best_epoch\n","    report_data['best_validation_scaled_mse'] = best_val_loss_scaled\n","\n","    model_for_eval = CGVAELSTMModelWithHourlyHeads( # Use CGVAELSTMModel\n","        static_node_in_dim=static_node_in_dim, global_env_in_dim=global_env_in_dim, time_in_dim=time_in_dim,\n","        global_env_emb_dim=config.get('global_env_emb_dim', 16), time_emb_dim=config.get('time_emb_dim', 8),\n","        cgvae_hidden_dim=config.get('gcn_hidden_dim', 128),\n","        cgvae_latent_dim=config.get('cgvae_latent_dim', 64),\n","        cgvae_output_dim=config.get('gcn_output_dim', 128),\n","        cgvae_num_encoder_layers=config.get('cgvae_num_encoder_layers', 2),\n","        cgvae_num_decoder_layers=config.get('cgvae_num_decoder_layers', 2),\n","        cgvae_dropout_rate=config.get('dropout_rate_gcn', 0.3),\n","        num_relations=num_relations,\n","        lstm_hidden_dim=config.get('gru_hidden_dim', 128),\n","        fusion_mlp_output_dim=config.get('fusion_mlp_output_dim', 128),\n","        fusion_mlp_hidden_dim=config.get('fusion_mlp_hidden_dim', 64),\n","        dropout_rate_fusion_mlp=config.get('dropout_rate_fusion_mlp', 0.2),\n","        num_lstm_layers=config.get('num_gru_layers', 1), T_pred_horizon=T_PRED_HORIZON,\n","        dropout_rate_encoders=config.get('dropout_rate_encoders', 0.1),\n","        dropout_rate_lstm=config.get('dropout_rate_gru', 0.2),\n","        mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim', 64),\n","        dropout_rate_pred_head=config.get('dropout_rate_pred_head', 0.2)\n","    ).to(device)\n","    try: model_for_eval.load_state_dict(torch.load(model_save_path, map_location=device))\n","    except Exception as e: print(f\"无法加载最佳模型 ({e})，将使用训练循环结束时的模型。\"); model_for_eval = model\n","\n","    print(\"\\n评估最佳模型在训练集上...\"); best_model_train_loss_scaled, best_model_train_hourly_metrics, train_eval_duration = evaluate_epoch(model_for_eval, train_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Best Model on Train\")\n","    report_data['best_model_train_set_metrics_hourly'] = best_model_train_hourly_metrics; report_data['best_model_train_set_metrics_aggregated'] = calculate_aggregated_metrics_report(best_model_train_hourly_metrics, T_PRED_HORIZON)\n","    report_data['best_model_train_set_eval_time_seconds'] = train_eval_duration; _print_hourly_metrics_summary(\"最佳模型训练集\", best_model_train_hourly_metrics, T_PRED_HORIZON)\n","\n","    report_data['best_model_validation_set_metrics_hourly'] = best_val_hourly_metrics_original\n","    if best_val_hourly_metrics_original: report_data['best_model_validation_set_metrics_aggregated'] = calculate_aggregated_metrics_report(best_val_hourly_metrics_original, T_PRED_HORIZON)\n","    else:\n","        _, reeval_val_metrics, val_eval_duration = evaluate_epoch(model_for_eval, val_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Best Model on Val (Re-eval)\")\n","        report_data['best_model_validation_set_metrics_hourly'] = reeval_val_metrics\n","        report_data['best_model_validation_set_metrics_aggregated'] = calculate_aggregated_metrics_report(reeval_val_metrics, T_PRED_HORIZON)\n","        report_data['best_model_validation_set_eval_time_seconds'] = val_eval_duration\n","    _print_hourly_metrics_summary(\"最佳模型验证集\", report_data['best_model_validation_set_metrics_hourly'], T_PRED_HORIZON)\n","\n","\n","    print(\"\\n评估最佳模型在测试集上...\"); test_loss_scaled, test_hourly_metrics_original, test_inference_duration = evaluate_epoch(model_for_eval, test_loader, device, timeline_time_features_on_device, node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Test\")\n","    report_data['test_set_inference_time_seconds'] = test_inference_duration; report_data['best_model_test_set_metrics_hourly'] = test_hourly_metrics_original; report_data['best_model_test_set_metrics_aggregated'] = calculate_aggregated_metrics_report(test_hourly_metrics_original, T_PRED_HORIZON)\n","    print(\"\\n\" + \"=\"*20 + \" 最终测试集评估结果 (CGVAE-LSTM) \" + \"=\"*20); print(f\"平均测试 Scaled MSE: {test_loss_scaled:.4f}\") # Renamed\n","    _print_hourly_metrics_summary(\"测试集\", test_hourly_metrics_original, T_PRED_HORIZON)\n","\n","    agg_test = report_data['best_model_test_set_metrics_aggregated']\n","    print(f\"平均测试 MSE (Orig) : {agg_test.get('avg_mse', np.nan):.4f} (Std: {agg_test.get('std_mse', np.nan):.4f})\")\n","    print(f\"平均测试 R2 (Orig)  : {agg_test.get('avg_r2', np.nan):.4f} (Std: {agg_test.get('std_r2', np.nan):.4f})\")\n","    print(f\"平均测试 MAE (Orig) : {agg_test.get('avg_mae', np.nan):.4f} (Std: {agg_test.get('std_mae', np.nan):.4f})\")\n","    print(f\"平均测试 RMSE (Orig): {agg_test.get('avg_rmse', np.nan):.4f} (Std: {agg_test.get('std_rmse', np.nan):.4f})\")\n","    print(\"=\"*70)\n","\n","    report_file_path = Path(config['results_dir']) / f\"training_report_cgvae_lstm_seed{seed}.json\" # Renamed\n","    try:\n","        class NpEncoder(json.JSONEncoder):\n","            def default(self, obj):\n","                if isinstance(obj, np.integer): return int(obj)\n","                if isinstance(obj, np.floating): return float(obj)\n","                if isinstance(obj, np.ndarray): return obj.tolist()\n","                if isinstance(obj, torch.Tensor): return obj.tolist()\n","                if isinstance(obj, Path): return str(obj)\n","                return super(NpEncoder, self).default(obj)\n","        with open(report_file_path, 'w') as f: json.dump(report_data, f, indent=4, cls=NpEncoder)\n","        print(f\"训练报告已保存到: {report_file_path}\")\n","    except Exception as e: print(f\"保存训练报告失败: {e}\")\n","\n","    return model_for_eval, node_feat_mean, node_feat_std, target_mean, target_std\n","\n","def _print_hourly_metrics_summary(set_name, hourly_metrics, T_pred_horizon, indent=\"  \"):\n","    # ... (identical) ...\n","    if hourly_metrics is None:\n","        print(f\"{indent}{set_name} metrics not available.\")\n","        return\n","    print(f\"\\n{indent}每小时 {set_name} 指标 (Original Scale):\")\n","    header_printed = False\n","    for hour_idx in range(T_pred_horizon):\n","        metrics = hourly_metrics.get(hour_idx, {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0})\n","        if not header_printed:\n","            print(f\"{indent}  Hour | {'R2':>13s} | {'MSE':>14s} | {'MAE':>14s} | {'RMSE':>15s} | {'Count':>7s}\")\n","            header_printed = True\n","        print(f\"{indent}  {hour_idx:02d}   | {metrics.get('r2', np.nan):13.4f} | {metrics.get('mse', np.nan):14.4f} | {metrics.get('mae', np.nan):14.4f} | {metrics.get('rmse', np.nan):15.4f} | {metrics.get('count', 0):7d}\")\n","    aggregated = calculate_aggregated_metrics_report(hourly_metrics, T_pred_horizon)\n","    print(f\"{indent}  Aggregated Avg R2   : {aggregated.get('avg_r2', np.nan):.4f} (Std: {aggregated.get('std_r2', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MSE  : {aggregated.get('avg_mse', np.nan):.4f} (Std: {aggregated.get('std_mse', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MAE  : {aggregated.get('avg_mae', np.nan):.4f} (Std: {aggregated.get('std_mae', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg RMSE : {aggregated.get('avg_rmse', np.nan):.4f} (Std: {aggregated.get('std_rmse', np.nan):.4f})\")\n","\n","# ===========================================================\n","# 6. 主执行块\n","# ===========================================================\n","if __name__ == \"__main__\":\n","    gc.collect()\n","    if torch.cuda.is_available(): torch.cuda.empty_cache()\n","\n","    DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","\n","    if not DRIVE_BASE_PATH.exists(): DRIVE_BASE_PATH.mkdir(parents=True, exist_ok=True)\n","\n","    DATA_SUBDIR = Path(\"Result/Sequential_12Hour_Data\")\n","    DATA_FILENAME = \"graph_seq_20230503_SeqH7to19_NpyH8fill0.0.pkl\"\n","    RESULTS_SUBDIR = Path(\"Result/Final_CGVAELSTM1\")\n","    RESULTS_SAVE_DIR = DRIVE_BASE_PATH / RESULTS_SUBDIR\n","    os.makedirs(RESULTS_SAVE_DIR, exist_ok=True)\n","    DATA_PATH = DRIVE_BASE_PATH / DATA_SUBDIR / DATA_FILENAME\n","\n","    DATA_YEAR = 2023; DATA_MONTH = 5; DATA_DAY = 3\n","    START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES = 8; PREDICTION_HORIZON = 12\n","\n","    training_config = {\n","        'seed': 42, 'batch_size': 8, 'lr': 0.001, 'weight_decay': 1e-5,\n","        'max_epochs': 1000, 'scheduler_patience': 20, 'early_stopping_patience': 45,\n","        'T_pred_horizon': PREDICTION_HORIZON, 'results_dir': str(RESULTS_SAVE_DIR),\n","        'global_env_emb_dim': 16, 'time_emb_dim': 8,\n","\n","        # Parameters for CGVAE (using repurposed GCN keys)\n","        'gcn_hidden_dim': 128,           # -> cgvae_hidden_dim\n","        'cgvae_latent_dim': 64,         # New: Latent dimension for CGVAE\n","        'gcn_output_dim': 128,           # -> cgvae_output_dim (output of CGVAE decoder)\n","        'cgvae_num_encoder_layers': 3,  # New\n","        'cgvae_num_decoder_layers': 3,  # New\n","        'dropout_rate_gcn': 0.3,        # -> cgvae_dropout_rate\n","        'num_relations': 5,             # For RGCNConv within CGVAE\n","\n","        # Parameters for LSTM (using repurposed GRU keys)\n","        'gru_hidden_dim': 128, # This will be lstm_hidden_dim\n","        'num_gru_layers': 1,  # This will be num_lstm_layers\n","        'dropout_rate_gru': 0.2, # This will be dropout_rate_lstm\n","\n","        # MLP and other parameters\n","        'mlp_prediction_hidden_dim': 64,\n","        'fusion_mlp_output_dim': 128,\n","        'fusion_mlp_hidden_dim': 64,\n","        'dropout_rate_fusion_mlp': 0.2, 'dropout_rate_encoders': 0.1,\n","        'dropout_rate_pred_head': 0.2,\n","        'use_amp': False, 'enable_profiler': False, 'num_workers': 0,\n","        'pin_memory': False, 'train_split_ratio': 0.7, 'val_split_ratio': 0.2,\n","        'h0_from_first_step': True\n","    }\n","\n","    all_graph_sequences_loaded = None\n","    try:\n","        if not DATA_PATH.exists(): raise FileNotFoundError(f\"数据文件在指定路径未找到: {DATA_PATH}\")\n","        with open(DATA_PATH, \"rb\") as f: all_graph_sequences_loaded = pickle.load(f)\n","        if not all_graph_sequences_loaded or not isinstance(all_graph_sequences_loaded, list) or not all_graph_sequences_loaded[0] or not isinstance(all_graph_sequences_loaded[0], list):\n","            raise ValueError(\"加载的数据格式不正确。\")\n","\n","        expected_len_per_sequence = training_config['T_pred_horizon'] + 1\n","\n","        processed_sequences = []\n","        for i, seq in enumerate(all_graph_sequences_loaded):\n","            if not isinstance(seq, list) or len(seq) != expected_len_per_sequence: continue\n","            valid_seq = True\n","            for step_idx, graph_step_data in enumerate(seq):\n","                # Check for edge_attr for RGCN-CGVAE\n","                if not isinstance(graph_step_data, Data) or not hasattr(graph_step_data, 'x') or graph_step_data.x is None or \\\n","                   not hasattr(graph_step_data, 'edge_index') or graph_step_data.edge_index is None or \\\n","                   not hasattr(graph_step_data, 'edge_attr') or graph_step_data.edge_attr is None or \\\n","                   graph_step_data.edge_attr.shape[1] < (1 if training_config.get('num_relations',0) > 0 else 0) or \\\n","                   not hasattr(graph_step_data, 'graph_global_env_features') or \\\n","                   (step_idx > 0 and (not hasattr(graph_step_data, 'y') or graph_step_data.y is None)):\n","                    # If RGCN needs specific edge_attr shape (e.g. for relation types)\n","                    if training_config.get('num_relations',0) > 0 and hasattr(graph_step_data, 'edge_attr') and \\\n","                       graph_step_data.edge_attr is not None and graph_step_data.edge_attr.shape[1] < 5: # Example check\n","                       print(f\"Warning: Seq {i}, step {step_idx} has edge_attr dim {graph_step_data.edge_attr.shape[1]}, RGCN may need more for types.\")\n","                    valid_seq = False; break\n","                if step_idx > 0 and isinstance(graph_step_data.y, torch.Tensor) and graph_step_data.y.ndim == 1:\n","                    graph_step_data.y = graph_step_data.y.unsqueeze(1)\n","            if valid_seq: processed_sequences.append(seq)\n","        if not processed_sequences: raise ValueError(f\"数据处理后没有长度为 {expected_len_per_sequence} 的有效序列。\")\n","        all_graph_sequences = processed_sequences\n","        print(f\"成功加载并处理 {len(all_graph_sequences)} 个空间窗口的序列数据。\")\n","    except Exception as e: print(f\"加载或验证数据时发生错误: {e}\"); all_graph_sequences = None\n","\n","    if all_graph_sequences:\n","        base_datetime_for_timeline = dt_datetime(DATA_YEAR, DATA_MONTH, DATA_DAY, START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES)\n","        time_features_for_dataset_timeline = generate_time_features_for_sequence(\n","            base_datetime_for_timeline,\n","            training_config['T_pred_horizon']\n","        )\n","\n","        trained_model, final_node_mean, final_node_std, final_target_mean, final_target_std = main_training_cgvae_lstm_hourly_heads( # Renamed call\n","            all_graph_sequences, training_config, time_features_for_dataset_timeline\n","        )\n","        print(\"CGVAE-LSTM 模型训练和评估完成!\")\n","    else:\n","        print(\"由于数据加载失败或数据为空，训练流程未启动。\")"]},{"cell_type":"markdown","metadata":{"id":"HddOV6Y9cmn7"},"source":["##GGAN+LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrwZP3L4cnJ0"},"outputs":[],"source":["# ===========================================================\n","# 0. 环境 & 依赖\n","# ===========================================================\n","import os\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch_geometric.nn import RGCNConv\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.loader import DataLoader\n","import pickle\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.metrics import r2_score\n","import gc\n","import math\n","from datetime import datetime as dt_datetime, timedelta\n","from pathlib import Path\n","import time\n","import json\n","import torchprofile\n","\n","# gc.collect()\n","# torch.cuda.empty_cache()\n","# torch.cuda.ipc_collect()\n","\n","\n","# ===========================================================\n","# 1. 特征生成 & 辅助模块 (No changes)\n","# ===========================================================\n","\n","def generate_time_features_for_sequence(base_dt_obj, num_steps):\n","    time_features_list = []\n","    for i in range(num_steps):\n","        current_dt = base_dt_obj + timedelta(hours=i)\n","        hour_norm = current_dt.hour / 23.0\n","        day_of_year_norm = current_dt.timetuple().tm_yday / (366.0 if current_dt.year % 4 == 0 and (current_dt.year % 100 != 0 or current_dt.year % 400 == 0) else 365.0)\n","        hour_sin = math.sin(2 * math.pi * hour_norm)\n","        hour_cos = math.cos(2 * math.pi * hour_norm)\n","        doy_sin = math.sin(2 * math.pi * day_of_year_norm)\n","        doy_cos = math.cos(2 * math.pi * day_of_year_norm)\n","        time_features_list.append(torch.tensor([hour_sin, hour_cos, doy_sin, doy_cos], dtype=torch.float32))\n","    return torch.stack(time_features_list)\n","\n","\n","class MLPEncoder(nn.Module):\n","    def __init__(self, in_dim, out_dim, hid_dim=None, dropout_rate=0.1):\n","        super().__init__()\n","        if hid_dim is None:\n","            hid_dim = max(min(in_dim, out_dim), (in_dim + out_dim) // 2)\n","            if hid_dim == 0 and out_dim > 0 : hid_dim = out_dim\n","            if hid_dim == 0 and in_dim > 0 : hid_dim = in_dim\n","            if hid_dim == 0 : hid_dim = 1\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(in_dim, hid_dim),\n","            nn.ReLU(),\n","            nn.LayerNorm(hid_dim),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(hid_dim, out_dim)\n","        )\n","    def forward(self, x):\n","        return self.mlp(x)\n","\n","# ===========================================================\n","# 2. C-GGAN Components (using RGCNConv)\n","# ===========================================================\n","\n","class RGCNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, num_relations, dropout_rate,\n","                 activation_fn=None, use_residual=True, is_discriminator_block=False): # Added is_discriminator_block for clarity\n","        super().__init__()\n","        self.conv = RGCNConv(in_channels, out_channels, num_relations=num_relations)\n","        self.norm = nn.BatchNorm1d(out_channels)\n","        self.activation = activation_fn if activation_fn is not None else nn.PReLU()\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.use_residual = use_residual\n","        self.is_discriminator_block = is_discriminator_block\n","\n","\n","        if self.use_residual:\n","            if in_channels == out_channels:\n","                self.residual_projection = nn.Identity()\n","            else:\n","                self.residual_projection = nn.Linear(in_channels, out_channels)\n","\n","    def forward(self, x_input, edge_index, edge_attr):\n","        if x_input.size(0) == 0: return x_input\n","\n","        if edge_attr is None:\n","             # If edge_attr is None, but num_relations > 1, RGCNConv will error.\n","             # For this setup, we expect edge_attr to always be provided if num_relations > 1.\n","            if self.conv.num_relations > 1:\n","                 raise ValueError(\"RGCNBlock: edge_attr is None but num_relations > 1 in RGCNConv.\")\n","            edge_type = None # GCN-like behavior if num_relations is 1\n","        else:\n","            if edge_attr.shape[1] >= 5:\n","                edge_type = edge_attr[:, 4].long()\n","            elif edge_attr.shape[1] > 0 : # Assume first column is relation type if fewer than 5 cols\n","                edge_type = edge_attr[:,0].long()\n","            else:\n","                raise ValueError(\"RGCNBlock: edge_attr has 0 columns but is not None.\")\n","\n","\n","        h = self.conv(x_input, edge_index, edge_type=edge_type)\n","        if h.shape[0] > 1:\n","            h = self.norm(h)\n","        h = self.activation(h)\n","        h = self.dropout(h)\n","\n","        if self.use_residual:\n","            projected_x_input = self.residual_projection(x_input)\n","            h = h + projected_x_input\n","        return h\n","\n","class NodeFeatureGeneratorRGCN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_relations, dropout_rate):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.output_dim = output_dim\n","        self.num_relations = num_relations\n","\n","        layers = []\n","        current_dim = input_dim\n","        for i in range(num_layers):\n","            block_use_residual = (i > 0)\n","            layers.append(RGCNBlock(current_dim, hidden_dim, num_relations, dropout_rate,\n","                                   use_residual=block_use_residual if current_dim == hidden_dim else False))\n","            current_dim = hidden_dim\n","        self.rgcn_layers = nn.ModuleList(layers)\n","        self.fc_out = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x, edge_index, edge_attr):\n","        for layer in self.rgcn_layers:\n","            x = layer(x, edge_index, edge_attr)\n","        transformed_features = self.fc_out(x)\n","        return transformed_features\n","\n","class PredictionDiscriminatorRGCN(nn.Module):\n","    def __init__(self, original_node_feature_dim, prediction_dim,\n","                 hidden_dim, num_layers, num_relations, dropout_rate):\n","        super().__init__()\n","        self.initial_input_dim = original_node_feature_dim + prediction_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.num_relations = num_relations\n","\n","        layers = []\n","        current_dim = self.initial_input_dim\n","        for i in range(num_layers):\n","            block_use_residual = (i > 0)\n","            layers.append(RGCNBlock(current_dim, hidden_dim, num_relations, dropout_rate,\n","                                   activation_fn=nn.LeakyReLU(0.2, inplace=True),\n","                                   use_residual=block_use_residual if current_dim == hidden_dim else False,\n","                                   is_discriminator_block=True))\n","            current_dim = hidden_dim\n","        self.rgcn_layers = nn.ModuleList(layers)\n","        self.fc_out = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, x_original_normalized, y_candidate_scaled, edge_index, edge_attr):\n","        # x_original_normalized: (N_masked_or_full, F_node_orig)\n","        # y_candidate_scaled: (N_masked_or_full, 1)\n","        # edge_index & edge_attr: Pertain to the graph from which x_original and y_candidate are derived.\n","        # IMPORTANT: If x_original_normalized and y_candidate_scaled are subsets of nodes,\n","        # edge_index must be re-mapped to this subset for RGCNBlock to work correctly.\n","        # The current fix will pass full x and y and mask AFTER discriminator.\n","\n","        discriminator_input_features = torch.cat([x_original_normalized, y_candidate_scaled], dim=1)\n","\n","        h = discriminator_input_features\n","        for layer in self.rgcn_layers:\n","            h = layer(h, edge_index, edge_attr) # edge_attr should be for the full graph\n","        logits = self.fc_out(h)\n","        return logits\n","\n","\n","# ===========================================================\n","# 2.b CGGAN-LSTM 模型定义 (Main Model - Generator Path)\n","# ===========================================================\n","class CGGANLSTMModelWithHourlyHeads(nn.Module):\n","    def __init__(self,\n","                 static_node_in_dim,\n","                 global_env_in_dim,\n","                 time_in_dim,\n","                 global_env_emb_dim,\n","                 time_emb_dim,\n","                 gen_hidden_dim,\n","                 gen_output_dim,\n","                 gen_num_layers,\n","                 gen_dropout_rate,\n","                 num_relations,\n","                 lstm_hidden_dim,\n","                 fusion_mlp_output_dim=None,\n","                 fusion_mlp_hidden_dim=None,\n","                 dropout_rate_fusion_mlp=0.1,\n","                 num_lstm_layers=1,\n","                 T_pred_horizon=12,\n","                 dropout_rate_encoders=0.1,\n","                 dropout_rate_lstm=0.2,\n","                 mlp_prediction_hidden_dim=64,\n","                 dropout_rate_pred_head=0.2\n","                ):\n","        super().__init__()\n","        self.T_pred_horizon = T_pred_horizon\n","        self.static_node_in_dim = static_node_in_dim\n","        self.global_env_in_dim = global_env_in_dim\n","        self.time_in_dim = time_in_dim\n","        self.gen_output_dim = gen_output_dim\n","        self.lstm_hidden_dim = lstm_hidden_dim\n","        self.num_relations = num_relations\n","\n","        self.global_env_encoder = MLPEncoder(global_env_in_dim, global_env_emb_dim, dropout_rate=dropout_rate_encoders)\n","        self.time_encoder = MLPEncoder(time_in_dim, time_emb_dim, dropout_rate=dropout_rate_encoders)\n","\n","        self.h0_c0_from_gnn_encoder = MLPEncoder(gen_output_dim, lstm_hidden_dim, dropout_rate=dropout_rate_encoders)\n","\n","        self.node_feature_generator_h0 = NodeFeatureGeneratorRGCN(\n","            input_dim=static_node_in_dim, hidden_dim=gen_hidden_dim, output_dim=gen_output_dim,\n","            num_layers=gen_num_layers, num_relations=num_relations, dropout_rate=gen_dropout_rate\n","        )\n","        self.node_feature_generator_sequence = NodeFeatureGeneratorRGCN(\n","            input_dim=static_node_in_dim, hidden_dim=gen_hidden_dim, output_dim=gen_output_dim,\n","            num_layers=gen_num_layers, num_relations=num_relations, dropout_rate=gen_dropout_rate\n","        )\n","\n","        concatenated_feature_dim = gen_output_dim + global_env_emb_dim + time_emb_dim\n","        actual_fusion_mlp_output_dim = fusion_mlp_output_dim if fusion_mlp_output_dim is not None else concatenated_feature_dim\n","        self.fusion_mlp_input_dim = concatenated_feature_dim\n","\n","        self.fusion_mlp = MLPEncoder(\n","            in_dim=concatenated_feature_dim,\n","            out_dim=actual_fusion_mlp_output_dim,\n","            hid_dim=fusion_mlp_hidden_dim,\n","            dropout_rate=dropout_rate_fusion_mlp\n","        )\n","\n","        lstm_input_size_actual = actual_fusion_mlp_output_dim\n","        self.lstm_input_dim = lstm_input_size_actual\n","\n","        self.lstm = nn.LSTM(\n","            input_size=lstm_input_size_actual,\n","            hidden_size=lstm_hidden_dim,\n","            num_layers=num_lstm_layers,\n","            batch_first=True,\n","            dropout=dropout_rate_lstm if num_lstm_layers > 1 else 0.0\n","        )\n","\n","        self.hourly_prediction_heads = nn.ModuleList()\n","        for _ in range(T_pred_horizon):\n","            self.hourly_prediction_heads.append(\n","                nn.Sequential(\n","                    nn.Linear(lstm_hidden_dim, mlp_prediction_hidden_dim),\n","                    nn.ReLU(),\n","                    nn.Dropout(dropout_rate_pred_head),\n","                    nn.Linear(mlp_prediction_hidden_dim, 1)\n","                )\n","            )\n","\n","        self.register_buffer('node_feat_mean', torch.zeros(static_node_in_dim))\n","        self.register_buffer('node_feat_std', torch.ones(static_node_in_dim))\n","\n","    def forward(self, list_of_batched_timesteps: list, timeline_time_features: torch.Tensor, device: torch.device):\n","        pyg_batch_7am = list_of_batched_timesteps[0].to(device)\n","        # Ensure x is normalized before passing to generator\n","        normalized_x_7am = (pyg_batch_7am.x - self.node_feat_mean) / (self.node_feat_std + 1e-8)\n","\n","        edge_attr_7am = pyg_batch_7am.edge_attr\n","        if edge_attr_7am is None: raise ValueError(\"edge_attr required for NodeFeatureGeneratorRGCN (h0).\")\n","        edge_attr_7am = edge_attr_7am.float()\n","\n","        generated_features_7am = self.node_feature_generator_h0(\n","            normalized_x_7am, # Pass normalized x\n","            pyg_batch_7am.edge_index,\n","            edge_attr=edge_attr_7am\n","        )\n","        h0_features_for_lstm_nodes = self.h0_c0_from_gnn_encoder(generated_features_7am)\n","\n","        h0_for_lstm = h0_features_for_lstm_nodes.unsqueeze(0)\n","        c0_for_lstm = torch.zeros_like(h0_for_lstm)\n","\n","        if self.lstm.num_layers > 1:\n","            h0_for_lstm = h0_for_lstm.repeat(self.lstm.num_layers, 1, 1)\n","            c0_for_lstm = c0_for_lstm.repeat(self.lstm.num_layers, 1, 1)\n","        initial_hidden_state = (h0_for_lstm, c0_for_lstm)\n","\n","        all_lstm_input_features_over_time = []\n","        for t_pred_idx in range(self.T_pred_horizon):\n","            pyg_batch_this_timestep = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","            normalized_x_t = (pyg_batch_this_timestep.x - self.node_feat_mean) / (self.node_feat_std + 1e-8)\n","\n","            edge_attr_this_timestep = pyg_batch_this_timestep.edge_attr\n","            if edge_attr_this_timestep is None: raise ValueError(\"edge_attr required for NodeFeatureGeneratorRGCN (sequence).\")\n","            edge_attr_this_timestep = edge_attr_this_timestep.float()\n","\n","            generated_features_t = self.node_feature_generator_sequence(\n","                normalized_x_t, # Pass normalized x\n","                pyg_batch_this_timestep.edge_index,\n","                edge_attr=edge_attr_this_timestep\n","            )\n","\n","            global_env_feat_t_unencoded = pyg_batch_this_timestep.graph_global_env_features\n","            expected_num_graphs_in_batch_t = pyg_batch_this_timestep.num_graphs\n","            expected_global_features_dim = self.global_env_encoder.mlp[0].in_features\n","            if not (global_env_feat_t_unencoded.shape == (expected_num_graphs_in_batch_t, expected_global_features_dim)):\n","                if global_env_feat_t_unencoded.ndim == 1 and \\\n","                   global_env_feat_t_unencoded.shape[0] == expected_num_graphs_in_batch_t * expected_global_features_dim:\n","                    global_env_feat_t_unencoded = global_env_feat_t_unencoded.view(expected_num_graphs_in_batch_t, expected_global_features_dim)\n","                elif global_env_feat_t_unencoded.numel() == expected_num_graphs_in_batch_t * expected_global_features_dim:\n","                    global_env_feat_t_unencoded = global_env_feat_t_unencoded.view(expected_num_graphs_in_batch_t, expected_global_features_dim)\n","                else:\n","                    print(f\"Warning: LSTM Input Time {t_pred_idx}: Correcting global_env_feat shape from {global_env_feat_t_unencoded.shape} to ({expected_num_graphs_in_batch_t}, {expected_global_features_dim}) with zeros due to mismatch.\")\n","                    global_env_feat_t_unencoded = torch.zeros(expected_num_graphs_in_batch_t, expected_global_features_dim, device=device)\n","            global_env_emb_t = self.global_env_encoder(global_env_feat_t_unencoded)\n","            global_env_emb_t_expanded = global_env_emb_t[pyg_batch_this_timestep.batch]\n","\n","            current_raw_time_feat_for_timestep_t = timeline_time_features[t_pred_idx, :].to(device)\n","            current_emb_time_feat_for_timestep_t = self.time_encoder(current_raw_time_feat_for_timestep_t)\n","            num_nodes_in_pyg_batch = pyg_batch_this_timestep.num_nodes\n","            time_emb_t_expanded_to_nodes = current_emb_time_feat_for_timestep_t.unsqueeze(0).expand(num_nodes_in_pyg_batch, -1)\n","\n","            concatenated_features_for_timestep_t = torch.cat([generated_features_t, global_env_emb_t_expanded, time_emb_t_expanded_to_nodes], dim=-1)\n","            fused_features_for_timestep_t = self.fusion_mlp(concatenated_features_for_timestep_t)\n","            all_lstm_input_features_over_time.append(fused_features_for_timestep_t)\n","\n","        stacked_lstm_input_features = torch.stack(all_lstm_input_features_over_time, dim=1)\n","\n","        if initial_hidden_state[0].shape[1] != stacked_lstm_input_features.shape[0]:\n","            print(f\"CRITICAL WARNING: Node count mismatch for LSTM h0/c0 ({initial_hidden_state[0].shape[1]}) and LSTM input sequence ({stacked_lstm_input_features.shape[0]}).\")\n","            if initial_hidden_state[0].shape[1] > stacked_lstm_input_features.shape[0]:\n","                h0_adj = initial_hidden_state[0][:, :stacked_lstm_input_features.shape[0], :]\n","                c0_adj = initial_hidden_state[1][:, :stacked_lstm_input_features.shape[0], :]\n","                initial_hidden_state = (h0_adj, c0_adj)\n","\n","        lstm_out, _ = self.lstm(stacked_lstm_input_features, initial_hidden_state)\n","\n","        all_hourly_final_predictions_scaled = []\n","        for t in range(self.T_pred_horizon):\n","            lstm_out_t = lstm_out[:, t, :]\n","            prediction_t_scaled = self.hourly_prediction_heads[t](lstm_out_t)\n","            all_hourly_final_predictions_scaled.append(prediction_t_scaled.squeeze(-1))\n","\n","        predictions_scaled = torch.stack(all_hourly_final_predictions_scaled, dim=1)\n","        return predictions_scaled\n","\n","# ===========================================================\n","# 3. 评估指标函数 - NO CHANGES\n","# ===========================================================\n","# ... (mse_loss_masked, calculate_hourly_metrics are identical) ...\n","def mse_loss_masked(predictions_scaled, targets_scaled, mask):\n","    expanded_mask = mask.unsqueeze(1).expand_as(targets_scaled)\n","    valid_targets_mask = ~torch.isnan(targets_scaled)\n","    final_mask = expanded_mask & valid_targets_mask\n","    if final_mask.sum() == 0:\n","        return torch.tensor(0.0, device=predictions_scaled.device, requires_grad=True)\n","    loss = F.mse_loss(predictions_scaled[final_mask], targets_scaled[final_mask])\n","    return loss\n","\n","def calculate_hourly_metrics(predictions_scaled, targets_scaled, node_masks, target_mean, target_std):\n","    target_mean_cpu = target_mean.cpu()\n","    target_std_cpu = target_std.cpu()\n","    preds_unscaled = predictions_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","    targets_unscaled = targets_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","\n","    num_nodes_total, T_horizon = preds_unscaled.shape\n","    hourly_metrics_dict = {}\n","\n","    preds_np = preds_unscaled.numpy()\n","    targets_np = targets_unscaled.numpy()\n","    mask_np = node_masks.cpu().numpy()\n","\n","    for t in range(T_horizon):\n","        preds_t_all_nodes = preds_np[:, t]\n","        targets_t_all_nodes = targets_np[:, t]\n","        preds_t_on_loss_nodes = preds_t_all_nodes[mask_np]\n","        targets_t_on_loss_nodes = targets_t_all_nodes[mask_np]\n","        valid_target_data_mask_t = ~np.isnan(targets_t_on_loss_nodes)\n","        preds_t_final_valid = preds_t_on_loss_nodes[valid_target_data_mask_t]\n","        targets_t_final_valid = targets_t_on_loss_nodes[valid_target_data_mask_t]\n","\n","        if preds_t_final_valid.shape[0] < 2:\n","            hourly_metrics_dict[t] = {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count': 0}\n","            continue\n","        mse = np.mean((preds_t_final_valid - targets_t_final_valid)**2)\n","        mae = np.mean(np.abs(preds_t_final_valid - targets_t_final_valid))\n","        rmse = np.sqrt(mse)\n","        try:\n","            r2 = r2_score(targets_t_final_valid, preds_t_final_valid)\n","        except ValueError:\n","            r2 = np.nan\n","        hourly_metrics_dict[t] = {'mse': mse, 'mae': mae, 'rmse': rmse, 'r2': r2, 'count': preds_t_final_valid.shape[0]}\n","    return hourly_metrics_dict\n","\n","# ===========================================================\n","# 4. 对抗训练与评估循环\n","# ===========================================================\n","def train_epoch_adversarial(model_G, model_D, loader,\n","                            optimizer_G, optimizer_D,\n","                            device, timeline_time_features,\n","                            node_feat_mean, node_feat_std,\n","                            target_mean, target_std,\n","                            lambda_adv, T_pred_horizon, static_node_in_dim): # Added static_node_in_dim\n","    model_G.train()\n","    model_D.train()\n","\n","    total_loss_G_epoch = 0\n","    total_loss_D_epoch = 0\n","    total_pred_loss_epoch = 0\n","    total_adv_loss_G_epoch = 0\n","    num_sequences_processed = 0\n","\n","    # Ensure GNN modules within model_G have access to scalers if needed by GNN input normalization\n","    # model_G itself handles it via registered buffers, which is good.\n","\n","    target_mean_dev = target_mean.to(device)\n","    target_std_dev = target_std.to(device)\n","    node_feat_mean_dev = node_feat_mean.to(device) # For normalizing x before D\n","    node_feat_std_dev = node_feat_std.to(device)   # For normalizing x before D\n","\n","\n","    epoch_start_time = time.time()\n","\n","    for list_of_batched_timesteps in loader:\n","        # list_of_batched_timesteps[0] is obs, [1:] are for prediction horizon\n","        num_graphs_in_batch = list_of_batched_timesteps[0].num_graphs\n","\n","\n","        # --- Train Discriminator ---\n","        optimizer_D.zero_grad()\n","\n","        with torch.no_grad():\n","            predictions_batch_scaled_fake = model_G(list_of_batched_timesteps,\n","                                                    timeline_time_features.to(device),\n","                                                    device).detach()\n","\n","        loss_D_batch_total = torch.tensor(0.0, device=device)\n","\n","        for t_idx in range(T_pred_horizon):\n","            current_graph_data = list_of_batched_timesteps[t_idx + 1].to(device)\n","            x_original_t_normalized = (current_graph_data.x - node_feat_mean_dev) / (node_feat_std_dev + 1e-8)\n","\n","            y_real_t_original = current_graph_data.y.squeeze()\n","            y_real_t_scaled = ((y_real_t_original - target_mean_dev) / (target_std_dev + 1e-8)).unsqueeze(-1)\n","            y_fake_t_scaled = predictions_batch_scaled_fake[:, t_idx].unsqueeze(-1)\n","\n","            mask_for_D_loss_t = ~current_graph_data.building_mask\n","            valid_targets_mask_t = ~torch.isnan(y_real_t_scaled.squeeze())\n","            final_mask_real_t = mask_for_D_loss_t & valid_targets_mask_t\n","\n","            edge_attr_t = current_graph_data.edge_attr\n","            if edge_attr_t is not None: edge_attr_t = edge_attr_t.float()\n","            else: raise ValueError(f\"Discriminator requires edge_attr at t_idx={t_idx}\")\n","\n","            # Real samples\n","            if final_mask_real_t.sum() > 0:\n","                # Pass only the masked nodes' features to D if D is not graph-aware beyond node features\n","                # However, D uses RGCNConv, so it needs full graph structure for context,\n","                # but we want to compute loss only on relevant nodes.\n","                # So, D processes all nodes, then we mask the logits.\n","                logits_real_all_nodes = model_D(x_original_t_normalized,\n","                                                y_real_t_scaled,\n","                                                current_graph_data.edge_index,\n","                                                edge_attr_t)\n","                loss_D_real_t = F.binary_cross_entropy_with_logits(\n","                    logits_real_all_nodes[final_mask_real_t],\n","                    torch.ones_like(logits_real_all_nodes[final_mask_real_t])\n","                )\n","            else:\n","                loss_D_real_t = torch.tensor(0.0, device=device)\n","\n","            # Fake samples\n","            if mask_for_D_loss_t.sum() > 0:\n","                logits_fake_all_nodes = model_D(x_original_t_normalized,\n","                                                y_fake_t_scaled,\n","                                                current_graph_data.edge_index,\n","                                                edge_attr_t)\n","                loss_D_fake_t = F.binary_cross_entropy_with_logits(\n","                    logits_fake_all_nodes[mask_for_D_loss_t],\n","                    torch.zeros_like(logits_fake_all_nodes[mask_for_D_loss_t])\n","                )\n","            else:\n","                loss_D_fake_t = torch.tensor(0.0, device=device)\n","\n","            loss_D_t = (loss_D_real_t + loss_D_fake_t) / 2.0\n","            if not (torch.isnan(loss_D_t) or torch.isinf(loss_D_t)):\n","                 loss_D_batch_total += loss_D_t\n","\n","        if T_pred_horizon > 0: # Avoid division by zero if T_pred_horizon is 0\n","            avg_loss_D_batch = loss_D_batch_total / T_pred_horizon\n","            if avg_loss_D_batch.item() > 0 :\n","                 avg_loss_D_batch.backward()\n","                 optimizer_D.step()\n","            total_loss_D_epoch += avg_loss_D_batch.item() * num_graphs_in_batch\n","\n","\n","        # --- Train Generator (Main Model G) ---\n","        optimizer_G.zero_grad()\n","\n","        predictions_batch_scaled_for_G = model_G(list_of_batched_timesteps,\n","                                                 timeline_time_features.to(device),\n","                                                 device)\n","\n","        # 1. Prediction Loss (MSE)\n","        targets_list_for_loss_scaled = []\n","        first_prediction_graph_step_in_batch_G = list_of_batched_timesteps[1].to(device)\n","        mask_for_G_pred_loss = ~first_prediction_graph_step_in_batch_G.building_mask\n","\n","        for t_pred_idx in range(T_pred_horizon):\n","            current_target_timestep_batch = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","            targets_t_nodes_original = current_target_timestep_batch.y.squeeze()\n","            targets_t_nodes_scaled = (targets_t_nodes_original - target_mean_dev) / (target_std_dev + 1e-8)\n","            targets_list_for_loss_scaled.append(targets_t_nodes_scaled)\n","        targets_batch_scaled = torch.stack(targets_list_for_loss_scaled, dim=1)\n","\n","        loss_pred_G = mse_loss_masked(predictions_batch_scaled_for_G, targets_batch_scaled, mask_for_G_pred_loss)\n","\n","        # 2. Adversarial Loss for Generator\n","        loss_adv_G_batch_total = torch.tensor(0.0, device=device)\n","        for t_idx in range(T_pred_horizon):\n","            current_graph_data_G = list_of_batched_timesteps[t_idx + 1].to(device)\n","            x_original_normalized_t_G = (current_graph_data_G.x - node_feat_mean_dev) / (node_feat_std_dev + 1e-8)\n","            y_fake_t_scaled_for_G = predictions_batch_scaled_for_G[:, t_idx].unsqueeze(-1)\n","            mask_for_adv_G_loss_t = ~current_graph_data_G.building_mask\n","\n","            edge_attr_t_G = current_graph_data_G.edge_attr\n","            if edge_attr_t_G is not None: edge_attr_t_G = edge_attr_t_G.float()\n","            else: raise ValueError(f\"Generator's adversarial pass requires edge_attr at t_idx={t_idx}\")\n","\n","\n","            if mask_for_adv_G_loss_t.sum() > 0:\n","                # Pass full node features, D will process graph, then mask logits for loss\n","                fake_logits_for_G_all_nodes = model_D(x_original_normalized_t_G,\n","                                                      y_fake_t_scaled_for_G,\n","                                                      current_graph_data_G.edge_index,\n","                                                      edge_attr_t_G)\n","                loss_adv_G_t = F.binary_cross_entropy_with_logits(\n","                    fake_logits_for_G_all_nodes[mask_for_adv_G_loss_t],\n","                    torch.ones_like(fake_logits_for_G_all_nodes[mask_for_adv_G_loss_t])\n","                )\n","                if not (torch.isnan(loss_adv_G_t) or torch.isinf(loss_adv_G_t)):\n","                    loss_adv_G_batch_total += loss_adv_G_t\n","\n","        avg_loss_adv_G_batch = torch.tensor(0.0, device=device)\n","        if T_pred_horizon > 0:\n","            avg_loss_adv_G_batch = loss_adv_G_batch_total / T_pred_horizon\n","\n","        loss_G_total_batch = loss_pred_G + lambda_adv * avg_loss_adv_G_batch\n","\n","        if not torch.isnan(loss_G_total_batch) and not torch.isinf(loss_G_total_batch) and loss_G_total_batch.item() > 0:\n","            loss_G_total_batch.backward()\n","            optimizer_G.step()\n","\n","        total_loss_G_epoch += loss_G_total_batch.item() * num_graphs_in_batch\n","        total_pred_loss_epoch += loss_pred_G.item() * num_graphs_in_batch\n","        total_adv_loss_G_epoch += avg_loss_adv_G_batch.item() * num_graphs_in_batch\n","\n","        num_sequences_processed += num_graphs_in_batch\n","\n","    epoch_duration = time.time() - epoch_start_time\n","    avg_loss_G = total_loss_G_epoch / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","    avg_loss_D = total_loss_D_epoch / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","    avg_pred_loss = total_pred_loss_epoch / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","    avg_adv_loss_G = total_adv_loss_G_epoch / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","\n","    return avg_loss_G, avg_loss_D, avg_pred_loss, avg_adv_loss_G, epoch_duration\n","\n","\n","# Evaluate epoch remains the same, it only uses the Generator (main model)\n","def evaluate_epoch(model_G, loader, device, timeline_time_features, # model_G is the main model\n","                   node_feat_mean, node_feat_std, target_mean, target_std, epoch_type=\"Eval\"):\n","    # ... (evaluate_epoch remains identical to the GINE-LSTM version, ensure it uses model_G) ...\n","    model_G.eval() # Use model_G for evaluation\n","    all_batch_predictions_scaled = []\n","    all_batch_targets_scaled = []\n","    all_batch_masks_for_metrics = []\n","    total_loss_scaled = 0 # This will be MSE loss\n","    num_sequences_processed = 0\n","    model_G.node_feat_mean = node_feat_mean.to(device)\n","    model_G.node_feat_std = node_feat_std.to(device)\n","\n","    target_mean_dev = target_mean.to(device)\n","    target_std_dev = target_std.to(device)\n","\n","    eval_start_time = time.time()\n","    with torch.no_grad():\n","        for list_of_batched_timesteps in loader:\n","            predictions_batch_scaled = model_G(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","            targets_list_original = []\n","            first_predicted_timestep_batch = list_of_batched_timesteps[1].to(device) # Assuming 1 obs step\n","            mask_for_metrics = ~first_predicted_timestep_batch.building_mask\n","\n","            for t_pred_idx in range(model_G.T_pred_horizon): # Use model_G.T_pred_horizon\n","                current_target_timestep_batch = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","                targets_t_nodes_original = current_target_timestep_batch.y.squeeze()\n","                targets_list_original.append(targets_t_nodes_original)\n","\n","            targets_batch_original = torch.stack(targets_list_original, dim=1)\n","            targets_batch_scaled_for_loss = (targets_batch_original - target_mean_dev) / (target_std_dev + 1e-8)\n","\n","            loss = mse_loss_masked(predictions_batch_scaled, targets_batch_scaled_for_loss, mask_for_metrics)\n","\n","            num_sequences_in_this_super_batch = list_of_batched_timesteps[0].num_graphs\n","\n","            if not torch.isnan(loss) and not torch.isinf(loss):\n","                total_loss_scaled += loss.item() * num_sequences_in_this_super_batch\n","            num_sequences_processed += num_sequences_in_this_super_batch\n","\n","            all_batch_predictions_scaled.append(predictions_batch_scaled.cpu())\n","            all_batch_targets_scaled.append(targets_batch_scaled_for_loss.cpu())\n","            all_batch_masks_for_metrics.append(mask_for_metrics.cpu())\n","\n","    eval_duration = time.time() - eval_start_time\n","    avg_loss_scaled = total_loss_scaled / num_sequences_processed if num_sequences_processed > 0 else 0.0\n","\n","    if not all_batch_predictions_scaled:\n","        empty_metrics = {t: {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0} for t in range(model_G.T_pred_horizon)}\n","        return avg_loss_scaled, empty_metrics, eval_duration\n","\n","    final_predictions_scaled = torch.cat(all_batch_predictions_scaled, dim=0)\n","    final_targets_scaled = torch.cat(all_batch_targets_scaled, dim=0)\n","    final_masks_for_metrics = torch.cat(all_batch_masks_for_metrics, dim=0)\n","\n","    hourly_metrics_original_scale = calculate_hourly_metrics(final_predictions_scaled, final_targets_scaled,\n","                                                             final_masks_for_metrics, target_mean.cpu(), target_std.cpu())\n","    return avg_loss_scaled, hourly_metrics_original_scale, eval_duration\n","\n","\n","# ===========================================================\n","# 5. 主训练流程 (CGGAN-LSTM)\n","# ===========================================================\n","def calculate_aggregated_metrics_report(hourly_metrics_dict, T_pred_horizon):\n","    # ... (identical) ...\n","    metrics_to_aggregate = ['r2', 'mse', 'mae', 'rmse']\n","    aggregated_report = {}\n","    for metric_name in metrics_to_aggregate:\n","        values = [hourly_metrics_dict[t][metric_name] for t in range(T_pred_horizon) if t in hourly_metrics_dict and not np.isnan(hourly_metrics_dict[t][metric_name])]\n","        if values:\n","            aggregated_report[f'avg_{metric_name}'] = np.mean(values)\n","            aggregated_report[f'std_{metric_name}'] = np.std(values)\n","        else:\n","            aggregated_report[f'avg_{metric_name}'] = np.nan\n","            aggregated_report[f'std_{metric_name}'] = np.nan\n","    return aggregated_report\n","\n","def main_training_cggan_lstm_hourly_heads( # Renamed\n","    all_sequences_data: list,\n","    config: dict,\n","    time_features_for_dataset: torch.Tensor\n","):\n","    train_start_time = time.time()\n","    report_data = {'config': config}\n","\n","    seed = config.get('seed', 42)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"使用设备: {device}\")\n","    report_data['device'] = str(device)\n","\n","    T_PRED_HORIZON = config.get('T_pred_horizon', 12)\n","    expected_input_len = T_PRED_HORIZON + 1\n","    valid_sequences_data = [seq for seq in all_sequences_data if isinstance(seq, list) and len(seq) == expected_input_len]\n","    if len(valid_sequences_data) != len(all_sequences_data):\n","        print(f\"警告: 从 {len(all_sequences_data)} 个序列中筛选出 {len(valid_sequences_data)} 个长度为 {expected_input_len} 的有效序列。\")\n","    if not valid_sequences_data:\n","        raise ValueError(f\"没有找到长度为 {expected_input_len} 的有效序列数据。\")\n","    all_sequences_data = valid_sequences_data\n","\n","    # Dataset split\n","    num_total_sequences = len(all_sequences_data)\n","    indices = np.random.permutation(num_total_sequences)\n","    train_split_ratio = config.get('train_split_ratio', 0.7)\n","    val_split_ratio = config.get('val_split_ratio', 0.2)\n","    train_size = int(train_split_ratio * num_total_sequences)\n","    val_size = int(val_split_ratio * num_total_sequences)\n","    train_indices = indices[:train_size]\n","    val_indices = indices[train_size : train_size + val_size]\n","    test_indices = indices[train_size + val_size :]\n","    train_dataset = [all_sequences_data[i] for i in train_indices]\n","    val_dataset   = [all_sequences_data[i] for i in val_indices]\n","    test_dataset  = [all_sequences_data[i] for i in test_indices]\n","    report_data['dataset_split'] = {'total_sequences': num_total_sequences, 'train_size': len(train_dataset), 'val_size': len(val_dataset), 'test_size': len(test_dataset)}\n","\n","    # Scaler calculation\n","    all_train_node_features_list = []\n","    all_train_target_values_list_for_scaling = []\n","    for seq in train_dataset:\n","        for i_step, graph_data in enumerate(seq):\n","            if hasattr(graph_data, 'x') and graph_data.x is not None:\n","                all_train_node_features_list.append(graph_data.x)\n","            if i_step > 0 and hasattr(graph_data, 'y') and graph_data.y is not None:\n","                y_original = graph_data.y.squeeze(); current_mask_for_loss = ~graph_data.building_mask\n","                valid_target_indices = current_mask_for_loss & ~torch.isnan(y_original)\n","                if valid_target_indices.sum() > 0:\n","                    all_train_target_values_list_for_scaling.append(y_original[valid_target_indices])\n","\n","    if not all_train_node_features_list: raise ValueError(\"训练数据中未找到节点特征 'x'，无法计算scaler！\")\n","    all_train_node_features_tensor = torch.cat(all_train_node_features_list, dim=0)\n","    node_feat_mean = torch.mean(all_train_node_features_tensor, dim=0); node_feat_std = torch.std(all_train_node_features_tensor, dim=0)\n","    node_feat_std[node_feat_std < 1e-8] = 1.0\n","    scaler_path_x = Path(config['results_dir']) / \"node_feature_scaler_cggan_lstm.pth\" # Renamed\n","    torch.save({'mean': node_feat_mean, 'std': node_feat_std}, scaler_path_x); print(f\"节点特征x scaler已保存到: {scaler_path_x}\")\n","\n","    if not all_train_target_values_list_for_scaling:\n","        target_mean = torch.tensor(0.0); target_std = torch.tensor(1.0)\n","    else:\n","        all_train_target_values_tensor = torch.cat(all_train_target_values_list_for_scaling, dim=0)\n","        target_mean = torch.mean(all_train_target_values_tensor.float()); target_std = torch.std(all_train_target_values_tensor.float())\n","        if target_std < 1e-8: target_std = torch.tensor(1.0)\n","    target_scaler_path = Path(config['results_dir']) / \"target_scaler_cggan_lstm.pth\" # Renamed\n","    torch.save({'mean': target_mean, 'std': target_std}, target_scaler_path); print(f\"目标值y scaler已保存到: {target_scaler_path}\")\n","\n","    # DataLoaders\n","    batch_size = config.get('batch_size', 8); num_workers = config.get('num_workers', 0)\n","    pin_memory_flag = config.get('pin_memory', False) and device.type == 'cuda'\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=num_workers, pin_memory=pin_memory_flag)\n","    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n","    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n","\n","    # Model Initialization\n","    sample_graph_7am_for_dims = all_sequences_data[0][0]\n","    static_node_in_dim = sample_graph_7am_for_dims.x.shape[1]\n","    global_env_in_dim = sample_graph_7am_for_dims.graph_global_env_features.shape[0] if sample_graph_7am_for_dims.graph_global_env_features.ndim == 1 else sample_graph_7am_for_dims.graph_global_env_features.shape[1]\n","    time_in_dim = time_features_for_dataset.shape[1]\n","    num_relations = config.get('num_relations', 5)\n","\n","    # Generator (Main Model)\n","    model_G = CGGANLSTMModelWithHourlyHeads(\n","        static_node_in_dim=static_node_in_dim, global_env_in_dim=global_env_in_dim, time_in_dim=time_in_dim,\n","        global_env_emb_dim=config.get('global_env_emb_dim', 16), time_emb_dim=config.get('time_emb_dim', 8),\n","        gen_hidden_dim=config.get('gcn_hidden_dim', 128),\n","        gen_output_dim=config.get('gcn_output_dim', 128),\n","        gen_num_layers=config.get('cggan_gen_num_layers', 2),\n","        gen_dropout_rate=config.get('dropout_rate_gcn', 0.3),\n","        num_relations=num_relations,\n","        lstm_hidden_dim=config.get('gru_hidden_dim', 128),\n","        fusion_mlp_output_dim=config.get('fusion_mlp_output_dim', 128),\n","        fusion_mlp_hidden_dim=config.get('fusion_mlp_hidden_dim', 64),\n","        dropout_rate_fusion_mlp=config.get('dropout_rate_fusion_mlp', 0.2),\n","        num_lstm_layers=config.get('num_gru_layers', 1),\n","        T_pred_horizon=T_PRED_HORIZON,\n","        dropout_rate_encoders=config.get('dropout_rate_encoders', 0.1),\n","        dropout_rate_lstm=config.get('dropout_rate_gru', 0.2),\n","        mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim', 64),\n","        dropout_rate_pred_head=config.get('dropout_rate_pred_head', 0.2)\n","    ).to(device)\n","\n","    model_G.node_feat_mean = node_feat_mean.to(device); model_G.node_feat_std = node_feat_std.to(device)\n","    total_params_G = sum(p.numel() for p in model_G.parameters() if p.requires_grad)\n","    print(f\"模型总参数量 (Generator - CGGAN-LSTM): {total_params_G:,}\");\n","    report_data['model_G_total_parameters'] = total_params_G\n","\n","    # Discriminator\n","    model_D = PredictionDiscriminatorRGCN(\n","        original_node_feature_dim=static_node_in_dim,\n","        prediction_dim=1,\n","        hidden_dim=config.get('cggan_disc_hidden_dim', 64),\n","        num_layers=config.get('cggan_disc_num_layers', 2),\n","        num_relations=num_relations,\n","        dropout_rate=config.get('cggan_disc_dropout_rate', 0.3)\n","    ).to(device)\n","    total_params_D = sum(p.numel() for p in model_D.parameters() if p.requires_grad)\n","    print(f\"模型总参数量 (Discriminator): {total_params_D:,}\");\n","    report_data['model_D_total_parameters'] = total_params_D\n","\n","\n","    # ===== Component FLOPS Calculation =====\n","    print(\"\\nCalculating MACs for model components (approximate FLOPS):\")\n","    report_data['component_gmacs'] = {}\n","    dummy_nodes_component = 2500\n","    dummy_edges_component = 60000\n","    dummy_batch_global_comp = 1\n","\n","    model_G.eval()\n","    model_D.eval()\n","\n","    # 1. NodeFeatureGeneratorRGCN (from model_G)\n","    try:\n","        gen_gnn_to_profile = model_G.node_feature_generator_h0\n","        dummy_x_gen_gnn = torch.randn(dummy_nodes_component, gen_gnn_to_profile.input_dim, device=device)\n","        dummy_ei_gen_gnn = torch.randint(0, dummy_nodes_component, (2, dummy_edges_component), device=device)\n","        dummy_ea_gen_gnn = torch.randn(dummy_edges_component, 5, device=device) # Assuming edge_attr has 5 features for RGCN\n","        if gen_gnn_to_profile.num_relations > 0 and dummy_ea_gen_gnn.shape[1] >=5:\n","             dummy_ea_gen_gnn[:, 4] = torch.randint(0, gen_gnn_to_profile.num_relations, (dummy_edges_component,), device=device).float()\n","        elif gen_gnn_to_profile.num_relations > 0 and dummy_ea_gen_gnn.shape[1] > 0 :\n","             dummy_ea_gen_gnn[:,0] = torch.randint(0, gen_gnn_to_profile.num_relations, (dummy_edges_component,), device=device).float()\n","\n","\n","        macs_gen_gnn = torchprofile.profile_macs(gen_gnn_to_profile, args=(dummy_x_gen_gnn, dummy_ei_gen_gnn, dummy_ea_gen_gnn))\n","        report_data['component_gmacs']['node_feature_generator_rgcn'] = macs_gen_gnn / 1e9\n","        print(f\"  NodeFeatureGeneratorRGCN GMACs: {macs_gen_gnn / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling NodeFeatureGeneratorRGCN: {e}\"); report_data['component_gmacs']['node_feature_generator_rgcn'] = \"Error\"\n","\n","    # 2. PredictionDiscriminatorRGCN\n","    try:\n","        disc_to_profile = model_D\n","        dummy_x_orig_disc = torch.randn(dummy_nodes_component, static_node_in_dim, device=device)\n","        dummy_y_cand_disc = torch.randn(dummy_nodes_component, 1, device=device)\n","        dummy_ei_disc = torch.randint(0, dummy_nodes_component, (2, dummy_edges_component), device=device)\n","        dummy_ea_disc = torch.randn(dummy_edges_component, 5, device=device) # Assuming edge_attr has 5 features\n","        if disc_to_profile.num_relations > 0 and dummy_ea_disc.shape[1] >=5:\n","             dummy_ea_disc[:,4] = torch.randint(0, disc_to_profile.num_relations, (dummy_edges_component,), device=device).float()\n","        elif disc_to_profile.num_relations > 0 and dummy_ea_disc.shape[1] > 0:\n","             dummy_ea_disc[:,0] = torch.randint(0, disc_to_profile.num_relations, (dummy_edges_component,), device=device).float()\n","\n","\n","        macs_disc = torchprofile.profile_macs(disc_to_profile, args=(dummy_x_orig_disc, dummy_y_cand_disc, dummy_ei_disc, dummy_ea_disc))\n","        report_data['component_gmacs']['prediction_discriminator_rgcn'] = macs_disc / 1e9\n","        print(f\"  PredictionDiscriminatorRGCN GMACs: {macs_disc / 1e9:.4f}\")\n","    except Exception as e:\n","        print(f\"  Error profiling PredictionDiscriminatorRGCN: {e}\"); report_data['component_gmacs']['prediction_discriminator_rgcn'] = \"Error\"\n","\n","    # 3. LSTM Layer (Manual MAC Calculation - from model_G)\n","    print(f\"  Manually Calculating MACs for LSTM Layer:\")\n","    try:\n","        lstm_layer = model_G.lstm\n","        N_nodes = dummy_nodes_component\n","        L_seq = T_PRED_HORIZON\n","        H_in = lstm_layer.input_size\n","        H_hidden = lstm_layer.hidden_size\n","        num_layers = lstm_layer.num_layers\n","        macs_lstm_manual = N_nodes * L_seq * 4 * (H_in * H_hidden + H_hidden * H_hidden)\n","        if num_layers > 1:\n","            macs_lstm_manual += N_nodes * L_seq * (num_layers - 1) * 4 * (H_hidden * H_hidden + H_hidden * H_hidden)\n","        gmacs_lstm_manual = macs_lstm_manual / 1e9\n","        report_data['component_gmacs']['lstm_layer'] = gmacs_lstm_manual\n","        report_data['component_gmacs']['lstm_layer_profiling_notes'] = \"Manually calculated based on formula.\"\n","        print(f\"  LSTM Parameters: input_size={H_in}, hidden_size={H_hidden}, num_layers={num_layers}\")\n","        print(f\"  Used for calculation: N_nodes={N_nodes}, L_seq={L_seq}\")\n","        print(f\"  LSTM Layer GMACs (Manual): {gmacs_lstm_manual:.4f} (for sequence length {L_seq})\")\n","    except Exception as e:\n","        print(f\"  Error manually calculating LSTM Layer MACs: {e}\")\n","        report_data['component_gmacs']['lstm_layer'] = \"Error\"\n","        report_data['component_gmacs']['lstm_layer_profiling_notes'] = f\"Error during manual calculation: {str(e)}\"\n","\n","    # 4. Other MLPs from model_G\n","    try:\n","        fusion_mlp_to_profile = model_G.fusion_mlp\n","        dummy_input_fusion_mlp = torch.randn(dummy_nodes_component, model_G.fusion_mlp_input_dim, device=device)\n","        macs_fusion_mlp = torchprofile.profile_macs(fusion_mlp_to_profile, args=(dummy_input_fusion_mlp,))\n","        report_data['component_gmacs']['fusion_mlp'] = macs_fusion_mlp / 1e9\n","        print(f\"  Fusion MLP GMACs: {macs_fusion_mlp / 1e9:.4f}\")\n","    except Exception as e: print(f\"  Error profiling Fusion MLP: {e}\"); report_data['component_gmacs']['fusion_mlp'] = \"Error\"\n","    try:\n","        pred_head_to_profile = model_G.hourly_prediction_heads[0]\n","        dummy_input_pred_head = torch.randn(dummy_nodes_component, model_G.lstm_hidden_dim, device=device)\n","        macs_pred_head = torchprofile.profile_macs(pred_head_to_profile, args=(dummy_input_pred_head,))\n","        report_data['component_gmacs']['prediction_head_mlp'] = macs_pred_head / 1e9\n","        print(f\"  Prediction Head MLP GMACs: {macs_pred_head / 1e9:.4f}\")\n","    except Exception as e: print(f\"  Error profiling Prediction Head: {e}\"); report_data['component_gmacs']['prediction_head_mlp'] = \"Error\"\n","    try:\n","        encoder_to_profile = model_G.global_env_encoder\n","        dummy_input_encoder = torch.randn(dummy_batch_global_comp, model_G.global_env_in_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['global_env_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  Global Env Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e: print(f\"  Error profiling Global Env Encoder: {e}\"); report_data['component_gmacs']['global_env_encoder_mlp'] = \"Error\"\n","    try:\n","        encoder_to_profile = model_G.time_encoder\n","        dummy_input_encoder = torch.randn(dummy_batch_global_comp, model_G.time_in_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['time_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  Time Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e: print(f\"  Error profiling Time Encoder: {e}\"); report_data['component_gmacs']['time_encoder_mlp'] = \"Error\"\n","    try:\n","        encoder_to_profile = model_G.h0_c0_from_gnn_encoder\n","        dummy_input_encoder = torch.randn(dummy_nodes_component, model_G.gen_output_dim, device=device)\n","        macs_encoder = torchprofile.profile_macs(encoder_to_profile, args=(dummy_input_encoder,))\n","        report_data['component_gmacs']['h0_c0_from_gnn_encoder_mlp'] = macs_encoder / 1e9\n","        print(f\"  H0/C0 from GNN Encoder MLP GMACs: {macs_encoder / 1e9:.4f}\")\n","    except Exception as e: print(f\"  Error profiling H0/C0 from GNN Encoder: {e}\"); report_data['component_gmacs']['h0_c0_from_gnn_encoder_mlp'] = \"Error\"\n","\n","    model_G.train()\n","    model_D.train()\n","    # ===== End Component FLOPS Calculation =====\n","\n","    optimizer_G = torch.optim.Adam(model_G.parameters(), lr=config.get('lr_g', config.get('lr', 0.001)),\n","                                   betas=(config.get('beta1_g', 0.5), 0.999))\n","    optimizer_D = torch.optim.Adam(model_D.parameters(), lr=config.get('lr_d', config.get('lr', 0.001)),\n","                                   betas=(config.get('beta1_d', 0.5), 0.999))\n","\n","    scheduler_G = ReduceLROnPlateau(optimizer_G, mode='min', factor=0.5,\n","                                    patience=config.get('scheduler_patience_g', config.get('scheduler_patience', 20)), verbose=True)\n","\n","    best_val_loss_scaled = float('inf'); best_val_hourly_metrics_original = None; best_epoch = 0\n","    patience_counter = 0; max_epochs = config.get('max_epochs', 300); early_stopping_patience = config.get('early_stopping_patience', 45)\n","    lambda_adv = config.get('lambda_adv', 0.01)\n","    model_save_path = Path(config['results_dir']) / f\"best_cgganlstm_model_G_seed{seed}.pth\"\n","\n","    timeline_time_features_on_device = time_features_for_dataset.to(device)\n","    target_mean_cpu = target_mean.cpu(); target_std_cpu = target_std.cpu()\n","    target_mean_on_device = target_mean.to(device); target_std_on_device = target_std.to(device)\n","    epoch_times = []\n","\n","    for epoch in range(1, max_epochs + 1):\n","        avg_loss_G, avg_loss_D, avg_pred_loss, avg_adv_loss_G, epoch_duration = train_epoch_adversarial(\n","            model_G, model_D, train_loader, optimizer_G, optimizer_D,\n","            device, timeline_time_features_on_device,\n","            node_feat_mean, node_feat_std,\n","            target_mean_on_device, target_std_on_device,\n","            lambda_adv, T_PRED_HORIZON, static_node_in_dim\n","        )\n","        epoch_times.append(epoch_duration)\n","\n","        val_loss_scaled, val_hourly_metrics_original, _ = evaluate_epoch(\n","            model_G, val_loader, device,\n","            timeline_time_features_on_device,\n","            node_feat_mean, node_feat_std,\n","            target_mean_cpu, target_std_cpu,\n","            epoch_type=\"Validation\"\n","        )\n","\n","        scheduler_G.step(val_loss_scaled)\n","\n","        print(f\"Epoch {epoch:03d} | G Pred MSE: {avg_pred_loss:.4f} | G Adv: {avg_adv_loss_G:.4f} | D Loss: {avg_loss_D:.4f} | Val Scaled MSE: {val_loss_scaled:.4f} | LR_G: {optimizer_G.param_groups[0]['lr']:.6f} | Time: {epoch_duration:.2f}s\")\n","        _print_hourly_metrics_summary(\"Val\", val_hourly_metrics_original, T_PRED_HORIZON, indent=\"                     \")\n","\n","        if val_loss_scaled < best_val_loss_scaled:\n","            best_val_loss_scaled = val_loss_scaled; best_val_hourly_metrics_original = val_hourly_metrics_original; best_epoch = epoch\n","            patience_counter = 0;\n","            torch.save(model_G.state_dict(), model_save_path)\n","            print(f\"                     ---> Best Generator model saved (Epoch: {epoch}, Val Scaled MSE: {best_val_loss_scaled:.4f})\")\n","        else:\n","            patience_counter += 1\n","        if patience_counter >= early_stopping_patience:\n","            print(f\"Early stopping at epoch {epoch} due to no improvement.\"); break\n","\n","    total_training_duration = time.time() - train_start_time\n","    report_data['total_training_time_seconds'] = total_training_duration\n","    report_data['average_epoch_time_seconds'] = np.mean(epoch_times) if epoch_times else np.nan\n","    report_data['num_epochs_trained'] = epoch; report_data['best_validation_epoch'] = best_epoch\n","    report_data['best_validation_scaled_mse'] = best_val_loss_scaled\n","\n","    model_G_for_eval = CGGANLSTMModelWithHourlyHeads(\n","        static_node_in_dim=static_node_in_dim, global_env_in_dim=global_env_in_dim, time_in_dim=time_in_dim,\n","        global_env_emb_dim=config.get('global_env_emb_dim', 16), time_emb_dim=config.get('time_emb_dim', 8),\n","        gen_hidden_dim=config.get('gcn_hidden_dim', 128), gen_output_dim=config.get('gcn_output_dim', 128),\n","        gen_num_layers=config.get('cggan_gen_num_layers', 2), gen_dropout_rate=config.get('dropout_rate_gcn', 0.3),\n","        num_relations=num_relations,\n","        lstm_hidden_dim=config.get('gru_hidden_dim', 128),\n","        fusion_mlp_output_dim=config.get('fusion_mlp_output_dim', 128),\n","        fusion_mlp_hidden_dim=config.get('fusion_mlp_hidden_dim', 64),\n","        dropout_rate_fusion_mlp=config.get('dropout_rate_fusion_mlp', 0.2),\n","        num_lstm_layers=config.get('num_gru_layers', 1), T_pred_horizon=T_PRED_HORIZON,\n","        dropout_rate_encoders=config.get('dropout_rate_encoders', 0.1),\n","        dropout_rate_lstm=config.get('dropout_rate_gru', 0.2),\n","        mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim', 64),\n","        dropout_rate_pred_head=config.get('dropout_rate_pred_head', 0.2)\n","    ).to(device)\n","    try:\n","        model_G_for_eval.load_state_dict(torch.load(model_save_path, map_location=device))\n","        print(f\"Successfully loaded best Generator model from {model_save_path}\")\n","    except Exception as e:\n","        print(f\"无法加载最佳Generator模型 ({e})，将使用训练循环结束时的Generator模型。\")\n","        model_G_for_eval = model_G\n","\n","    print(\"\\n评估最佳Generator模型在训练集上...\");\n","    best_model_train_loss_scaled, best_model_train_hourly_metrics, train_eval_duration = evaluate_epoch(\n","        model_G_for_eval, train_loader, device, timeline_time_features_on_device,\n","        node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu,\n","        epoch_type=\"Best Model on Train\"\n","    )\n","    report_data['best_model_train_set_metrics_hourly'] = best_model_train_hourly_metrics\n","    report_data['best_model_train_set_metrics_aggregated'] = calculate_aggregated_metrics_report(best_model_train_hourly_metrics, T_PRED_HORIZON)\n","    report_data['best_model_train_set_eval_time_seconds'] = train_eval_duration\n","    _print_hourly_metrics_summary(\"最佳模型训练集\", best_model_train_hourly_metrics, T_PRED_HORIZON)\n","\n","    report_data['best_model_validation_set_metrics_hourly'] = best_val_hourly_metrics_original\n","    if best_val_hourly_metrics_original:\n","        report_data['best_model_validation_set_metrics_aggregated'] = calculate_aggregated_metrics_report(best_val_hourly_metrics_original, T_PRED_HORIZON)\n","    _print_hourly_metrics_summary(\"最佳模型验证集\", report_data['best_model_validation_set_metrics_hourly'], T_PRED_HORIZON)\n","\n","\n","    print(\"\\n评估最佳Generator模型在测试集上...\");\n","    test_loss_scaled, test_hourly_metrics_original, test_inference_duration = evaluate_epoch(\n","        model_G_for_eval, test_loader, device, timeline_time_features_on_device,\n","        node_feat_mean, node_feat_std, target_mean_cpu, target_std_cpu, epoch_type=\"Test\"\n","    )\n","    report_data['test_set_inference_time_seconds'] = test_inference_duration\n","    report_data['best_model_test_set_metrics_hourly'] = test_hourly_metrics_original\n","    report_data['best_model_test_set_metrics_aggregated'] = calculate_aggregated_metrics_report(test_hourly_metrics_original, T_PRED_HORIZON)\n","    print(\"\\n\" + \"=\"*20 + \" 最终测试集评估结果 (CGGAN-LSTM) \" + \"=\"*20); print(f\"平均测试 Scaled MSE: {test_loss_scaled:.4f}\")\n","    _print_hourly_metrics_summary(\"测试集\", test_hourly_metrics_original, T_PRED_HORIZON)\n","\n","    agg_test = report_data['best_model_test_set_metrics_aggregated']\n","    print(f\"平均测试 MSE (Orig) : {agg_test.get('avg_mse', np.nan):.4f} (Std: {agg_test.get('std_mse', np.nan):.4f})\")\n","    print(f\"平均测试 R2 (Orig)  : {agg_test.get('avg_r2', np.nan):.4f} (Std: {agg_test.get('std_r2', np.nan):.4f})\")\n","    print(f\"平均测试 MAE (Orig) : {agg_test.get('avg_mae', np.nan):.4f} (Std: {agg_test.get('std_mae', np.nan):.4f})\")\n","    print(f\"平均测试 RMSE (Orig): {agg_test.get('avg_rmse', np.nan):.4f} (Std: {agg_test.get('std_rmse', np.nan):.4f})\")\n","    print(\"=\"*70)\n","\n","    report_file_path = Path(config['results_dir']) / f\"training_report_cggan_lstm_seed{seed}.json\"\n","    try:\n","        class NpEncoder(json.JSONEncoder):\n","            def default(self, obj):\n","                if isinstance(obj, np.integer): return int(obj)\n","                if isinstance(obj, np.floating): return float(obj)\n","                if isinstance(obj, np.ndarray): return obj.tolist()\n","                if isinstance(obj, torch.Tensor): return obj.tolist()\n","                if isinstance(obj, Path): return str(obj)\n","                return super(NpEncoder, self).default(obj)\n","        with open(report_file_path, 'w') as f: json.dump(report_data, f, indent=4, cls=NpEncoder)\n","        print(f\"训练报告已保存到: {report_file_path}\")\n","    except Exception as e: print(f\"保存训练报告失败: {e}\")\n","\n","    return model_G_for_eval, node_feat_mean, node_feat_std, target_mean, target_std\n","\n","def _print_hourly_metrics_summary(set_name, hourly_metrics, T_pred_horizon, indent=\"  \"):\n","    # ... (identical) ...\n","    if hourly_metrics is None:\n","        print(f\"{indent}{set_name} metrics not available.\")\n","        return\n","    print(f\"\\n{indent}每小时 {set_name} 指标 (Original Scale):\")\n","    header_printed = False\n","    for hour_idx in range(T_pred_horizon):\n","        metrics = hourly_metrics.get(hour_idx, {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0})\n","        if not header_printed:\n","            print(f\"{indent}  Hour | {'R2':>13s} | {'MSE':>14s} | {'MAE':>14s} | {'RMSE':>15s} | {'Count':>7s}\")\n","            header_printed = True\n","        print(f\"{indent}  {hour_idx:02d}   | {metrics.get('r2', np.nan):13.4f} | {metrics.get('mse', np.nan):14.4f} | {metrics.get('mae', np.nan):14.4f} | {metrics.get('rmse', np.nan):15.4f} | {metrics.get('count', 0):7d}\")\n","    aggregated = calculate_aggregated_metrics_report(hourly_metrics, T_pred_horizon)\n","    print(f\"{indent}  Aggregated Avg R2   : {aggregated.get('avg_r2', np.nan):.4f} (Std: {aggregated.get('std_r2', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MSE  : {aggregated.get('avg_mse', np.nan):.4f} (Std: {aggregated.get('std_mse', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MAE  : {aggregated.get('avg_mae', np.nan):.4f} (Std: {aggregated.get('std_mae', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg RMSE : {aggregated.get('avg_rmse', np.nan):.4f} (Std: {aggregated.get('std_rmse', np.nan):.4f})\")\n","\n","# ===========================================================\n","# 6. 主执行块\n","# ===========================================================\n","if __name__ == \"__main__\":\n","    # gc.collect()\n","    # if torch.cuda.is_available(): torch.cuda.empty_cache()\n","\n","    DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","\n","    if not DRIVE_BASE_PATH.exists(): DRIVE_BASE_PATH.mkdir(parents=True, exist_ok=True)\n","\n","    DATA_SUBDIR = Path(\"Result/Sequential_12Hour_Data\")\n","    DATA_FILENAME = \"graph_seq_20230503_SeqH7to19_NpyH8fill0.0.pkl\"\n","    RESULTS_SUBDIR = Path(\"Result/Final_CGGANLSTM1\")\n","    RESULTS_SAVE_DIR = DRIVE_BASE_PATH / RESULTS_SUBDIR\n","    os.makedirs(RESULTS_SAVE_DIR, exist_ok=True)\n","    DATA_PATH = DRIVE_BASE_PATH / DATA_SUBDIR / DATA_FILENAME\n","\n","\n","    DATA_YEAR = 2023; DATA_MONTH = 5; DATA_DAY = 3\n","    START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES = 8; PREDICTION_HORIZON = 12\n","\n","    training_config = {\n","        'seed': 42, 'batch_size': 8, # Keep consistent with previous runs for data loading\n","        'lr_g': 0.0002, 'lr_d': 0.00005, 'beta1_g': 0.5, 'beta1_d': 0.5, # GAN specific, might need tuning\n","        'lambda_adv': 0.001, # Weight for adversarial loss - start small, tune if needed\n","\n","        'max_epochs': 1000, 'scheduler_patience_g': 20, 'early_stopping_patience': 45, # Standard training setup\n","        'T_pred_horizon': PREDICTION_HORIZON,\n","        'results_dir': str(RESULTS_SAVE_DIR),\n","\n","        'global_env_emb_dim': 16, 'time_emb_dim': 8, # Consistent\n","\n","        # === Parameters for NodeFeatureGeneratorRGCN (Generator's GNN part) ===\n","        'gcn_hidden_dim': 128,          # Renamed to 'gen_hidden_dim' in model, maps to CGGAN_Generator hidden dim\n","        'gcn_output_dim': 128,          # Renamed to 'gen_output_dim' in model, output of Generator GNN (feeds LSTM pipeline)\n","        'cggan_gen_num_layers': 3,      # Your RGCNModule had 3 conv layers. Let's match that.\n","        'dropout_rate_gcn': 0.3,        # Renamed to 'gen_dropout_rate' in model, dropout in Generator GNN\n","\n","        # === Parameters for PredictionDiscriminatorRGCN ===\n","        'cggan_disc_hidden_dim': 64,    # Discriminator can be a bit smaller\n","        'cggan_disc_num_layers': 2,     # e.g., 2 layers for Discriminator GNN\n","        'cggan_disc_dropout_rate': 0.3,\n","\n","        # === Common GNN Parameter ===\n","        'num_relations': 5,             # Critical for RGCNConv in both G and D\n","\n","        # === Parameters for LSTM (using repurposed GRU keys as before) ===\n","        'gru_hidden_dim': 128,          # This will be lstm_hidden_dim\n","        'num_gru_layers': 1,            # This will be num_lstm_layers\n","        'dropout_rate_gru': 0.2,        # This will be dropout_rate_lstm\n","\n","        # === MLP and other parameters (match previous RGCN+LSTM setup) ===\n","        'mlp_prediction_hidden_dim': 64,\n","        'fusion_mlp_output_dim': 128,     # Should align with LSTM input if direct, or output of fusion MLP\n","        'fusion_mlp_hidden_dim': 64,\n","        'dropout_rate_fusion_mlp': 0.2,\n","        'dropout_rate_encoders': 0.1,\n","        'dropout_rate_pred_head': 0.2,\n","\n","        'use_amp': False, 'enable_profiler': False, 'num_workers': 0,\n","        'pin_memory': False, 'train_split_ratio': 0.7, 'val_split_ratio': 0.2,\n","        'h0_from_first_step': True\n","    }\n","\n","    all_graph_sequences_loaded = None\n","    try:\n","        if not DATA_PATH.exists(): raise FileNotFoundError(f\"数据文件在指定路径未找到: {DATA_PATH}\")\n","        with open(DATA_PATH, \"rb\") as f: all_graph_sequences_loaded = pickle.load(f)\n","        if not all_graph_sequences_loaded or not isinstance(all_graph_sequences_loaded, list) or not all_graph_sequences_loaded[0] or not isinstance(all_graph_sequences_loaded[0], list):\n","            raise ValueError(\"加载的数据格式不正确。\")\n","\n","        expected_len_per_sequence = training_config['T_pred_horizon'] + 1\n","\n","        processed_sequences = []\n","        for i, seq in enumerate(all_graph_sequences_loaded):\n","            if not isinstance(seq, list) or len(seq) != expected_len_per_sequence: continue\n","            valid_seq = True\n","            for step_idx, graph_step_data in enumerate(seq):\n","                if not isinstance(graph_step_data, Data) or not hasattr(graph_step_data, 'x') or graph_step_data.x is None or \\\n","                   not hasattr(graph_step_data, 'edge_index') or graph_step_data.edge_index is None or \\\n","                   not hasattr(graph_step_data, 'edge_attr') or graph_step_data.edge_attr is None or \\\n","                   graph_step_data.edge_attr.shape[1] < (5 if training_config.get('num_relations',0) > 0 else 1) or \\\n","                   not hasattr(graph_step_data, 'graph_global_env_features') or \\\n","                   (step_idx > 0 and (not hasattr(graph_step_data, 'y') or graph_step_data.y is None)):\n","                    if hasattr(graph_step_data, 'edge_attr') and graph_step_data.edge_attr is not None and \\\n","                       graph_step_data.edge_attr.shape[1] < (5 if training_config.get('num_relations',0) > 0 else 1) :\n","                       print(f\"Warning: Seq {i}, step {step_idx} has edge_attr dim {graph_step_data.edge_attr.shape[1]}, RGCN needs compatible dim.\")\n","                    valid_seq = False; break\n","                if step_idx > 0 and isinstance(graph_step_data.y, torch.Tensor) and graph_step_data.y.ndim == 1:\n","                    graph_step_data.y = graph_step_data.y.unsqueeze(1)\n","            if valid_seq: processed_sequences.append(seq)\n","        if not processed_sequences: raise ValueError(f\"数据处理后没有长度为 {expected_len_per_sequence} 的有效序列。\")\n","        all_graph_sequences = processed_sequences\n","        print(f\"成功加载并处理 {len(all_graph_sequences)} 个空间窗口的序列数据。\")\n","    except Exception as e: print(f\"加载或验证数据时发生错误: {e}\"); all_graph_sequences = None\n","\n","    if all_graph_sequences:\n","        base_datetime_for_timeline = dt_datetime(DATA_YEAR, DATA_MONTH, DATA_DAY, START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES)\n","        time_features_for_dataset_timeline = generate_time_features_for_sequence(\n","            base_datetime_for_timeline,\n","            training_config['T_pred_horizon']\n","        )\n","\n","        trained_model_G, final_node_mean, final_node_std, final_target_mean, final_target_std = main_training_cggan_lstm_hourly_heads(\n","            all_graph_sequences, training_config, time_features_for_dataset_timeline\n","        )\n","        print(\"CGGAN-LSTM 模型训练和评估完成!\")\n","    else:\n","        print(\"由于数据加载失败或数据为空，训练流程未启动。\")"]},{"cell_type":"markdown","metadata":{"id":"TajR3S03l2Rl"},"source":["##CGAN+LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RLj9zfyl5Oj"},"outputs":[],"source":["# ===========================================================\n","# 0. 环境 & 依赖\n","# ===========================================================\n","import os\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.loader import DataLoader\n","import pickle\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.metrics import r2_score\n","import gc\n","import math\n","from datetime import datetime as dt_datetime, timedelta\n","from pathlib import Path\n","import time\n","import json\n","import torchprofile\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","torch.cuda.ipc_collect()\n","\n","# ===========================================================\n","# 1. 特征生成 & 辅助模块\n","# ===========================================================\n","def generate_time_features_for_sequence(base_dt_obj, num_steps):\n","    time_features_list = []\n","    for i in range(num_steps):\n","        current_dt = base_dt_obj + timedelta(hours=i)\n","        hour_norm = current_dt.hour / 23.0\n","        day_of_year_norm = current_dt.timetuple().tm_yday / (366.0 if current_dt.year % 4 == 0 and (current_dt.year % 100 != 0 or current_dt.year % 400 == 0) else 365.0)\n","        hour_sin = math.sin(2 * math.pi * hour_norm); hour_cos = math.cos(2 * math.pi * hour_norm)\n","        doy_sin = math.sin(2 * math.pi * day_of_year_norm); doy_cos = math.cos(2 * math.pi * day_of_year_norm)\n","        time_features_list.append(torch.tensor([hour_sin, hour_cos, doy_sin, doy_cos], dtype=torch.float32))\n","    return torch.stack(time_features_list)\n","\n","class MLPEncoder(nn.Module):\n","    def __init__(self, in_dim, out_dim, hid_dim=None, dropout_rate=0.1, activation_fn=nn.ReLU, add_layer_norm=True):\n","        super().__init__()\n","        if hid_dim is None:\n","            hid_dim = max(min(in_dim, out_dim), (in_dim + out_dim) // 2)\n","            if hid_dim == 0 and out_dim > 0 : hid_dim = out_dim\n","            if hid_dim == 0 and in_dim > 0 : hid_dim = in_dim\n","            if hid_dim == 0 : hid_dim = 1\n","        layers = [nn.Linear(in_dim, hid_dim)]\n","        if activation_fn is not None: layers.append(activation_fn())\n","        if add_layer_norm: layers.append(nn.LayerNorm(hid_dim))\n","        layers.append(nn.Dropout(dropout_rate)); layers.append(nn.Linear(hid_dim, out_dim))\n","        self.mlp = nn.Sequential(*layers)\n","    def forward(self, x): return self.mlp(x)\n","\n","# ===========================================================\n","# 2. U-Net based Generator and PatchGAN Discriminator (Per-Pixel LSTM version)\n","# ===========================================================\n","class PreUNetFusionModule(nn.Module):\n","    def __init__(self, static_feat_dim, global_env_emb_dim, time_emb_dim, unet_input_channels, dropout_rate=0.1):\n","        super().__init__()\n","        input_channels = static_feat_dim + global_env_emb_dim + time_emb_dim\n","        self.fusion_convs = nn.Sequential(\n","            nn.Conv2d(input_channels, (input_channels + unet_input_channels)//2, 3, 1, 1, bias=False),\n","            nn.InstanceNorm2d((input_channels + unet_input_channels)//2), nn.ReLU(inplace=True), nn.Dropout2d(dropout_rate),\n","            nn.Conv2d((input_channels + unet_input_channels)//2, unet_input_channels, 3, 1, 1, bias=False),\n","            nn.InstanceNorm2d(unet_input_channels), nn.ReLU(inplace=True)\n","        )\n","        self.input_channels_for_profiling = input_channels\n","        self.output_channels_for_profiling = unet_input_channels\n","    def forward(self, static_img_feat, expanded_global_emb, expanded_time_emb):\n","        combined = torch.cat([static_img_feat, expanded_global_emb, expanded_time_emb], dim=1)\n","        return self.fusion_convs(combined)\n","\n","class UNetFeatureExtractorModule(nn.Module):\n","    def __init__(self, input_channels, output_feature_channels=32,\n","                 encoder_channels=(64, 128, 256), middle_channels=256, decoder_channels=(128, 64)):\n","        super().__init__()\n","        self.input_channels_for_profiling = input_channels\n","        self.output_feature_channels_for_profiling = output_feature_channels\n","        self.encoder1 = self._encoder_block(input_channels, encoder_channels[0])\n","        self.encoder2 = self._encoder_block(encoder_channels[0], encoder_channels[1])\n","        self.encoder3 = self._encoder_block(encoder_channels[1], encoder_channels[2])\n","        self.middle = nn.Sequential(nn.Conv2d(encoder_channels[2], middle_channels, 3,1,1,bias=False), nn.InstanceNorm2d(middle_channels), nn.ReLU(inplace=True))\n","        self.decoder3 = self._decoder_block(middle_channels + encoder_channels[2], decoder_channels[0])\n","        self.decoder2 = self._decoder_block(decoder_channels[0] + encoder_channels[1], decoder_channels[1])\n","        self.final_feature_conv = nn.Conv2d(decoder_channels[1] + encoder_channels[0], output_feature_channels, 1)\n","\n","    def _encoder_block(self, in_c, out_c, norm=True):\n","        layers = [nn.Conv2d(in_c, out_c, 4, 2, 1, bias=False)]\n","        if norm: layers.append(nn.InstanceNorm2d(out_c))\n","        layers.append(nn.LeakyReLU(0.2, inplace=True)); return nn.Sequential(*layers)\n","    def _decoder_block(self, in_c, out_c, norm=True):\n","        layers = [nn.ConvTranspose2d(in_c, out_c, 4, 2, 1, bias=False)]\n","        if norm: layers.append(nn.InstanceNorm2d(out_c))\n","        layers.append(nn.ReLU(inplace=True)); return nn.Sequential(*layers)\n","    def forward(self, x):\n","        e1 = self.encoder1(x); e2 = self.encoder2(e1); e3 = self.encoder3(e2)\n","        m = self.middle(e3)\n","        d3 = self.decoder3(torch.cat([m, e3], dim=1))\n","        if d3.size()[2:] != e2.size()[2:]: d3 = F.interpolate(d3, size=e2.size()[2:], mode='bilinear', align_corners=False)\n","        d2 = self.decoder2(torch.cat([d3, e2], dim=1))\n","        if d2.size()[2:] != e1.size()[2:]: d2 = F.interpolate(d2, size=e1.size()[2:], mode='bilinear', align_corners=False)\n","        feature_map = self.final_feature_conv(torch.cat([d2, e1], dim=1))\n","        return F.interpolate(feature_map, size=x.size()[2:], mode='bilinear', align_corners=False)\n","\n","class H0PixelFeatureExtractor(nn.Module):\n","    def __init__(self, static_feat_dim, # 16\n","                 unet_input_ch_for_h0_unet, # e.g. static_feat_dim, or static_feat_dim+const_emb\n","                 unet_feature_output_channels_for_h0, # Output of U-Net for h0 path\n","                 unet_enc_ch_list, unet_mid_ch, unet_dec_ch_list, # U-Net structure for H0\n","                 lstm_hidden_dim, num_lstm_layers, H_img=50, W_img=50, dropout_rate=0.1):\n","        super().__init__()\n","        self.num_lstm_layers = num_lstm_layers\n","        # This U-Net processes the 7am image to get features for h0\n","        self.h0_unet_feature_extractor = UNetFeatureExtractorModule(\n","            input_channels=unet_input_ch_for_h0_unet, # Typically static_feat_dim (16)\n","            output_feature_channels=unet_feature_output_channels_for_h0, # e.g., 32\n","            encoder_channels=unet_enc_ch_list,\n","            middle_channels=unet_mid_ch,\n","            decoder_channels=unet_dec_ch_list\n","        )\n","        # MLP to project each pixel's U-Net feature to LSTM hidden dim\n","        self.h0_pixel_projector = MLPEncoder(\n","            in_dim=unet_feature_output_channels_for_h0, # C_unet_out from h0_unet\n","            out_dim=lstm_hidden_dim,\n","            hid_dim= (unet_feature_output_channels_for_h0 + lstm_hidden_dim) // 2,\n","            dropout_rate=dropout_rate, add_layer_norm=False\n","        )\n","        self.static_feat_dim_for_profiling = static_feat_dim\n","        self.unet_feature_output_channels_for_h0_profiling = unet_feature_output_channels_for_h0\n","\n","\n","    def forward(self, static_7am_image_normalized): # [B, 16, H, W]\n","        # If h0_unet_feature_extractor needs specific input channels different from static_feat_dim,\n","        # prepare it here (e.g. if it also expects fused features, but usually not for h0)\n","        # For now, assuming h0_unet_feature_extractor takes static_7am_image_normalized directly\n","        # if its input_channels match static_feat_dim.\n","\n","        unet_features_7am = self.h0_unet_feature_extractor(static_7am_image_normalized) # [B, C_unet_out_h0, H, W]\n","        B, C_unet_out_h0, H, W = unet_features_7am.shape\n","\n","        pixel_features_7am = unet_features_7am.permute(0, 2, 3, 1).reshape(B * H * W, C_unet_out_h0) # [B*H*W, C_unet_out_h0]\n","        projected_pixel_features = self.h0_pixel_projector(pixel_features_7am) # [B*H*W, lstm_hidden_dim]\n","\n","        h0 = projected_pixel_features.unsqueeze(0).repeat(self.num_lstm_layers, 1, 1) # [Num_LSTM_Layers, B*H*W, lstm_hidden_dim]\n","        return h0\n","\n","class UNetPerPixelLSTMHeadModel_Generator(nn.Module): # GENERATOR\n","    def __init__(self, static_feat_dim, global_env_in_dim, time_in_dim, global_env_emb_dim, time_emb_dim,\n","                 unet_input_channels_after_fusion, dropout_rate_pre_fusion,\n","                 unet_feature_output_channels, unet_encoder_channels_list, unet_middle_channels_val, unet_decoder_channels_list,\n","                 # H0 Extractor specific U-Net params\n","                 h0_unet_input_channels, h0_unet_output_channels,\n","                 h0_unet_enc_ch_list, h0_unet_mid_ch, h0_unet_dec_ch_list,\n","                 lstm_hidden_dim, num_lstm_layers, dropout_rate_lstm,\n","                 mlp_prediction_hidden_dim, dropout_rate_pred_head,\n","                 T_pred_horizon, H_img=50, W_img=50, dropout_rate_other_mlps=0.1):\n","        super().__init__()\n","        self.T_pred_horizon = T_pred_horizon; self.static_feat_dim = static_feat_dim\n","        self.H_img, self.W_img = H_img, W_img\n","        self.unet_input_channels_after_fusion_for_profiling = unet_input_channels_after_fusion\n","        self.unet_feature_output_channels_for_profiling = unet_feature_output_channels\n","\n","        self.global_env_encoder = MLPEncoder(global_env_in_dim, global_env_emb_dim, dropout_rate=dropout_rate_other_mlps)\n","        self.time_encoder = MLPEncoder(time_in_dim, time_emb_dim, dropout_rate=dropout_rate_other_mlps)\n","        self.pre_unet_fusion = PreUNetFusionModule(static_feat_dim, global_env_emb_dim, time_emb_dim, unet_input_channels_after_fusion, dropout_rate_pre_fusion)\n","\n","        # Main U-Net for per-timestep feature extraction\n","        self.unet_feature_extractor = UNetFeatureExtractorModule(unet_input_channels_after_fusion, unet_feature_output_channels, unet_encoder_channels_list, unet_middle_channels_val, unet_decoder_channels_list)\n","\n","        # H0 Pixel Feature Extractor\n","        self.h0_pixel_feature_extractor = H0PixelFeatureExtractor(\n","            static_feat_dim=static_feat_dim, # Input to its own U-Net is the 16-channel static image\n","            unet_input_ch_for_h0_unet=h0_unet_input_channels, # Configurable, typically static_feat_dim\n","            unet_feature_output_channels_for_h0=h0_unet_output_channels, # Output of its U-Net\n","            unet_enc_ch_list=h0_unet_enc_ch_list, unet_mid_ch=h0_unet_mid_ch, unet_dec_ch_list=h0_unet_dec_ch_list,\n","            lstm_hidden_dim=lstm_hidden_dim, num_lstm_layers=num_lstm_layers,\n","            H_img=H_img, W_img=W_img, dropout_rate=dropout_rate_other_mlps\n","        )\n","\n","        self.lstm_input_size_for_profiling = unet_feature_output_channels # LSTM input is C_unet_out (per pixel)\n","        self.lstm_hidden_dim_for_profiling = lstm_hidden_dim\n","        self.lstm = nn.LSTM(unet_feature_output_channels, lstm_hidden_dim, num_lstm_layers, batch_first=True, dropout=dropout_rate_lstm if num_lstm_layers > 1 else 0.0)\n","\n","        self.hourly_prediction_heads = nn.ModuleList()\n","        for _ in range(T_pred_horizon): # Output 1 value per pixel\n","            self.hourly_prediction_heads.append(MLPEncoder(lstm_hidden_dim, 1, mlp_prediction_hidden_dim, dropout_rate_pred_head, add_layer_norm=False, activation_fn=None)) # No activation for regression output\n","\n","        self.register_buffer('static_image_feat_mean', torch.zeros(static_feat_dim))\n","        self.register_buffer('static_image_feat_std', torch.ones(static_feat_dim))\n","\n","    def forward(self, list_of_batched_timesteps: list, timeline_time_features: torch.Tensor, device: torch.device):\n","        pyg_batch_7am = list_of_batched_timesteps[0].to(device)\n","        current_B = pyg_batch_7am.num_graphs; nodes_per_graph = self.H_img * self.W_img\n","\n","        static_x_image_flat = pyg_batch_7am.x\n","        normalized_static_x_flat = (static_x_image_flat - self.static_image_feat_mean.to(device)) / (self.static_image_feat_std.to(device) + 1e-8)\n","        static_x_image_normalized = normalized_static_x_flat.view(current_B, nodes_per_graph, self.static_feat_dim).permute(0,2,1).contiguous().view(current_B, self.static_feat_dim, self.H_img, self.W_img)\n","\n","        h0 = self.h0_pixel_feature_extractor(static_x_image_normalized); c0 = torch.zeros_like(h0, device=device)\n","        lstm_initial_state = (h0, c0)\n","\n","        all_unet_output_pixel_features_T = []; all_fused_images_for_unet_T = []\n","        for t_pred_idx in range(self.T_pred_horizon):\n","            pyg_batch_t = list_of_batched_timesteps[t_pred_idx + 1].to(device)\n","            global_env_feat_t_unenc = pyg_batch_t.graph_global_env_features\n","            exp_glob_dim = self.global_env_encoder.mlp[0].in_features\n","            if not (global_env_feat_t_unenc.shape==(current_B,exp_glob_dim)): global_env_feat_t_unenc=torch.zeros(current_B,exp_glob_dim,device=device)\n","            glob_emb_t = self.global_env_encoder(global_env_feat_t_unenc).unsqueeze(-1).unsqueeze(-1).expand(-1,-1,self.H_img,self.W_img)\n","            time_emb_t = self.time_encoder(timeline_time_features[t_pred_idx,:].to(device)).unsqueeze(0).unsqueeze(-1).unsqueeze(-1).expand(current_B,-1,self.H_img,self.W_img)\n","\n","            fused_img_t = self.pre_unet_fusion(static_x_image_normalized, glob_emb_t, time_emb_t) # Use normalized static for consistency\n","            all_fused_images_for_unet_T.append(fused_img_t)\n","            unet_feat_map_t = self.unet_feature_extractor(fused_img_t) # [B, C_unet_out, H, W]\n","            # Permute and reshape for per-pixel LSTM: [B, C, H, W] -> [B, H, W, C] -> [B*H*W, C]\n","            pixel_feats_t = unet_feat_map_t.permute(0,2,3,1).reshape(current_B * nodes_per_graph, -1)\n","            all_unet_output_pixel_features_T.append(pixel_feats_t)\n","\n","        lstm_input_sequence = torch.stack(all_unet_output_pixel_features_T, dim=1) # [B*H*W, T, C_unet_out]\n","        stacked_fused_images_for_unet = torch.stack(all_fused_images_for_unet_T, dim=1) # [B, T, C_fused, H, W] (for D)\n","\n","        lstm_out_sequence, _ = self.lstm(lstm_input_sequence, lstm_initial_state) # [B*H*W, T, LSTM_Hidden_Dim]\n","\n","        all_hourly_predictions_flat_pixels = []\n","        for t in range(self.T_pred_horizon):\n","            pred_pixels_t = self.hourly_prediction_heads[t](lstm_out_sequence[:, t, :]) # [B*H*W, 1]\n","            all_hourly_predictions_flat_pixels.append(pred_pixels_t.squeeze(-1)) # [B*H*W]\n","\n","        stacked_predictions_flat = torch.stack(all_hourly_predictions_flat_pixels, dim=1) # [B*H*W, T] (for eval metrics)\n","\n","        # Reshape for GAN L1 loss & D: [B*H*W, T] -> [B, H*W, T] -> [B, T, H*W] -> [B, T, 1, H, W]\n","        generated_sequence_images = stacked_predictions_flat.view(current_B, nodes_per_graph, self.T_pred_horizon)\\\n","                                     .permute(0,2,1).contiguous().view(current_B, self.T_pred_horizon, 1, self.H_img, self.W_img)\n","\n","        return generated_sequence_images, stacked_fused_images_for_unet, stacked_predictions_flat\n","\n","class PatchGANDiscriminator(nn.Module):\n","    def __init__(self, input_channels):\n","        super().__init__(); self.input_channels_for_profiling = input_channels\n","        self.model = nn.Sequential(\n","            self._disc_block(input_channels, 64, stride=2, normalize=False),\n","            self._disc_block(64, 128, stride=2), self._disc_block(128, 256, stride=2),\n","            self._disc_block(256, 512, stride=1, padding=1),\n","            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n","        )\n","    def _disc_block(self, in_c, out_c, stride=2, padding=1, normalize=True):\n","        layers = [nn.Conv2d(in_c, out_c, 4, stride, padding, bias=False)]\n","        if normalize: layers.append(nn.InstanceNorm2d(out_c, affine=True))\n","        layers.append(nn.LeakyReLU(0.2, inplace=True)); return nn.Sequential(*layers)\n","    def forward(self, x): return self.model(x)\n","\n","# ===========================================================\n","# 3. Loss Functions & Metrics (masked_l1_loss, calculate_hourly_metrics - as before)\n","# ===========================================================\n","# ... (masked_l1_loss and calculate_hourly_metrics are identical to previous cGAN code block)\n","def masked_l1_loss(generated_images, target_images, image_masks):\n","    if image_masks.sum() == 0: return torch.tensor(0.0, device=generated_images.device, requires_grad=True)\n","    if image_masks.ndim == 3: image_masks = image_masks.unsqueeze(1) # [B,H,W] -> [B,1,H,W]\n","    image_masks = image_masks.bool()\n","    valid_targets_mask = ~torch.isnan(target_images)\n","    final_mask = image_masks & valid_targets_mask\n","    if final_mask.sum() == 0: return torch.tensor(0.0, device=generated_images.device, requires_grad=True)\n","    diff = torch.abs(generated_images - target_images)\n","    return diff[final_mask].mean()\n","\n","def calculate_hourly_metrics(predictions_flat_scaled, targets_flat_scaled, node_masks_flat, target_mean, target_std):\n","    target_mean_cpu = target_mean.cpu(); target_std_cpu = target_std.cpu()\n","    preds_unscaled = predictions_flat_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","    targets_unscaled = targets_flat_scaled.clone().cpu() * (target_std_cpu + 1e-8) + target_mean_cpu\n","    num_nodes_total, T_horizon = preds_unscaled.shape; hourly_metrics_dict = {}\n","    preds_np = preds_unscaled.numpy(); targets_np = targets_unscaled.numpy()\n","    mask_np = node_masks_flat.cpu().numpy().astype(bool)\n","    for t in range(T_horizon):\n","        preds_t_all_nodes = preds_np[:, t]; targets_t_all_nodes = targets_np[:, t]\n","        preds_t_on_loss_nodes = preds_t_all_nodes[mask_np]; targets_t_on_loss_nodes = targets_t_all_nodes[mask_np]\n","        valid_target_data_mask_t = ~np.isnan(targets_t_on_loss_nodes)\n","        preds_t_final = preds_t_on_loss_nodes[valid_target_data_mask_t]; targets_t_final = targets_t_on_loss_nodes[valid_target_data_mask_t]\n","        if preds_t_final.shape[0] < 2: hourly_metrics_dict[t] = {'mse':np.nan,'mae':np.nan,'rmse':np.nan,'r2':np.nan,'count':0}; continue\n","        mse = np.mean((preds_t_final - targets_t_final)**2); mae = np.mean(np.abs(preds_t_final - targets_t_final))\n","        rmse = np.sqrt(mse); r2 = np.nan\n","        try: r2 = r2_score(targets_t_final, preds_t_final)\n","        except ValueError: pass\n","        hourly_metrics_dict[t] = {'mse':mse,'mae':mae,'rmse':rmse,'r2':r2,'count':preds_t_final.shape[0]}\n","    return hourly_metrics_dict\n","\n","# ===========================================================\n","# 4. GAN Training and Evaluation Epochs (Adapted for Per-Pixel LSTM)\n","# ===========================================================\n","# train_epoch_unet_lstm_cgan and evaluate_epoch_unet_lstm_cgan are largely the same logic\n","# as the previous cGAN, but ensure G's outputs are handled correctly.\n","def train_epoch_unet_pixel_lstm_cgan( # Renamed\n","    model_G, model_D, loader, optimizer_G, optimizer_D,\n","    criterion_GAN, criterion_L1, lambda_L1,\n","    device, timeline_time_features, H_img, W_img,\n","    static_img_feat_mean, static_img_feat_std, target_mean, target_std\n","):\n","    model_G.train(); model_D.train()\n","    total_loss_G, total_loss_D, total_loss_G_GAN, total_loss_G_L1 = 0,0,0,0\n","    num_sequences_processed = 0\n","    model_G.static_image_feat_mean = static_img_feat_mean.to(device)\n","    model_G.static_image_feat_std = static_img_feat_std.to(device)\n","    target_mean_dev = target_mean.to(device); target_std_dev = target_std.to(device)\n","    epoch_start_time = time.time()\n","\n","    for list_of_batched_timesteps in loader:\n","        current_B = list_of_batched_timesteps[0].num_graphs\n","        # G forward returns: generated_img_seq [B,T,1,H,W], stacked_fused_inputs [B,T,C_fused,H,W], preds_flat [B*H*W,T]\n","        generated_sequence_images, stacked_fused_conditions, _ = model_G(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","\n","        # --- Train Discriminator ---\n","        optimizer_D.zero_grad()\n","        loss_D_real_accum = 0; loss_D_fake_accum = 0\n","        for t in range(model_G.T_pred_horizon):\n","            real_target_y_flat = list_of_batched_timesteps[t+1].y.to(device).squeeze()\n","            real_target_images_t = real_target_y_flat.view(current_B, 1, H_img, W_img)\n","            real_target_images_scaled_t = (real_target_images_t - target_mean_dev) / (target_std_dev + 1e-8)\n","            fused_condition_t = stacked_fused_conditions[:, t, :, :, :]\n","            D_input_real = torch.cat((fused_condition_t, real_target_images_scaled_t), dim=1)\n","            pred_real = model_D(D_input_real)\n","            loss_D_real_accum += criterion_GAN(pred_real, torch.ones_like(pred_real, device=device))\n","            fake_images_t = generated_sequence_images[:, t, :, :, :].detach()\n","            D_input_fake = torch.cat((fused_condition_t, fake_images_t), dim=1)\n","            pred_fake = model_D(D_input_fake)\n","            loss_D_fake_accum += criterion_GAN(pred_fake, torch.zeros_like(pred_fake, device=device))\n","        loss_D = 0.5 * (loss_D_real_accum + loss_D_fake_accum) / model_G.T_pred_horizon # Average over T\n","        if not torch.isnan(loss_D): loss_D.backward(); optimizer_D.step()\n","        total_loss_D += loss_D.item() * current_B\n","\n","        # --- Train Generator ---\n","        optimizer_G.zero_grad()\n","        loss_G_GAN_accum = 0; loss_G_L1_accum = 0\n","        building_mask_flat = list_of_batched_timesteps[1].building_mask.to(device) # [B*H*W]\n","        image_mask_for_loss = (~building_mask_flat).view(current_B, 1, H_img, W_img) # [B,1,H,W]\n","\n","        for t in range(model_G.T_pred_horizon):\n","            fake_images_t = generated_sequence_images[:, t, :, :, :]\n","            fused_condition_t = stacked_fused_conditions[:, t, :, :, :]\n","            D_input_for_G = torch.cat((fused_condition_t, fake_images_t), dim=1)\n","            pred_fake_for_G = model_D(D_input_for_G)\n","            loss_G_GAN_accum += criterion_GAN(pred_fake_for_G, torch.ones_like(pred_fake_for_G, device=device))\n","\n","            real_target_y_flat = list_of_batched_timesteps[t+1].y.to(device).squeeze()\n","            real_target_images_t = real_target_y_flat.view(current_B, 1, H_img, W_img)\n","            real_target_images_scaled_t = (real_target_images_t - target_mean_dev) / (target_std_dev + 1e-8)\n","            loss_G_L1_accum += masked_l1_loss(fake_images_t, real_target_images_scaled_t, image_mask_for_loss)\n","\n","        avg_loss_G_GAN = loss_G_GAN_accum / model_G.T_pred_horizon\n","        avg_loss_G_L1 = loss_G_L1_accum / model_G.T_pred_horizon\n","        loss_G = avg_loss_G_GAN + lambda_L1 * avg_loss_G_L1\n","        if not torch.isnan(loss_G): loss_G.backward(); optimizer_G.step()\n","        total_loss_G += loss_G.item() * current_B\n","        total_loss_G_GAN += avg_loss_G_GAN.item() * current_B\n","        total_loss_G_L1 += avg_loss_G_L1.item() * current_B\n","        num_sequences_processed += current_B\n","\n","    epoch_duration = time.time() - epoch_start_time\n","    avg_loss_G = total_loss_G/num_sequences_processed if num_sequences_processed > 0 else 0\n","    avg_loss_D = total_loss_D/num_sequences_processed if num_sequences_processed > 0 else 0\n","    avg_loss_G_GAN = total_loss_G_GAN/num_sequences_processed if num_sequences_processed > 0 else 0\n","    avg_loss_G_L1 = total_loss_G_L1/num_sequences_processed if num_sequences_processed > 0 else 0\n","    return avg_loss_G, avg_loss_D, avg_loss_G_GAN, avg_loss_G_L1, epoch_duration\n","\n","def evaluate_epoch_unet_pixel_lstm_cgan( # Renamed\n","    model_G, loader, criterion_L1, device, timeline_time_features, H_img, W_img,\n","    static_img_feat_mean, static_img_feat_std, target_mean, target_std, lambda_L1, epoch_type=\"Eval\"\n","):\n","    model_G.eval()\n","    all_batch_predictions_flat_scaled = [] # This comes from model_G's 3rd output\n","    all_batch_targets_flat_scaled = []\n","    all_batch_masks_flat_for_metrics = []\n","    total_eval_loss_G_L1_scaled = 0\n","    num_sequences_processed = 0\n","    model_G.static_image_feat_mean = static_img_feat_mean.to(device)\n","    model_G.static_image_feat_std = static_img_feat_std.to(device)\n","    target_mean_dev = target_mean.to(device); target_std_dev = target_std.to(device)\n","    eval_start_time = time.time()\n","\n","    with torch.no_grad():\n","        for list_of_batched_timesteps in loader:\n","            current_B = list_of_batched_timesteps[0].num_graphs\n","            generated_sequence_images, _, predictions_final_flat_scaled = model_G(list_of_batched_timesteps, timeline_time_features.to(device), device)\n","\n","            eval_loss_G_L1_accum_batch = 0\n","            building_mask_flat = list_of_batched_timesteps[1].building_mask.to(device)\n","            image_mask_for_loss = (~building_mask_flat).view(current_B, 1, H_img, W_img)\n","\n","            temp_targets_scaled_T_list = []\n","            for t in range(model_G.T_pred_horizon):\n","                fake_images_t = generated_sequence_images[:, t, :, :, :]\n","                real_target_y_flat = list_of_batched_timesteps[t+1].y.to(device).squeeze()\n","                real_target_images_t = real_target_y_flat.view(current_B, 1, H_img, W_img)\n","                real_target_images_scaled_t = (real_target_images_t - target_mean_dev) / (target_std_dev + 1e-8)\n","                eval_loss_G_L1_accum_batch += masked_l1_loss(fake_images_t, real_target_images_scaled_t, image_mask_for_loss)\n","                temp_targets_scaled_T_list.append(real_target_images_scaled_t.permute(0,2,3,1).reshape(-1)) # Flatten for metrics\n","\n","            total_eval_loss_G_L1_scaled += (eval_loss_G_L1_accum_batch / model_G.T_pred_horizon).item() * current_B\n","            all_batch_predictions_flat_scaled.append(predictions_final_flat_scaled.cpu())\n","            all_batch_targets_flat_scaled.append(torch.stack(temp_targets_scaled_T_list, dim=1).cpu()) # [B*H*W, T]\n","            all_batch_masks_flat_for_metrics.append((~list_of_batched_timesteps[1].building_mask).cpu())\n","            num_sequences_processed += current_B\n","\n","    eval_duration = time.time() - eval_start_time\n","    avg_eval_loss_G_L1_scaled = total_eval_loss_G_L1_scaled / num_sequences_processed if num_sequences_processed > 0 else 0\n","    if not all_batch_predictions_flat_scaled:\n","        empty_metrics = {t: {'mse':np.nan,'mae':np.nan,'rmse':np.nan,'r2':np.nan,'count':0} for t in range(model_G.T_pred_horizon)}\n","        return avg_eval_loss_G_L1_scaled, empty_metrics, eval_duration\n","    final_predictions_flat_scaled = torch.cat(all_batch_predictions_flat_scaled, dim=0)\n","    final_targets_flat_scaled = torch.cat(all_batch_targets_flat_scaled, dim=0)\n","    final_masks_flat_for_metrics = torch.cat(all_batch_masks_flat_for_metrics, dim=0)\n","    hourly_metrics_original_scale = calculate_hourly_metrics(final_predictions_flat_scaled, final_targets_flat_scaled, final_masks_flat_for_metrics, target_mean.cpu(), target_std.cpu())\n","    return avg_eval_loss_G_L1_scaled, hourly_metrics_original_scale, eval_duration\n","\n","# ===========================================================\n","# 5. 主训练流程 (U-Net Per-Pixel LSTM + Heads + cGAN)\n","# ===========================================================\n","def calculate_aggregated_metrics_report(hourly_metrics_dict, T_pred_horizon): # No change\n","    # ... (Identical)\n","    metrics_to_aggregate = ['r2', 'mse', 'mae', 'rmse']\n","    aggregated_report = {}\n","    for metric_name in metrics_to_aggregate:\n","        values = [hourly_metrics_dict[t][metric_name] for t in range(T_pred_horizon) if t in hourly_metrics_dict and not np.isnan(hourly_metrics_dict[t][metric_name])]\n","        if values: aggregated_report[f'avg_{metric_name}'] = np.mean(values); aggregated_report[f'std_{metric_name}'] = np.std(values)\n","        else: aggregated_report[f'avg_{metric_name}'] = np.nan; aggregated_report[f'std_{metric_name}'] = np.nan\n","    return aggregated_report\n","\n","def main_training_unet_pixel_lstm_cgan( # Renamed\n","    all_sequences_data: list, config: dict, time_features_for_dataset: torch.Tensor\n","):\n","    train_start_time = time.time(); report_data = {'config': config}\n","    seed = config.get('seed', 42); np.random.seed(seed); torch.manual_seed(seed)\n","    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"使用设备: {device}\"); report_data['device'] = str(device)\n","    T_PRED_HORIZON = config.get('T_pred_horizon',12); H_IMG=config.get('H_IMG',50); W_IMG=config.get('W_IMG',50)\n","    nodes_per_image = H_IMG*W_IMG; expected_input_len = T_PRED_HORIZON+1\n","\n","    # Data validation and splitting (Identical to previous main_training function)\n","    valid_sequences_data = [] # ... (Copy from previous main_training) ...\n","    for seq in all_sequences_data:\n","        if isinstance(seq, list) and len(seq) == expected_input_len:\n","            valid_step = True\n","            for step_data in seq:\n","                if not (hasattr(step_data,'x') and step_data.x is not None and step_data.x.shape[0]%nodes_per_image==0 and step_data.x.shape[1]==config.get('static_feat_dim',16)):\n","                    valid_step=False; break\n","            if valid_step: valid_sequences_data.append(seq)\n","    if len(valid_sequences_data) != len(all_sequences_data): print(f\"警告:筛选出{len(valid_sequences_data)}/{len(all_sequences_data)}有效序列。\")\n","    if not valid_sequences_data: raise ValueError(\"无有效序列数据。\")\n","    all_sequences_data = valid_sequences_data\n","    num_total_sequences = len(all_sequences_data); indices = np.random.permutation(num_total_sequences)\n","    train_s_r=config.get('train_split_ratio',0.7); val_s_r=config.get('val_split_ratio',0.2)\n","    train_size=int(train_s_r*num_total_sequences); val_size=int(val_s_r*num_total_sequences)\n","    train_idx=indices[:train_size]; val_idx=indices[train_size:train_size+val_size]; test_idx=indices[train_size+val_size:]\n","    train_dataset=[all_sequences_data[i] for i in train_idx]; val_dataset=[all_sequences_data[i] for i in val_idx]; test_dataset=[all_sequences_data[i] for i in test_idx]\n","    report_data['dataset_split']={'total':num_total_sequences,'train':len(train_dataset),'val':len(val_dataset),'test':len(test_dataset)}\n","\n","    # Scaler calculation (Identical)\n","    all_7am_node_feat_list = [s[0].x for s in train_dataset if hasattr(s[0],'x') and s[0].x is not None] # ... (Copy) ...\n","    if not all_7am_node_feat_list: raise ValueError(\"训练数据7am无特征x！\")\n","    all_7am_tensor = torch.cat(all_7am_node_feat_list,dim=0)\n","    static_img_mean=torch.mean(all_7am_tensor,dim=0); static_img_std=torch.std(all_7am_tensor,dim=0)\n","    static_img_std[static_img_std<1e-8]=1.0\n","    scaler_path_x = Path(config['results_dir'])/\"static_img_scaler_unet_pixel_lstm_cgan.pth\"\n","    torch.save({'mean':static_img_mean,'std':static_img_std},scaler_path_x);print(f\"静态图像特征scaler:{scaler_path_x}\")\n","    all_train_tgt_val_list = [] # ... (Copy target scaler code) ...\n","    for seq in train_dataset:\n","        for i_s, g_data in enumerate(seq):\n","            if i_s>0 and hasattr(g_data,'y') and g_data.y is not None:\n","                y_orig=g_data.y.squeeze(); mask_loss=~g_data.building_mask; valid_idx=mask_loss&~torch.isnan(y_orig)\n","                if valid_idx.sum()>0: all_train_tgt_val_list.append(y_orig[valid_idx])\n","    if not all_train_tgt_val_list: tgt_mean=torch.tensor(0.0); tgt_std=torch.tensor(1.0)\n","    else: all_tgt_tensor=torch.cat(all_train_tgt_val_list,dim=0); tgt_mean=torch.mean(all_tgt_tensor.float()); tgt_std=torch.std(all_tgt_tensor.float());\n","    if tgt_std<1e-8: tgt_std=torch.tensor(1.0)\n","    tgt_scaler_path=Path(config['results_dir'])/\"tgt_scaler_unet_pixel_lstm_cgan.pth\"\n","    torch.save({'mean':tgt_mean,'std':tgt_std},tgt_scaler_path); print(f\"目标y scaler:{tgt_scaler_path}\")\n","\n","    # DataLoaders (Identical)\n","    batch_size=config.get('batch_size',4); num_workers=config.get('num_workers',0); pin_mem=config.get('pin_memory',False)&(device.type=='cuda')\n","    train_loader=DataLoader(train_dataset,batch_size,shuffle=True,drop_last=True,num_workers=num_workers,pin_memory=pin_mem)\n","    val_loader=DataLoader(val_dataset,batch_size,shuffle=False,drop_last=False,num_workers=num_workers,pin_memory=pin_mem)\n","    test_loader=DataLoader(test_dataset,batch_size,shuffle=False,drop_last=False,num_workers=num_workers,pin_memory=pin_mem)\n","\n","    # Model Initialization\n","    s_g_7am = all_sequences_data[0][0] # sample_graph_7am_for_dims\n","    stat_feat_dim = s_g_7am.x.shape[1]; glob_env_in_dim=s_g_7am.graph_global_env_features.shape[-1]; time_in_dim=time_features_for_dataset.shape[1]\n","    if stat_feat_dim != config.get('static_feat_dim',16): config['static_feat_dim']=stat_feat_dim\n","\n","    model_G = UNetPerPixelLSTMHeadModel_Generator( # GENERATOR\n","        static_feat_dim=config['static_feat_dim'], global_env_in_dim=glob_env_in_dim, time_in_dim=time_in_dim,\n","        global_env_emb_dim=config.get('global_env_emb_dim'), time_emb_dim=config.get('time_emb_dim'),\n","        unet_input_channels_after_fusion=config.get('unet_input_channels_after_fusion'), dropout_rate_pre_fusion=config.get('dropout_rate_pre_fusion'),\n","        unet_feature_output_channels=config.get('unet_feature_output_channels'), unet_encoder_channels_list=config.get('unet_encoder_channels_list'),\n","        unet_middle_channels_val=config.get('unet_middle_channels_val'), unet_decoder_channels_list=config.get('unet_decoder_channels_list'),\n","        h0_unet_input_channels=config.get('h0_unet_input_channels'), h0_unet_output_channels=config.get('h0_unet_output_channels'),\n","        h0_unet_enc_ch_list=config.get('h0_unet_enc_ch_list'), h0_unet_mid_ch=config.get('h0_unet_mid_ch'), h0_unet_dec_ch_list=config.get('h0_unet_dec_ch_list'),\n","        lstm_hidden_dim=config.get('lstm_hidden_dim'), num_lstm_layers=config.get('num_lstm_layers'), dropout_rate_lstm=config.get('dropout_rate_lstm'),\n","        mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim'), dropout_rate_pred_head=config.get('dropout_rate_pred_head'),\n","        T_pred_horizon=T_PRED_HORIZON, H_img=H_IMG, W_img=W_IMG, dropout_rate_other_mlps=config.get('dropout_rate_other_mlps')\n","    ).to(device)\n","    model_D = PatchGANDiscriminator(input_channels=config.get('discriminator_input_channels')).to(device)\n","    model_G.static_image_feat_mean = static_img_mean.to(device); model_G.static_image_feat_std = static_img_std.to(device)\n","    params_G = sum(p.numel() for p in model_G.parameters() if p.requires_grad); params_D = sum(p.numel() for p in model_D.parameters() if p.requires_grad)\n","    print(f\"Generator Params: {params_G:,}, Discriminator Params: {params_D:,}\"); report_data.update({'p_G':params_G, 'p_D':params_D, 'p_total':params_G+params_D})\n","\n","    # FLOPS Calculation\n","    print(\"\\nCalculating MACs for UNet Per-Pixel LSTM cGAN components:\"); report_data['component_gmacs'] = {}\n","    dummy_B=1; model_G.eval(); model_D.eval() # ... (FLOPS sections for G sub-modules and D as in previous cGAN block) ...\n","    try: macs_ge = torchprofile.profile_macs(model_G.global_env_encoder, args=(torch.randn(dummy_B, glob_env_in_dim, device=device),)) / 1e9\n","    except Exception: macs_ge=\"Error\" ; print(f\"Error profiling G_global_env_encoder_mlp\")\n","    report_data['component_gmacs']['G_global_env_encoder_mlp(single)'] = macs_ge\n","    try: macs_t = torchprofile.profile_macs(model_G.time_encoder, args=(torch.randn(time_in_dim, device=device),)) / 1e9\n","    except Exception: macs_t=\"Error\"; print(f\"Error profiling G_time_encoder_mlp\")\n","    report_data['component_gmacs']['G_time_encoder_mlp(single)'] = macs_t\n","    dummy_stat_img = torch.randn(dummy_B, config['static_feat_dim'], H_IMG, W_IMG, device=device)\n","    dummy_exp_ge = torch.randn(dummy_B, config.get('global_env_emb_dim'), H_IMG, W_IMG, device=device)\n","    dummy_exp_te = torch.randn(dummy_B, config.get('time_emb_dim'), H_IMG, W_IMG, device=device)\n","    try: macs_pre_fus = torchprofile.profile_macs(model_G.pre_unet_fusion, args=(dummy_stat_img, dummy_exp_ge, dummy_exp_te)) / 1e9\n","    except Exception: macs_pre_fus=\"Error\"; print(f\"Error profiling G_pre_unet_fusion\")\n","    report_data['component_gmacs']['G_pre_unet_fusion(single)'] = macs_pre_fus\n","    dummy_unet_in = torch.randn(dummy_B, config.get('unet_input_channels_after_fusion'), H_IMG, W_IMG, device=device)\n","    try: macs_unet_ext = torchprofile.profile_macs(model_G.unet_feature_extractor, args=(dummy_unet_in,)) / 1e9\n","    except Exception: macs_unet_ext=\"Error\"; print(f\"Error profiling G_unet_feature_extractor\")\n","    report_data['component_gmacs']['G_unet_feature_extractor(single)'] = macs_unet_ext\n","    dummy_h0_unet_in = torch.randn(dummy_B, config.get('h0_unet_input_channels'), H_IMG, W_IMG, device=device) # If H0 UNet input is different\n","    try: macs_h0_ext = torchprofile.profile_macs(model_G.h0_pixel_feature_extractor, args=(dummy_h0_unet_in if config.get('h0_unet_input_channels') != config.get('static_feat_dim') else dummy_stat_img,)) / 1e9\n","    except Exception: macs_h0_ext=\"Error\"; print(f\"Error profiling G_h0_pixel_feature_extractor\")\n","    report_data['component_gmacs']['G_h0_pixel_feature_extractor'] = macs_h0_ext\n","    try: # LSTM Manual\n","        lstm_l = model_G.lstm; N_b_L = dummy_B*H_IMG*W_IMG; L_s_L=T_PRED_HORIZON; H_i_L=model_G.lstm_input_size_for_profiling; H_h_L=model_G.lstm_hidden_dim_for_profiling; n_l_L=lstm_l.num_layers\n","        macs_LST = N_b_L*L_s_L*4*(H_i_L*H_h_L+H_h_L*H_h_L)\n","        if n_l_L>1: macs_LST+=N_b_L*L_s_L*(n_l_L-1)*4*(H_h_L*H_h_L+H_h_L*H_h_L)\n","        gmacs_LST = macs_LST/1e9\n","    except Exception: gmacs_LST=\"Error\"; print(f\"Error in LSTM MACs\")\n","    report_data['component_gmacs']['G_lstm_layer(manual)'] = gmacs_LST\n","    try: macs_ph = torchprofile.profile_macs(model_G.hourly_prediction_heads[0], args=(torch.randn(dummy_B*H_IMG*W_IMG, config.get('lstm_hidden_dim'), device=device),)) / 1e9\n","    except Exception: macs_ph=\"Error\"; print(f\"Error profiling G_prediction_head_mlp\")\n","    report_data['component_gmacs']['G_prediction_head_mlp(single)'] = macs_ph\n","    print(f\" G GMACs Est(single): GEEnc={macs_ge:.3f},TEnc={macs_t:.3f},PreFus={macs_pre_fus:.3f},UNetExt={macs_unet_ext:.3f},H0Ext={macs_h0_ext:.3f},LSTM={gmacs_LST:.3f},PredHead={macs_ph:.3f}\")\n","    try: # Discriminator\n","        dummy_D_in = torch.randn(dummy_B, config.get('discriminator_input_channels'), H_IMG, W_IMG, device=device)\n","        macs_D = torchprofile.profile_macs(model_D, args=(dummy_D_in,)) / 1e9\n","        report_data['component_gmacs']['D_patchgan_discriminator(single)'] = macs_D\n","        print(f\"  Discriminator PatchGAN GMACs (single pass): {macs_D:.4f}\")\n","    except Exception as e: print(f\"  Error profiling Discriminator: {e}\")\n","    model_G.train(); model_D.train()\n","\n","    # Optimizers, Schedulers, Loss criteria (as in previous cGAN block)\n","    opt_G = torch.optim.Adam(model_G.parameters(),lr=config.get('lr_G',0.0002),betas=(config.get('beta1',0.5),config.get('beta2',0.999)))\n","    opt_D = torch.optim.Adam(model_D.parameters(),lr=config.get('lr_D',0.0002),betas=(config.get('beta1',0.5),config.get('beta2',0.999)))\n","    sch_G = ReduceLROnPlateau(opt_G,'min',factor=0.5,patience=config.get('scheduler_patience_G',25),verbose=True)\n","    sch_D = ReduceLROnPlateau(opt_D,'min',factor=0.5,patience=config.get('scheduler_patience_D',25),verbose=True)\n","    crit_GAN = nn.MSELoss().to(device); crit_L1 = nn.L1Loss().to(device)\n","    lambda_L1_val = config.get('lambda_L1',100.0)\n","\n","    best_val_L1 = float('inf'); best_val_metrics = None; best_ep = 0; pat_count = 0\n","    max_ep = config.get('max_epochs',100); early_stop_pat = config.get('early_stopping_patience',30) # Reduced for quick testing\n","    model_G_path = Path(config['results_dir'])/f\"best_unet_pixel_lstm_cgan_G_s{seed}.pth\"\n","    model_D_path = Path(config['results_dir'])/f\"best_unet_pixel_lstm_cgan_D_s{seed}.pth\"\n","    timeline_feat_dev = time_features_for_dataset.to(device)\n","    tgt_mean_cpu = tgt_mean.cpu(); tgt_std_cpu = tgt_std.cpu()\n","    ep_times = []\n","\n","    for ep in range(1, max_ep + 1):\n","        avg_L_G, avg_L_D, avg_L_G_GAN, avg_L_G_L1, ep_dur = train_epoch_unet_pixel_lstm_cgan(\n","            model_G, model_D, train_loader, opt_G, opt_D, crit_GAN, crit_L1, lambda_L1_val,\n","            device, timeline_feat_dev, H_IMG, W_IMG, static_img_mean, static_img_std, tgt_mean, tgt_std\n","        )\n","        ep_times.append(ep_dur)\n","        val_L1, val_metrics, _ = evaluate_epoch_unet_pixel_lstm_cgan(\n","            model_G, val_loader, crit_L1, device, timeline_feat_dev, H_IMG, W_IMG,\n","            static_img_mean, static_img_std, tgt_mean, tgt_std, lambda_L1_val, epoch_type=\"Validation\"\n","        )\n","        sch_G.step(val_L1); sch_D.step(avg_L_D)\n","        print(f\"Ep {ep:03d}|T:{ep_dur:.1f}s|G_L:{avg_L_G:.4f}(GAN:{avg_L_G_GAN:.4f},L1:{avg_L_G_L1:.4f})|D_L:{avg_L_D:.4f}|Val_L1:{val_L1:.4f}|LR_G:{opt_G.param_groups[0]['lr']:.6f}\")\n","        _print_hourly_metrics_summary(\"Val\", val_metrics, T_PRED_HORIZON, indent=\" \"*21)\n","        if val_L1 < best_val_L1:\n","            best_val_L1=val_L1; best_val_metrics=val_metrics; best_ep=ep; pat_count=0\n","            torch.save(model_G.state_dict(),model_G_path); torch.save(model_D.state_dict(),model_D_path)\n","            print(f\"{' '*21}---> Best models saved (Ep:{ep}, Val L1:{best_val_L1:.4f})\")\n","        else: pat_count+=1\n","        if pat_count>=early_stop_pat: print(f\"Early stopping at ep {ep}.\"); break\n","\n","    # Reporting and final evaluation (copy & adapt from previous main_training)\n","    report_data['total_training_time_seconds'] = time.time()-train_start_time # ...\n","    report_data['average_epoch_time_seconds'] = np.mean(ep_times) if ep_times else np.nan\n","    report_data['num_epochs_trained'] = ep; report_data['best_validation_epoch'] = best_ep\n","    report_data['best_validation_scaled_L1_loss'] = best_val_L1\n","\n","    model_G_eval = UNetPerPixelLSTMHeadModel_Generator( # Instantiate with same config\n","        static_feat_dim=config['static_feat_dim'], global_env_in_dim=glob_env_in_dim, time_in_dim=time_in_dim,\n","        global_env_emb_dim=config.get('global_env_emb_dim'), time_emb_dim=config.get('time_emb_dim'),\n","        unet_input_channels_after_fusion=config.get('unet_input_channels_after_fusion'), dropout_rate_pre_fusion=config.get('dropout_rate_pre_fusion'),\n","        unet_feature_output_channels=config.get('unet_feature_output_channels'), unet_encoder_channels_list=config.get('unet_encoder_channels_list'),\n","        unet_middle_channels_val=config.get('unet_middle_channels_val'), unet_decoder_channels_list=config.get('unet_decoder_channels_list'),\n","        h0_unet_input_channels=config.get('h0_unet_input_channels'), h0_unet_output_channels=config.get('h0_unet_output_channels'),\n","        h0_unet_enc_ch_list=config.get('h0_unet_enc_ch_list'), h0_unet_mid_ch=config.get('h0_unet_mid_ch'), h0_unet_dec_ch_list=config.get('h0_unet_dec_ch_list'),\n","        lstm_hidden_dim=config.get('lstm_hidden_dim'), num_lstm_layers=config.get('num_lstm_layers'), dropout_rate_lstm=config.get('dropout_rate_lstm'),\n","        mlp_prediction_hidden_dim=config.get('mlp_prediction_hidden_dim'), dropout_rate_pred_head=config.get('dropout_rate_pred_head'),\n","        T_pred_horizon=T_PRED_HORIZON, H_img=H_IMG, W_img=W_IMG, dropout_rate_other_mlps=config.get('dropout_rate_other_mlps')\n","    ).to(device)\n","    try: model_G_eval.load_state_dict(torch.load(model_G_path, map_location=device))\n","    except Exception as e: print(f\"无法加载最佳G模型({e})\"); model_G_eval = model_G\n","\n","    print(\"\\nEval G on Train...\"); train_L1, train_metrics, train_dur = evaluate_epoch_unet_pixel_lstm_cgan(model_G_eval, train_loader, crit_L1, device, timeline_feat_dev, H_IMG,W_IMG, static_img_mean, static_img_std, tgt_mean_cpu, tgt_std_cpu, lambda_L1_val, \"Best G on Train\")\n","    report_data.update({'best_G_train_metrics_hr':train_metrics, 'best_G_train_metrics_agg':calculate_aggregated_metrics_report(train_metrics,T_PRED_HORIZON), 'best_G_train_eval_time_s':train_dur})\n","    _print_hourly_metrics_summary(\"最佳G训练集\", train_metrics, T_PRED_HORIZON)\n","    report_data['best_G_val_metrics_hr'] = best_val_metrics\n","    if best_val_metrics: report_data['best_G_val_metrics_agg'] = calculate_aggregated_metrics_report(best_val_metrics, T_PRED_HORIZON)\n","    _print_hourly_metrics_summary(\"最佳G验证集\", report_data['best_G_val_metrics_hr'], T_PRED_HORIZON)\n","    print(\"\\nEval G on Test...\"); test_L1, test_metrics, test_dur = evaluate_epoch_unet_pixel_lstm_cgan(model_G_eval, test_loader, crit_L1, device, timeline_feat_dev, H_IMG,W_IMG, static_img_mean, static_img_std, tgt_mean_cpu, tgt_std_cpu, lambda_L1_val, \"Test\")\n","    report_data.update({'test_infer_time_s':test_dur, 'best_G_test_metrics_hr':test_metrics, 'best_G_test_metrics_agg':calculate_aggregated_metrics_report(test_metrics,T_PRED_HORIZON), 'test_scaled_L1':test_L1})\n","    print(f\"\\n{'='*10} Final Test (U-Net Pixel LSTM cGAN - G) {'='*10}\\nAvg Test Scaled L1: {test_L1:.4f}\")\n","    _print_hourly_metrics_summary(\"测试集 (G)\", test_metrics, T_PRED_HORIZON)\n","    agg_tst = report_data['best_G_test_metrics_agg'] # ... (rest of printout for agg_test)\n","    print(f\"Avg Test MSE(Orig): {agg_tst.get('avg_mse',np.nan):.4f} (Std:{agg_tst.get('std_mse',np.nan):.4f})\")\n","    print(f\"Avg Test R2 (Orig): {agg_tst.get('avg_r2',np.nan):.4f} (Std:{agg_tst.get('std_r2',np.nan):.4f})\")\n","    print(f\"Avg Test MAE(Orig): {agg_tst.get('avg_mae',np.nan):.4f} (Std:{agg_tst.get('std_mae',np.nan):.4f})\")\n","    print(f\"Avg Test RMSE(Orig):{agg_tst.get('avg_rmse',np.nan):.4f} (Std:{agg_tst.get('std_rmse',np.nan):.4f})\")\n","    print(\"=\"*70)\n","    report_file = Path(config['results_dir'])/f\"report_unet_pixel_lstm_cgan_s{seed}.json\" # ... (JSON save)\n","    try:\n","        class NpEncoder(json.JSONEncoder):\n","            def default(self,o):\n","                if isinstance(o,np.integer):return int(o)\n","                if isinstance(o,np.floating):return float(o)\n","                if isinstance(o,np.ndarray):return o.tolist()\n","                if isinstance(o,torch.Tensor):return o.tolist()\n","                if isinstance(o,Path):return str(o)\n","                return super(NpEncoder,self).default(o)\n","        with open(report_file,'w') as f: json.dump(report_data,f,indent=4,cls=NpEncoder)\n","        print(f\"训练报告: {report_file}\")\n","    except Exception as e: print(f\"保存训练报告失败: {e}\")\n","    return model_G_eval, model_D, static_img_mean, static_img_std, tgt_mean, tgt_std\n","\n","\n","def _print_hourly_metrics_summary(set_name, hourly_metrics, T_pred_horizon, indent=\"  \"): # No change\n","    # ... (Identical)\n","    if hourly_metrics is None: print(f\"{indent}{set_name} metrics not available.\"); return\n","    print(f\"\\n{indent}每小时 {set_name} 指标 (Original Scale):\")\n","    header_printed = False\n","    for hour_idx in range(T_pred_horizon):\n","        metrics = hourly_metrics.get(hour_idx, {'mse': np.nan, 'mae': np.nan, 'rmse': np.nan, 'r2': np.nan, 'count':0})\n","        if not header_printed: print(f\"{indent}  Hour | {'R2':>13s} | {'MSE':>14s} | {'MAE':>14s} | {'RMSE':>15s} | {'Count':>7s}\"); header_printed = True\n","        print(f\"{indent}  {hour_idx:02d}   | {metrics.get('r2',np.nan):13.4f} | {metrics.get('mse',np.nan):14.4f} | {metrics.get('mae',np.nan):14.4f} | {metrics.get('rmse',np.nan):15.4f} | {metrics.get('count',0):7d}\")\n","    aggregated = calculate_aggregated_metrics_report(hourly_metrics, T_pred_horizon)\n","    print(f\"{indent}  Aggregated Avg R2   : {aggregated.get('avg_r2', np.nan):.4f} (Std: {aggregated.get('std_r2', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MSE  : {aggregated.get('avg_mse', np.nan):.4f} (Std: {aggregated.get('std_mse', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg MAE  : {aggregated.get('avg_mae', np.nan):.4f} (Std: {aggregated.get('std_mae', np.nan):.4f})\")\n","    print(f\"{indent}  Aggregated Avg RMSE : {aggregated.get('avg_rmse', np.nan):.4f} (Std: {aggregated.get('std_rmse', np.nan):.4f})\")\n","\n","\n","# ===========================================================\n","# 6. 主执行块 (Adapted for U-Net Per-Pixel LSTM cGAN)\n","# ===========================================================\n","if __name__ == \"__main__\":\n","    gc.collect(); torch.cuda.empty_cache()\n","\n","    DRIVE_BASE_PATH=Path(\"/content/drive/MyDrive/Colab Notebooks/Graph Data Process\")\n","    DRIVE_BASE_PATH.mkdir(parents=True, exist_ok=True)\n","    DATA_SUBDIR=Path(\"Result/Sequential_12Hour_Data\"); DATA_FILENAME=\"graph_seq_20230503_SeqH7to19_NpyH8fill0.0.pkl\"\n","    RESULTS_SUBDIR=Path(\"Result/Final_UNetPixelLSTM1\"); RESULTS_SAVE_DIR=DRIVE_BASE_PATH/RESULTS_SUBDIR\n","    RESULTS_SAVE_DIR.mkdir(parents=True,exist_ok=True); DATA_PATH=DRIVE_BASE_PATH/DATA_SUBDIR/DATA_FILENAME\n","    DATA_YEAR=2023; DATA_MONTH=5; DATA_DAY=3; START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES=8; PREDICTION_HORIZON=12\n","    H_IMG,W_IMG = 50,50\n","\n","    training_config = {\n","        'seed':42, 'batch_size':8, 'lr_G':0.0002, 'lr_D':0.0002, 'beta1':0.5, 'beta2':0.999, 'lambda_L1':100.0,\n","        'max_epochs':1000, 'scheduler_patience_G':20, 'scheduler_patience_D':20, 'early_stopping_patience':45, # Set max_epochs higher for real training\n","        'T_pred_horizon':PREDICTION_HORIZON, 'results_dir':str(RESULTS_SAVE_DIR), 'H_IMG':H_IMG, 'W_IMG':W_IMG,\n","        'static_feat_dim':16, 'global_env_emb_dim':16, 'time_emb_dim':8,\n","        'unet_input_channels_after_fusion':32, 'dropout_rate_pre_fusion':0.1,\n","        'unet_feature_output_channels':128, 'unet_encoder_channels_list':(64,128,256),\n","        'unet_middle_channels_val':512, 'unet_decoder_channels_list':(256,128,64),\n","        # H0 U-Net params (can be smaller than main U-Net)\n","        'h0_unet_input_channels': 16, # Takes the 16-channel static image\n","        'h0_unet_output_channels': 128, # Output channels of U-Net used within H0 extractor\n","        'h0_unet_enc_ch_list': (32, 64, 128), 'h0_unet_mid_ch': 256, 'h0_unet_dec_ch_list': (128, 64, 32),\n","        'lstm_hidden_dim':128, 'num_lstm_layers':1, 'dropout_rate_lstm':0.2, # LSTM hidden dim is output of H0 projector, and input to heads\n","        'mlp_prediction_hidden_dim':64, 'dropout_rate_pred_head':0.2,\n","        'dropout_rate_other_mlps':0.1,\n","        'discriminator_input_channels':32+1, # C_fused (32) + 1 (output image)\n","        'use_amp':False, 'enable_profiler':False, 'num_workers':0, 'pin_memory':False,\n","        'train_split_ratio':0.7, 'val_split_ratio':0.2,\n","    }\n","\n","    all_graph_sequences_loaded = None\n","    try: # Data loading (as before)\n","        if not DATA_PATH.exists(): raise FileNotFoundError(f\"Data file not found: {DATA_PATH}\")\n","        with open(DATA_PATH, \"rb\") as f: all_graph_sequences_loaded = pickle.load(f)\n","        if not all_graph_sequences_loaded or not isinstance(all_graph_sequences_loaded, list) or \\\n","           not all_graph_sequences_loaded[0] or not isinstance(all_graph_sequences_loaded[0], list):\n","            raise ValueError(\"Loaded data is not in expected list-of-lists format or is empty.\")\n","        print(f\"Original loaded sequences: {len(all_graph_sequences_loaded)}\")\n","    except Exception as e: print(f\"Error loading data: {e}\"); all_graph_sequences_loaded = None\n","\n","    if all_graph_sequences_loaded:\n","        base_dt_timeline = dt_datetime(DATA_YEAR,DATA_MONTH,DATA_DAY,START_HOUR_OF_DAY_FOR_TIMELINE_FEATURES)\n","        time_features_for_dataset = generate_time_features_for_sequence(base_dt_timeline, training_config['T_pred_horizon'])\n","        main_training_unet_pixel_lstm_cgan(all_graph_sequences_loaded, training_config, time_features_for_dataset)\n","        print(\"U-Net Per-Pixel LSTM cGAN training and evaluation complete!\")\n","    else:\n","        print(\"Training not started due to data loading issues.\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["szgoBzKA1xql","l4oT7hkt2Coz","bUuXd9vb2AUZ","5UPE93sr2D7Z","vmZK4T08LpKh","lafh8hfCBpkG","0KS-6YucB5Lk","HddOV6Y9cmn7","TajR3S03l2Rl"],"gpuType":"L4","provenance":[],"authorship_tag":"ABX9TyN8mjIsYNaJx/u+0FfMpg4p"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}